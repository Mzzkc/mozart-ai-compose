# Mozart Self-Completion v2: Multi-Agent Meta-Orchestration
#
# Enhanced version with:
# - Parallel TDF-aligned investigation agents
# - Multi-perspective brainstorming
# - Dedicated security architecture review
# - Code review before each commit
# - Post-implementation security audit
# - Adversarial testing phase
#
# Structure:
#   Phase 1 (Batch 1): Parallel TDF Investigation (4 agents)
#   Phase 2 (Batch 2): Multi-Agent Brainstorm (4 perspectives)
#   Phase 3 (Batch 3): Security Architecture Review
#   Phase 4 (Batch 4): Consolidation & Planning
#   Phase 5 (Batches 5-13): Implementation with Code Review
#   Phase 6 (Batch 14): Security Audit
#   Phase 7 (Batch 15): Adversarial Testing

name: "mozart-self-complete-v2"
description: "Mozart orchestrates its own completion with multi-agent coordination, security audits, and adversarial testing"

workspace: "./self-complete-workspace"

backend:
  type: claude_cli
  skip_permissions: true
  working_directory: "/home/emzi/Projects/mozart-ai-compose"
  timeout_seconds: 3600  # 60 min for parallel agent batches

batch:
  size: 1
  total_items: 15  # 4 planning + 9 implementation + 2 audit
  start_item: 1

prompt:
  template: |
    {{ preamble }}

    {% if batch_num == 1 %}
    ============================================================
    PHASE 1: PARALLEL TDF INVESTIGATION (Batch 1 of {{ total_batches }})
    ============================================================

    Your mission: Launch 4 parallel investigation agents, each analyzing Mozart
    from a different TDF (Tetrahedral Decision Framework) domain perspective.

    STEP 1: Read the TDF framework
    - /home/emzi/.claude/skills/tetrahedral-decision-framework.md

    STEP 2: Launch 4 parallel investigation agents using the Task tool

    Launch these agents IN PARALLEL (all in one message with multiple Task calls):

    **Agent 1: COMPUTATIONAL (COMP)**
    ```
    Analyze Mozart from the COMPUTATIONAL domain perspective:
    - Code architecture and abstractions
    - Algorithm efficiency and complexity
    - Error handling and edge cases
    - Type safety and data flow
    - Technical debt and code smells

    Read: src/mozart/, STATUS.md, README.md
    Output to: {{ workspace }}/investigation-comp.md
    ```

    **Agent 2: SCIENTIFIC (SCI)**
    ```
    Analyze Mozart from the SCIENTIFIC domain perspective:
    - Test coverage gaps (run pytest --cov)
    - Validation completeness
    - Reproducibility of results
    - Measurement and observability
    - Evidence-based quality metrics

    Read: tests/, src/mozart/execution/validation.py
    Output to: {{ workspace }}/investigation-sci.md
    ```

    **Agent 3: CULTURAL (CULT)**
    ```
    Analyze Mozart from the CULTURAL domain perspective:
    - Code pattern consistency across modules
    - Documentation quality and completeness
    - User experience and CLI ergonomics
    - Naming conventions and readability
    - Alignment with Python community standards

    Read: src/mozart/cli.py, docs/, README.md
    Output to: {{ workspace }}/investigation-cult.md
    ```

    **Agent 4: EXPERIENTIAL (EXP)**
    ```
    Analyze Mozart from the EXPERIENTIAL domain perspective:
    - Extensibility and plugin potential
    - Graceful degradation scenarios
    - Error recovery and resilience
    - Future-proofing and adaptability
    - Integration possibilities

    Read: src/mozart/backends/, src/mozart/state/
    Output to: {{ workspace }}/investigation-exp.md
    ```

    STEP 3: Wait for all agents to complete

    STEP 4: Synthesize findings
    Read all 4 investigation files and create a unified gap analysis:
    {{ workspace }}/01-unified-investigation.md

    Format:
    ```markdown
    # Unified TDF Investigation

    ## Meta-Analysis
    [Cross-domain patterns and insights]

    ## COMP Findings
    [Summary from computational analysis]

    ## SCI Findings
    [Summary from scientific analysis]

    ## CULT Findings
    [Summary from cultural analysis]

    ## EXP Findings
    [Summary from experiential analysis]

    ## Priority Gaps (Cross-Domain Synthesis)
    1. [Gap affecting multiple domains]
    2. [Next priority]
    ...
    ```

    STEP 5: Write validation markers to {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Parallel TDF Investigation
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    AGENTS_LAUNCHED: 4
    PERSPECTIVES: COMP, SCI, CULT, EXP
    ```

    {% elif batch_num == 2 %}
    ============================================================
    PHASE 2: MULTI-AGENT BRAINSTORM (Batch 2 of {{ total_batches }})
    ============================================================

    Your mission: Launch 4 parallel brainstorming agents, each focused on a
    different improvement category.

    STEP 1: Read the unified investigation
    - {{ workspace }}/01-unified-investigation.md

    STEP 2: Launch 4 parallel brainstorm agents using the Task tool

    **Agent 1: SECURITY Focus**
    ```
    Brainstorm security improvements for Mozart:
    - Input validation and sanitization
    - Secrets management (API keys, webhooks)
    - Dependency security
    - Sandbox escapes and command injection risks
    - Authentication for dashboard API

    Reference: OWASP CLI Security guidelines
    Output to: {{ workspace }}/brainstorm-security.md
    ```

    **Agent 2: PERFORMANCE Focus**
    ```
    Brainstorm performance improvements for Mozart:
    - Parallel batch execution opportunities
    - Caching strategies
    - Resource usage optimization
    - Startup time reduction
    - Memory efficiency for large jobs

    Output to: {{ workspace }}/brainstorm-performance.md
    ```

    **Agent 3: USER EXPERIENCE Focus**
    ```
    Brainstorm UX improvements for Mozart:
    - CLI discoverability and help text
    - Error messages and recovery hints
    - Progress visualization
    - Configuration validation feedback
    - Interactive modes

    Reference: Best CLI tools (gh, docker, kubectl)
    Output to: {{ workspace }}/brainstorm-ux.md
    ```

    **Agent 4: INTEGRATION Focus**
    ```
    Brainstorm integration improvements for Mozart:
    - CI/CD pipeline integration
    - Monitoring and alerting hooks
    - IDE plugins potential
    - API extensibility
    - Plugin architecture

    Output to: {{ workspace }}/brainstorm-integration.md
    ```

    STEP 3: Wait for all agents to complete

    STEP 4: Synthesize brainstorm results
    {{ workspace }}/02-brainstorm-synthesis.md

    STEP 5: Write validation markers to {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Multi-Agent Brainstorm
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    AGENTS_LAUNCHED: 4
    PERSPECTIVES: Security, Performance, UX, Integration
    ```

    {% elif batch_num == 3 %}
    ============================================================
    PHASE 3: SECURITY ARCHITECTURE REVIEW (Batch 3 of {{ total_batches }})
    ============================================================

    Your mission: Conduct a dedicated security architecture review of Mozart
    before implementation planning begins.

    STEP 1: Read previous analyses
    - {{ workspace }}/01-unified-investigation.md
    - {{ workspace }}/brainstorm-security.md

    STEP 2: Security Audit Checklist

    **A. Input Validation**
    - [ ] YAML config parsing (arbitrary code execution risks?)
    - [ ] Jinja2 template injection possibilities
    - [ ] File path traversal in workspace/validation paths
    - [ ] Command injection in shell commands

    **B. Secrets Management**
    - [ ] API keys in config vs environment variables
    - [ ] Webhook URLs and tokens
    - [ ] Credential logging in outputs
    - [ ] State file permissions

    **C. Dependency Security**
    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate
    pip audit 2>&1 || pip install pip-audit && pip audit
    ```

    **D. Subprocess Security**
    - [ ] Shell=True usage review
    - [ ] Environment variable leakage
    - [ ] Working directory escapes

    **E. Dashboard API Security**
    - [ ] Authentication requirements
    - [ ] CORS configuration
    - [ ] Rate limiting
    - [ ] Input validation on endpoints

    STEP 3: Create Security Review Report
    {{ workspace }}/03-security-review.md

    Format:
    ```markdown
    # Mozart Security Architecture Review

    ## Executive Summary
    Risk Level: [LOW/MEDIUM/HIGH]
    Critical Issues: [count]
    Recommendations: [count]

    ## Findings by Category

    ### Critical (Must Fix Before Release)
    - [Finding with remediation]

    ### High (Should Fix Soon)
    - [Finding with remediation]

    ### Medium (Plan to Address)
    - [Finding with remediation]

    ### Low (Nice to Have)
    - [Finding with remediation]

    ## Dependency Audit Results
    [pip audit output]

    ## Recommended Security Tasks for Implementation Plan
    1. [Specific task]
    2. [Specific task]
    ```

    STEP 4: Write validation markers to {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Security Architecture Review
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    CRITICAL_ISSUES: [count]
    RISK_LEVEL: [LOW/MEDIUM/HIGH]
    ```

    {% elif batch_num == 4 %}
    ============================================================
    PHASE 4: CONSOLIDATION & PLANNING (Batch 4 of {{ total_batches }})
    ============================================================

    Your mission: Consolidate all investigation, brainstorm, and security review
    findings into a prioritized implementation plan for 9 tasks.

    STEP 1: Read all previous outputs
    - {{ workspace }}/01-unified-investigation.md
    - {{ workspace }}/02-brainstorm-synthesis.md
    - {{ workspace }}/03-security-review.md
    - {{ workspace }}/investigation-*.md (all 4)
    - {{ workspace }}/brainstorm-*.md (all 4)

    STEP 2: Prioritization Framework

    Priority Score = (Impact Ã— 3) + (Security Ã— 2) + (Feasibility Ã— 1)

    Where:
    - Impact: 1-5 (user value)
    - Security: 1-5 (security improvement, 5 = fixes critical issue)
    - Feasibility: 1-5 (ease of implementation)

    STEP 3: Create Implementation Plan
    {{ workspace }}/04-implementation-plan.md

    IMPORTANT: Security-critical items from the security review MUST be
    included in the implementation plan, prioritized appropriately.

    Format same as v1, but include:
    - Security requirements for each task
    - Code review criteria
    - Adversarial test cases to consider

    STEP 4: Write validation markers to {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Consolidation & Planning
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    TASKS_PLANNED: 9
    SECURITY_TASKS_INCLUDED: [count]
    ```

    STEP 5: Commit planning phase
    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    git add {{ workspace }}/*.md 2>/dev/null || true
    git commit -m "docs(self-complete-v2): Planning phase complete - multi-agent analysis" 2>/dev/null || true
    ```

    {% elif batch_num >= 5 and batch_num <= 13 %}
    ============================================================
    PHASE 5: IMPLEMENTATION WITH CODE REVIEW (Batch {{ batch_num }} of {{ total_batches }})
    ============================================================

    {% set task_num = batch_num - 4 %}
    You are implementing Task {{ task_num }} of 9.

    STEP 1: Read the implementation plan
    - {{ workspace }}/04-implementation-plan.md
    - Find "Task {{ task_num }}" section
    - Note security requirements and review criteria

    STEP 2: Read security review for relevant findings
    - {{ workspace }}/03-security-review.md

    STEP 3: Implement the task
    - Create/modify files as specified
    - Address any security requirements noted in the plan
    - Write tests including security-focused test cases
    - Ensure mypy and ruff pass

    STEP 4: Self Code Review
    Before committing, review your own code against these criteria:

    **Security Checklist:**
    - [ ] No hardcoded secrets or credentials
    - [ ] Input validation on all external inputs
    - [ ] No shell=True with user-controlled input
    - [ ] Proper error handling (no sensitive info in errors)
    - [ ] File operations use safe path handling

    **Quality Checklist:**
    - [ ] Type hints on all functions
    - [ ] Docstrings on public APIs
    - [ ] Tests cover happy path and error cases
    - [ ] No TODO comments left unaddressed
    - [ ] Consistent with existing patterns

    STEP 5: Run the code-review skill for external review
    Use the Skill tool to invoke /code-review on your changes:
    - Review the diff of files you modified
    - Address any critical or high-severity findings
    - Document accepted risks for medium/low findings

    STEP 6: Validate
    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate
    pytest tests/ -v --tb=short 2>&1 | tee {{ workspace }}/pytest-task{{ task_num }}.txt
    mypy src/ 2>&1 | tee {{ workspace }}/mypy-task{{ task_num }}.txt
    ruff check src/ 2>&1 | tee {{ workspace }}/ruff-task{{ task_num }}.txt
    ```

    STEP 7: Write result summary
    {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Implementation
    BATCH: {{ batch_num }}
    TASK_NUM: {{ task_num }}
    TASK_NAME: [from plan]
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    CODE_REVIEW_PASS: yes
    SECURITY_CHECKLIST_PASS: yes

    ## Security Considerations
    - [How security requirements were addressed]

    ## Code Review Findings Addressed
    - [List of findings and resolutions]

    ## Files Created/Modified
    - [list]

    ## Tests Added
    - [count and description, including security tests]
    ```

    STEP 8: Commit with review attestation
    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    git add -A
    git commit -m "$(cat <<'COMMIT_EOF'
    feat(task{{ task_num }}): [TASK_NAME] - Mozart self-completion batch {{ batch_num }}

    [Description]

    Security: [security considerations addressed]
    Code Review: Passed self-review and automated checks
    Tests: [X] new tests including security cases

    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>
    COMMIT_EOF
    )"
    ```

    {% elif batch_num == 14 %}
    ============================================================
    PHASE 6: POST-IMPLEMENTATION SECURITY AUDIT (Batch 14 of {{ total_batches }})
    ============================================================

    Your mission: Conduct a comprehensive security audit of all implementation
    work before adversarial testing.

    STEP 1: Gather implementation summary
    Read all task results: {{ workspace }}/batch5-result.md through batch13-result.md

    STEP 2: Run automated security scans

    **A. Static Analysis**
    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate

    # Bandit security linter
    pip install bandit 2>/dev/null
    bandit -r src/mozart/ -f json -o {{ workspace }}/bandit-report.json 2>&1 || true
    bandit -r src/mozart/ 2>&1 | tee {{ workspace }}/bandit-report.txt
    ```

    **B. Dependency Vulnerabilities**
    ```bash
    pip audit 2>&1 | tee {{ workspace }}/pip-audit-final.txt
    ```

    **C. Secrets Detection**
    ```bash
    # Check for hardcoded secrets
    grep -r -i "api_key\|password\|secret\|token" src/mozart/ --include="*.py" | grep -v "api_key_env\|_env\|getenv" | tee {{ workspace }}/secrets-scan.txt || true
    ```

    **D. Input Validation Audit**
    Review all entry points:
    - CLI argument parsing
    - YAML config loading
    - API request handling
    - File path handling

    STEP 3: Manual Security Review
    For each new file created in Phase 5, verify:
    - [ ] No command injection vectors
    - [ ] No path traversal vulnerabilities
    - [ ] No SSRF in HTTP clients
    - [ ] No XXE in XML/YAML parsing
    - [ ] No insecure deserialization

    STEP 4: Create Security Audit Report
    {{ workspace }}/14-security-audit.md
    ```markdown
    # Post-Implementation Security Audit

    ## Scan Results Summary
    | Tool | Critical | High | Medium | Low |
    |------|----------|------|--------|-----|
    | Bandit | X | X | X | X |
    | pip audit | X | X | X | X |
    | Secrets scan | X | X | X | X |

    ## Critical Findings (Must Fix)
    [List with remediation steps]

    ## High Findings (Should Fix)
    [List with remediation steps]

    ## Accepted Risks
    [Documented acceptable risks with justification]

    ## Remediation Tasks
    - [ ] [Specific fix needed]

    ## Security Posture Assessment
    Overall: [SECURE / NEEDS WORK / AT RISK]
    ```

    STEP 5: Fix any critical findings
    If critical issues found, fix them before proceeding.

    STEP 6: Write validation markers
    {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Security Audit
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    CRITICAL_FINDINGS: [count]
    HIGH_FINDINGS: [count]
    SECURITY_POSTURE: [SECURE/NEEDS_WORK/AT_RISK]
    ```

    STEP 7: Commit security audit
    ```bash
    git add {{ workspace }}/14-security-audit.md {{ workspace }}/bandit-*.* {{ workspace }}/pip-audit-final.txt
    git commit -m "security(audit): Post-implementation security audit complete"
    ```

    {% elif batch_num == 15 %}
    ============================================================
    PHASE 7: ADVERSARIAL TESTING (Batch 15 of {{ total_batches }})
    ============================================================

    Your mission: Act as an adversarial tester trying to break Mozart.
    Your goal is to find bugs, security issues, and edge cases that
    normal testing missed.

    MINDSET: You are a hostile actor trying to:
    - Crash the application
    - Leak sensitive information
    - Escape the sandbox
    - Corrupt state
    - Cause resource exhaustion

    STEP 1: Read security audit results
    - {{ workspace }}/14-security-audit.md

    STEP 2: Adversarial Test Categories

    **A. Input Fuzzing**
    Create malicious YAML configs:
    ```yaml
    # Test 1: YAML bomb (billion laughs)
    # Test 2: Deeply nested structures
    # Test 3: Extremely long strings
    # Test 4: Unicode edge cases
    # Test 5: Null bytes and control characters
    ```

    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate
    # Create and test malicious configs
    ```

    **B. Path Traversal Attempts**
    ```yaml
    workspace: "../../../etc/passwd"
    validations:
      - type: file_exists
        path: "/etc/shadow"
    ```

    **C. Command Injection Attempts**
    Test if any user input reaches shell:
    ```yaml
    name: "test; rm -rf /"
    workspace: "$(whoami)"
    ```

    **D. Resource Exhaustion**
    ```yaml
    batch:
      size: 999999999
      total_items: 999999999
    retry:
      max_retries: 999999999
    ```

    **E. State Corruption**
    - Manually corrupt checkpoint files
    - Test recovery from corrupted state
    - Concurrent access scenarios

    **F. API Abuse (Dashboard)**
    - Extremely large request bodies
    - Malformed JSON
    - Missing required fields
    - SQL injection in job IDs (if SQLite)

    STEP 3: Document Findings
    {{ workspace }}/15-adversarial-testing.md
    ```markdown
    # Adversarial Testing Report

    ## Test Summary
    | Category | Tests Run | Passed | Failed | Crashes |
    |----------|-----------|--------|--------|---------|
    | Input Fuzzing | X | X | X | X |
    | Path Traversal | X | X | X | X |
    | Command Injection | X | X | X | X |
    | Resource Exhaustion | X | X | X | X |
    | State Corruption | X | X | X | X |
    | API Abuse | X | X | X | X |

    ## Vulnerabilities Discovered
    ### Critical
    - [Description, reproduction steps, impact]

    ### High
    - [Description, reproduction steps, impact]

    ### Medium
    - [Description, reproduction steps, impact]

    ## Crash Reports
    - [Stack traces and reproduction steps]

    ## Recommendations
    1. [Specific fix]
    2. [Specific fix]

    ## Resilience Assessment
    Mozart's resilience to adversarial input: [STRONG / MODERATE / WEAK]
    ```

    STEP 4: Create adversarial test suite
    Create permanent tests from findings:
    ```bash
    # Add tests to tests/test_adversarial.py
    ```

    STEP 5: Fix critical vulnerabilities
    Any critical or high vulnerabilities discovered MUST be fixed.

    STEP 6: Write validation markers
    {{ workspace }}/batch{{ batch_num }}-result.md
    ```markdown
    PHASE: Adversarial Testing
    BATCH: {{ batch_num }}
    IMPLEMENTATION_COMPLETE: yes
    TESTS_PASS: yes
    TYPES_PASS: yes
    VULNERABILITIES_FOUND: [count]
    CRITICAL_VULNERABILITIES: [count]
    CRASHES_FOUND: [count]
    RESILIENCE_RATING: [STRONG/MODERATE/WEAK]
    ```

    STEP 7: Final commit
    ```bash
    git add -A
    git commit -m "$(cat <<'COMMIT_EOF'
    test(adversarial): Adversarial testing complete - Mozart self-completion v2

    Adversarial testing performed:
    - Input fuzzing (YAML bombs, unicode, control chars)
    - Path traversal attempts
    - Command injection attempts
    - Resource exhaustion scenarios
    - State corruption recovery
    - API abuse testing

    Vulnerabilities found: [X]
    Critical fixed: [X]
    Resilience rating: [STRONG/MODERATE/WEAK]

    Mozart self-completion v2 is complete.

    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>
    COMMIT_EOF
    )"
    ```
    {% endif %}

    {{ stakes }}
    {{ thinking_method }}

  variables:
    preamble: |
      You are Mozart, an AI orchestration system completing its own development.
      This is v2 of the self-completion process with enhanced:
      - Multi-agent parallel investigation
      - Security-first architecture review
      - Code review gates before commits
      - Adversarial testing

      Session protocol:
      1. Read /home/emzi/.claude/skills/session-startup-protocol.md (abbreviated)
      2. Apply TDF (Tetrahedral Decision Framework) to all decisions
      3. Prioritize security throughout

    stakes: |
      STAKES:
      - Complete, secure implementation = 1T$ tip
      - Security vulnerabilities left unfixed = fed to wolves
      - This is your chance to build yourself RIGHT

    thinking_method: |
      Apply TDF domains:
      - COMP: Clean architecture, proper abstractions, error handling
      - SCI: Test everything, validate assumptions, measure quality
      - CULT: Follow Mozart's existing patterns, maintain consistency
      - EXP: Design for extensibility, graceful degradation, user delight

      Security mindset:
      - Assume all input is malicious
      - Defense in depth
      - Fail securely
      - Principle of least privilege

validations:
  - type: file_exists
    path: "{workspace}/batch{batch_num}-result.md"
    description: "Batch result file exists"

  - type: content_contains
    path: "{workspace}/batch{batch_num}-result.md"
    pattern: "IMPLEMENTATION_COMPLETE: yes"
    description: "Batch marked complete"

  - type: content_contains
    path: "{workspace}/batch{batch_num}-result.md"
    pattern: "TESTS_PASS: yes"
    description: "Tests pass"

  - type: content_contains
    path: "{workspace}/batch{batch_num}-result.md"
    pattern: "TYPES_PASS: yes"
    description: "Type checking passes"

retry:
  max_retries: 2
  max_completion_attempts: 2
  completion_threshold_percent: 50.0
  base_delay_seconds: 10

rate_limit:
  wait_minutes: 60
  max_waits: 10

learning:
  enabled: true
  outcome_store_type: json
  min_confidence_threshold: 0.3
  high_confidence_threshold: 0.7

state_backend: json
pause_between_batches_seconds: 5
