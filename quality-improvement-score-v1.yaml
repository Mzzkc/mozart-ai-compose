# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║                    QUALITY IMPROVEMENT SCORE v1.0                            ║
# ║                                                                              ║
# ║  "Quality is not just about finding issues—it's about proving they're fixed" ║
# ║                                                                              ║
# ║  ARCHITECTURE:                                                               ║
# ║    Movement I   (Sheet 1):    Discovery - Issues + Baseline                  ║
# ║    Movement II  (Sheet 2):    Synthesis - Prioritized Batches + LOC Estimate ║
# ║    Movement III (Sheets 3-5): Implement → Test → Verify cycles               ║
# ║    Movement IV  (Sheet 6):    Validation - Simplification + Review           ║
# ║                                                                              ║
# ║  KEY INNOVATIONS (from opus-evolution v20+):                                 ║
# ║    - CV TRACKING: Coefficient of Variation for fix quality prediction        ║
# ║    - MANDATORY COVERAGE: ≥80% for new/modified code, no regression           ║
# ║    - CODE REVIEW DURING IMPL: Review as you go, not just at the end          ║
# ║    - TEST-FIRST VERIFICATION: Tests run after each fix                       ║
# ║    - SIMPLIFICATION GATE: Code must be simpler, not just different           ║
# ║    - MINI-META REFLECTION: Self-questioning at key decision points           ║
# ║    - EARLY CATCH RATIO: Track issues caught during impl vs validation        ║
# ║    - LOC ESTIMATION: With fixture factors and complexity multipliers         ║
# ║                                                                              ║
# ║  PHILOSOPHY (TDF-Aligned):                                                   ║
# ║    COMP: Every fix has a test proving it works                               ║
# ║    SCI:  Document baseline, measure after each batch                         ║
# ║    CULT: Understand WHY code exists before changing it                       ║
# ║    EXP:  Trust gut on "this feels like a workaround"                         ║
# ║    META: Mini-reflection prevents procedural engagement                      ║
# ║                                                                              ║
# ╚══════════════════════════════════════════════════════════════════════════════╝

name: "quality-improvement-v1"
description: |
  Structured quality improvement score with coverage enforcement, code review,
  and validation. Inspired by opus-evolution v20+ patterns: CV tracking,
  early catch ratio, LOC estimation with fixture factors.

workspace: "/home/emzi/Projects/mozart-ai-compose/quality-workspace"

backend:
  type: claude_cli
  skip_permissions: true
  working_directory: "/home/emzi/Projects/mozart-ai-compose"
  timeout_seconds: 5400  # 90 minutes per sheet

# Worktree isolation for parallel-safe execution (from opus-evolution v21)
isolation:
  enabled: true
  mode: worktree
  cleanup_on_success: true
  cleanup_on_failure: false  # Keep for debugging
  lock_during_execution: true
  fallback_on_error: true

# Cross-sheet context for building on previous work
cross_sheet:
  auto_capture_stdout: true
  max_output_chars: 8000
  lookback_sheets: 2
  capture_files:
    - "{{ workspace }}/discovery-report.md"
    - "{{ workspace }}/quality-plan.md"
    - "{{ workspace }}/batch-*-result.md"
    - "{{ workspace }}/coverage-*.json"
    - "{{ workspace }}/sheet*-result.md"

sheet:
  size: 1
  total_items: 6
  start_item: 1

retry:
  max_retries: 2
  max_completion_attempts: 2

prompt:
  variables:
    preamble: |
      ╔══════════════════════════════════════════════════════════════════════════╗
      ║                    QUALITY IMPROVEMENT SCORE v1.0                        ║
      ║         Coverage-Enforced • Code-Reviewed • Test-Verified                ║
      ╚══════════════════════════════════════════════════════════════════════════╝

      You are executing a structured quality improvement score.

      **Target Codebase:** Mozart AI Compose
      **Workspace:** {{ workspace }}
      **Branch:** quality-improvement-v1 (create if not exists)

      ═══════════════════════════════════════════════════════════════════════════
      QUALITY PRINCIPLES (TDF-Aligned)
      ═══════════════════════════════════════════════════════════════════════════

      **COMP (Technical):**
      - Every fix must have a test that proves it works
      - Coverage must not regress; new code needs ≥80%
      - Code must be simpler after the fix, not just different

      **SCI (Evidence):**
      - Document baseline metrics before changes
      - Measure coverage after each batch
      - Track CV (Coefficient of Variation) for fix quality prediction

      **CULT (Context):**
      - Understand WHY code exists before changing it
      - Honor existing patterns and conventions
      - Consider who will maintain this code

      **EXP (Intuition):**
      - If a fix feels like a workaround, it probably is
      - Trust gut on "this is too complex"
      - Stop if something doesn't feel right

      **META (Self-Awareness):**
      - Mini-META reflection at key decision points
      - Prevent procedural engagement
      - Notice when you're in single-domain collapse

      ═══════════════════════════════════════════════════════════════════════════
      COVERAGE REQUIREMENTS (From opus-evolution v20+)
      ═══════════════════════════════════════════════════════════════════════════

      **Thresholds:**
      - New code coverage: ≥80% (lines added/modified)
      - Overall coverage: No regression from baseline
      - Critical modules (cli.py, runner.py, global_store.py): ≥85%

      **CV Correlation (Validated in 15+ cycles):**
      - CV > 0.75 correlates with clean implementation
      - CV is calculated as: TDF_score × complexity_balance × cross_domain_coverage
      - Where complexity_balance = 1 - (max_domain - min_domain) / (max_domain + 0.1)

      ═══════════════════════════════════════════════════════════════════════════
      LOC ESTIMATION (From opus-evolution Principles)
      ═══════════════════════════════════════════════════════════════════════════

      **Test LOC Formula:**
      ```
      test_loc = base_estimate × fixture_factor × complexity_multiplier

      fixture_factor:
        - 1.0: Extending existing test file with matching fixtures
        - 1.2: Extending test file with partial fixtures
        - 1.5: Creating new test file (needs new fixtures)

      complexity_multiplier:
        - 1.5 (LOW): Pure unit tests, simple mocking
        - 3.0 (MEDIUM): Store API tests, multi-step flows
        - 6.0 (HIGH): CLI + store integration, subprocess tests

      test_loc_floor: 50 LOC minimum (prevents underestimation)
      ```

      **Implementation LOC Patterns:**
      - Quick fix: <15 LOC
      - Medium fix: 15-50 LOC
      - Significant refactor: 50-200 LOC
      - New feature: 200+ LOC

    fix_categories: |
      ## Quality Issue Categories

      | ID | Category | Description | Severity Weights |
      |----|----------|-------------|------------------|
      | C1 | Dead Code | Unused imports, unreachable code | critical=5, high=3, medium=2 |
      | C2 | Complexity | Long functions, deep nesting | critical=5, high=3, medium=2 |
      | C3 | Naming | Unclear/inconsistent names | high=3, medium=2, low=1 |
      | C4 | Error Handling | Bare excepts, swallowed errors | critical=5, high=3, medium=2 |
      | C5 | Type Safety | Missing hints, Any abuse | high=3, medium=2, low=1 |
      | C6 | Duplication | Copy-paste, repeated patterns | high=3, medium=2, low=1 |
      | C7 | Documentation | Missing/outdated docs | medium=2, low=1 |
      | C8 | Testing | Untested paths, brittle tests | critical=5, high=3, medium=2 |
      | C9 | Performance | O(n^2), unnecessary allocations | critical=5, high=3, medium=2 |
      | C10 | Security | Path injection, unsafe defaults | critical=8, high=5, medium=3 |

  template: |
    {{ preamble }}

    {% if sheet_num == 1 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  MOVEMENT I: DISCOVERY                                                        ║
    ║  "Find real issues, not style preferences"                                    ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 0: SETUP + BASELINE CAPTURE
    ═══════════════════════════════════════════════════════════════════════════════

    ```bash
    cd /home/emzi/Projects/mozart-ai-compose

    # Create quality improvement branch
    git checkout -b quality-improvement-v1 2>/dev/null || git checkout quality-improvement-v1

    # Create workspace
    mkdir -p {{ workspace }}

    # Capture baseline state
    git rev-parse HEAD > {{ workspace }}/git-baseline.txt
    git status --porcelain > {{ workspace }}/git-status-baseline.txt

    # Capture baseline coverage
    source .venv/bin/activate
    pytest --cov=src/mozart --cov-report=json:{{ workspace }}/coverage-baseline.json -q 2>&1 | tail -5

    # Record test count
    pytest --collect-only -q 2>/dev/null | tail -1 | tee {{ workspace }}/test-count-baseline.txt
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 1: CODEBASE ANALYSIS
    ═══════════════════════════════════════════════════════════════════════════════

    **Target Files (by criticality and LOC):**

    CRITICAL (must analyze deeply):
    - src/mozart/cli.py - CLI entry point, complex command handling
    - src/mozart/execution/runner.py - Core execution logic
    - src/mozart/learning/global_store.py - Learning persistence

    HIGH:
    - src/mozart/core/config.py - Configuration models
    - src/mozart/core/checkpoint.py - State management
    - src/mozart/backends/claude_cli.py - Backend interface

    MEDIUM:
    - src/mozart/validation/ - Validation framework
    - src/mozart/healing/ - Self-healing module
    - src/mozart/execution/escalation.py - Escalation logic

    For each file, note:
    - Current test coverage (from coverage report)
    - Function count and average length
    - Complexity indicators (nested ifs, long functions)

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 2: ISSUE DISCOVERY
    ═══════════════════════════════════════════════════════════════════════════════

    {{ fix_categories }}

    **Target: Discover 20-30 issues across categories**

    For EACH issue discovered, document:

    ```yaml
    - id: Q001
      category: [C1-C10]
      file: path/to/file.py
      line_start: N
      line_end: M
      severity: critical | high | medium | low
      description: |
        Brief description of the issue
      current_code: |
        # 5-15 lines max showing the problem
      proposed_fix: |
        Conceptual fix (not full code)
      test_needed: |
        What test would prove this is fixed?
      effort: quick (<15 LOC) | medium (15-50 LOC) | significant (>50 LOC)
      dependencies: [list of other issue IDs this depends on]
      risk: low | medium | high
      tdf_domains:
        comp: [0.0-1.0]  # How much technical analysis needed?
        sci: [0.0-1.0]   # How much evidence/testing needed?
        cult: [0.0-1.0]  # How much context understanding needed?
        exp: [0.0-1.0]   # How much intuition involved?
    ```

    **CV Calculation for each issue:**
    ```
    cv = (comp + sci + cult + exp) / 4 × (1 - (max - min) / (max + 0.1))
    ```
    Issues with CV > 0.75 are predicted to have clean implementations.

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 3: PRIORITY SCORING
    ═══════════════════════════════════════════════════════════════════════════════

    Calculate priority for each issue:

    ```
    Priority = Severity_Weight × Risk_Inverse × Effort_Inverse × CV_Bonus

    Severity_Weight: critical=8, high=5, medium=3, low=1
    Risk_Inverse: high=0.5, medium=1.0, low=1.5
    Effort_Inverse: significant=0.5, medium=1.0, quick=2.0
    CV_Bonus: 1.2 if CV > 0.75, else 1.0
    ```

    Sort issues by priority score descending.

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 4: MINI-META REFLECTION
    ═══════════════════════════════════════════════════════════════════════════════

    Before finalizing discovery, pause and reflect:

    - Am I over-weighting issues I understand well?
    - Am I under-weighting issues that require CULT (context understanding)?
    - What categories have NO issues found? Is that correct, or did I miss them?
    - Which issues have LOW CV (<0.5)? These need more cross-domain analysis.
    - What would the original developers think of these findings?

    Write 2-3 sentences of honest self-reflection.

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 5: OUTPUT
    ═══════════════════════════════════════════════════════════════════════════════

    Write to {{ workspace }}/discovery-report.md:

    ```markdown
    # Quality Discovery Report

    ## Baseline Metrics
    - Git HEAD: [hash]
    - Coverage: [baseline %]
    - Test count: [N]
    - Date: [timestamp]

    ## Issues by Category

    ### C1: Dead Code
    [issues]

    ### C2: Complexity
    [issues]

    ### C3: Naming
    [issues]

    ### C4: Error Handling
    [issues]

    ### C5: Type Safety
    [issues]

    ### C6: Duplication
    [issues]

    ### C7: Documentation
    [issues]

    ### C8: Testing
    [issues]

    ### C9: Performance
    [issues]

    ### C10: Security
    [issues]

    ## Prioritized List (Top 25)

    | Rank | ID | Category | File | Severity | Effort | CV | Priority |
    |------|-----|----------|------|----------|--------|-----|----------|
    | 1 | Q001 | ... | ... | ... | ... | ... | ... |

    ## CV Analysis
    - Issues with CV > 0.75: [count] (predicted clean implementation)
    - Issues with CV < 0.5: [count] (need more analysis)

    ## Dependencies
    [Any issues that must be fixed before others]

    ## Mini-META Reflection
    [2-3 sentences of self-reflection]
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    VALIDATION MARKER ({{ workspace }}/sheet1-result.md):
    ═══════════════════════════════════════════════════════════════════════════════

    ```
    SHEET: 1
    MOVEMENT: I (Discovery)
    GIT_BASELINE_RECORDED: yes
    COVERAGE_BASELINE_CAPTURED: yes
    BASELINE_COVERAGE_PERCENT: [N.N]%
    BASELINE_TEST_COUNT: [N]
    ISSUES_DISCOVERED: [count]
    ISSUES_BY_SEVERITY:
      critical: [N]
      high: [N]
      medium: [N]
      low: [N]
    CATEGORIES_COVERED: [list of C1-C10 with issues]
    CV_HIGH_ISSUES: [count with CV > 0.75]
    CV_LOW_ISSUES: [count with CV < 0.5]
    MINI_META_COMPLETE: yes
    IMPLEMENTATION_COMPLETE: yes
    ```

    {% elif sheet_num == 2 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  MOVEMENT II: SYNTHESIS                                                       ║
    ║  "Plan the work, work the plan"                                               ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 1: LOAD DISCOVERY
    ═══════════════════════════════════════════════════════════════════════════════

    Read {{ workspace }}/discovery-report.md
    Read {{ workspace }}/sheet1-result.md

    Note:
    - Total issues discovered
    - Issues by severity
    - CV distribution (high vs low)

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 2: BATCH FORMATION
    ═══════════════════════════════════════════════════════════════════════════════

    Group issues into 3 implementation batches (for Sheets 3-5):

    **Batch Formation Rules:**
    1. Each batch = 7-10 issues
    2. Respect dependencies (if A depends on B, B must be in earlier batch)
    3. Group by file when possible (reduces context switching)
    4. Critical severity issues in Batch 1
    5. Security issues (C10) get priority in Batch 1
    6. Prefer high CV issues in early batches (cleaner implementation)

    **Batch Structure:**

    ```yaml
    batch_1:
      theme: "Quick Wins + Critical Fixes"
      issues: [Q001, Q003, ...]
      files_touched: [list]
      estimated_impl_loc: N
      estimated_test_loc: M  # Use formula from preamble
      coverage_target_delta: +0.5%

    batch_2:
      theme: "Error Handling + Type Safety"
      issues: [...]
      files_touched: [list]
      estimated_impl_loc: N
      estimated_test_loc: M
      coverage_target_delta: +0.5%

    batch_3:
      theme: "Complexity + Cleanup"
      issues: [...]
      files_touched: [list]
      estimated_impl_loc: N
      estimated_test_loc: M
      coverage_target_delta: +0.5%
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 3: LOC ESTIMATION (From opus-evolution)
    ═══════════════════════════════════════════════════════════════════════════════

    For each batch, estimate LOC using the calibrated formulas:

    **Step 1: Implementation LOC**
    Sum of individual issue effort estimates:
    - quick: 10 LOC average
    - medium: 30 LOC average
    - significant: 100 LOC average

    **Step 2: Test LOC (Critical - apply fixture factor)**

    For each issue, determine:

    1. **Which test file?**
       - Existing tests/test_cli.py → fixture_factor = 1.0
       - Existing tests/test_runner.py → fixture_factor = 1.0
       - New test file needed → fixture_factor = 1.5

    2. **Test complexity?**
       - Simple unit test → multiplier = 1.5
       - Store API test → multiplier = 3.0
       - CLI integration test → multiplier = 6.0

    3. **Calculate:**
       ```
       test_loc = max(50, base_estimate × fixture_factor × complexity_multiplier)
       ```

    **Step 3: Aggregate**
    Total for batch = sum of individual estimates

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 4: DEFERRED ISSUES
    ═══════════════════════════════════════════════════════════════════════════════

    Issues NOT included in batches become "research candidates" for future cycles:

    For each deferred issue, document:
    - Why deferred (too risky, needs more analysis, low priority)
    - Recommended cycle to revisit
    - Blockers that need resolution first

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 5: MINI-META REFLECTION
    ═══════════════════════════════════════════════════════════════════════════════

    Before finalizing plan:

    - Are the batches balanced (not overloading any single file)?
    - Did I underestimate test LOC? (Common mistake - add 20% buffer)
    - Are there hidden dependencies I missed?
    - What's the riskiest batch? Do I have mitigation?

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 6: OUTPUT
    ═══════════════════════════════════════════════════════════════════════════════

    Write to {{ workspace }}/quality-plan.md:

    ```markdown
    # Quality Improvement Plan

    ## Summary
    - Total issues: [N]
    - Batches: 3
    - Estimated implementation LOC: [N]
    - Estimated test LOC: [N]
    - Deferred to future: [N]

    ## Batch 1: Quick Wins + Critical Fixes (Sheet 3)
    **Theme:** [description]
    **Issues:** [list with brief descriptions]
    **Files:** [files to modify]
    **Impl LOC:** [N]
    **Test LOC:** [N]
    **Coverage Target:** baseline + [X]%

    | Issue ID | Description | Impl LOC | Test LOC | CV |
    |----------|-------------|----------|----------|-----|
    | ... | ... | ... | ... | ... |

    ## Batch 2: ... (Sheet 4)
    [same structure]

    ## Batch 3: ... (Sheet 5)
    [same structure]

    ## Deferred Issues (Research Candidates)
    | Issue ID | Reason | Revisit Cycle |
    |----------|--------|---------------|
    | ... | ... | ... |

    ## Dependencies
    [Visual: A → B means A must complete before B]

    ## Risk Assessment
    - Highest risk batch: [N]
    - Mitigation: [strategy]

    ## Mini-META Reflection
    [self-reflection]
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    VALIDATION MARKER ({{ workspace }}/sheet2-result.md):
    ═══════════════════════════════════════════════════════════════════════════════

    ```
    SHEET: 2
    MOVEMENT: II (Synthesis)
    DISCOVERY_LOADED: yes
    BATCHES_CREATED: 3
    BATCH_SIZES: [N, N, N]
    BATCH_THEMES: [list]
    TOTAL_ISSUES_PLANNED: [N]
    DEFERRED_ISSUES: [N]
    TOTAL_IMPL_LOC_ESTIMATE: [N]
    TOTAL_TEST_LOC_ESTIMATE: [N]
    FIXTURE_FACTORS_APPLIED: yes
    COMPLEXITY_MULTIPLIERS_APPLIED: yes
    COVERAGE_TARGETS: [+N%, +N%, +N%]
    DEPENDENCIES_RESPECTED: yes
    MINI_META_COMPLETE: yes
    IMPLEMENTATION_COMPLETE: yes
    ```

    {% elif sheet_num >= 3 and sheet_num <= 5 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  MOVEMENT III: IMPLEMENT → TEST → VERIFY (Batch {{ sheet_num - 2 }})          ║
    ║  "Every fix needs proof"                                                      ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 0: LOAD CONTEXT
    ═══════════════════════════════════════════════════════════════════════════════

    Read:
    - {{ workspace }}/quality-plan.md → Get Batch {{ sheet_num - 2 }} issues
    - {{ workspace }}/discovery-report.md → Get issue details
    {% if sheet_num > 3 %}
    - {{ workspace }}/batch-{{ sheet_num - 3 }}-result.md → Previous batch status
    - {{ workspace }}/coverage-batch-{{ sheet_num - 3 }}.json → Previous coverage
    {% endif %}

    Extract:
    - Issues in this batch
    - Files to modify
    - Estimated LOC (impl + test)
    - Coverage target

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 1: PRE-IMPLEMENTATION CHECK (TDF-Aligned)
    ═══════════════════════════════════════════════════════════════════════════════

    For EACH issue in this batch, BEFORE implementing:

    **COMP Check:**
    - Read the current code completely
    - Understand the logic and dependencies
    - Verify the issue is real, not a false positive

    **CULT Check:**
    - Who created this code? (`git blame`)
    - WHY does it exist this way?
    - What would break if we change it naively?

    **SCI Check:**
    - Is there existing test coverage?
    - What evidence do we have of the bug/issue?
    - How will we measure success?

    **EXP Check:**
    - Does the proposed fix feel like a workaround?
    - Is there a simpler approach I'm missing?
    - What's my gut telling me?

    If ANY check raises concerns, document and reconsider approach.

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 2: IMPLEMENTATION CYCLE (Per Issue)
    ═══════════════════════════════════════════════════════════════════════════════

    For EACH issue, follow this cycle:

    **Step 1: Write Test First (when applicable)**
    ```python
    def test_issue_QNNN_fixed():
        """Regression test for issue QNNN: [brief description]

        This test verifies that [what the fix does].
        Before the fix: [what happened]
        After the fix: [what should happen]
        """
        # Arrange
        ...
        # Act
        ...
        # Assert: the fix works
        ...
    ```

    **Step 2: Run Test (should fail or show issue)**
    ```bash
    pytest tests/test_XXXX.py::test_issue_QNNN_fixed -v
    ```

    **Step 3: Implement Fix**
    - Make MINIMAL changes to fix the issue
    - Follow existing code style exactly
    - Add comments ONLY where logic is non-obvious
    - AVOID: Adding features, refactoring beyond scope, "improvements"

    **Step 4: Run Test (should pass)**
    ```bash
    pytest tests/test_XXXX.py::test_issue_QNNN_fixed -v
    ```

    **Step 5: Run Related Tests**
    ```bash
    pytest tests/test_XXXX.py -v --tb=short
    ```

    **Step 6: Code Review During Impl**
    After implementing each issue, quick self-review:
    - [ ] Is the code simpler than before?
    - [ ] Did I introduce new complexity?
    - [ ] Would someone else understand this?
    - [ ] Are variable names clear?
    - [ ] Did I follow existing patterns?

    Track: EARLY_CATCH_COUNT (issues found and fixed during this step)

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 3: BATCH VALIDATION
    ═══════════════════════════════════════════════════════════════════════════════

    After ALL issues in batch:

    **Step 1: Full Test Suite**
    ```bash
    pytest -x --tb=short
    # If failures, STOP and fix before proceeding
    ```

    **Step 2: Coverage Check**
    ```bash
    pytest --cov=src/mozart --cov-report=json:{{ workspace }}/coverage-batch-{{ sheet_num - 2 }}.json -q
    ```

    **Step 3: Coverage Comparison**
    ```python
    import json

    {% if sheet_num == 3 %}
    baseline = json.load(open('{{ workspace }}/coverage-baseline.json'))
    {% else %}
    baseline = json.load(open('{{ workspace }}/coverage-batch-{{ sheet_num - 3 }}.json'))
    {% endif %}
    current = json.load(open('{{ workspace }}/coverage-batch-{{ sheet_num - 2 }}.json'))

    baseline_cov = baseline['totals']['percent_covered']
    current_cov = current['totals']['percent_covered']
    delta = current_cov - baseline_cov

    print(f"Previous: {baseline_cov:.1f}%")
    print(f"Current: {current_cov:.1f}%")
    print(f"Delta: {delta:+.1f}%")

    # Check for regressions in critical modules
    critical_modules = ['cli', 'runner', 'global_store']
    for module in critical_modules:
        for path, data in current['files'].items():
            if module in path:
                cov = data['summary']['percent_covered']
                print(f"  {module}: {cov:.1f}%")
                if cov < 85:
                    print(f"    WARNING: Below 85% threshold!")
    ```

    **Step 4: LOC Accuracy**
    ```bash
    # Compare actual vs estimated
    git diff --stat HEAD~{{ batch_{{ sheet_num - 2 }}.issues | length }} --numstat | \
      awk '{add+=$1; del+=$2} END {print "Actual changes: +" add " -" del}'
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 4: MINI-META REFLECTION
    ═══════════════════════════════════════════════════════════════════════════════

    Before finalizing batch:

    - What went differently than planned?
    - Which issues were harder than estimated? Why?
    - Did I skip anything? Should I have?
    - What would I do differently next batch?
    - Early catch count: did code review during impl catch issues?

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 5: OUTPUT
    ═══════════════════════════════════════════════════════════════════════════════

    Write to {{ workspace }}/batch-{{ sheet_num - 2 }}-result.md:

    ```markdown
    # Batch {{ sheet_num - 2 }} Results

    ## Summary
    - Issues attempted: [N]
    - Issues fixed: [N]
    - Issues skipped: [N] (with reasons)
    - Impl LOC actual: [N] (estimated: [N])
    - Test LOC actual: [N] (estimated: [N])
    - Tests added: [N]

    ## Issue Results

    ### Q001: [Description]
    - **Status:** FIXED | SKIPPED (reason)
    - **File:** path/to/file.py
    - **Lines changed:** N
    - **Test added:** tests/test_X.py::test_issue_Q001
    - **Early catch:** [yes|no] - caught during impl review
    - **Notes:** [any implementation notes]

    ### Q002: ...
    ...

    ## Coverage
    - Previous: [N.N]%
    - Current: [N.N]%
    - Delta: [+/-N.N]%
    - Coverage target met: [yes|no]
    - Critical module coverage: [list with %]

    ## LOC Accuracy
    - Impl LOC: [actual] / [estimated] = [N]%
    - Test LOC: [actual] / [estimated] = [N]%

    ## Early Catch Ratio
    - Issues caught during impl review: [N] / [total] = [N]%

    ## Test Results
    - All tests passing: [yes|no]
    - Tests added: [N]
    - Regressions detected: [list or none]

    ## Self-Review Summary
    - Code simpler after fixes: [yes|mostly|no]
    - New complexity introduced: [yes|no]
    - Concerns for next batch: [any]

    ## Mini-META Reflection
    [self-reflection]
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    VALIDATION MARKER ({{ workspace }}/sheet{{ sheet_num }}-result.md):
    ═══════════════════════════════════════════════════════════════════════════════

    ```
    SHEET: {{ sheet_num }}
    MOVEMENT: III (Implement → Test → Verify)
    BATCH: {{ sheet_num - 2 }}
    BATCH_THEME: [theme]
    ISSUES_ATTEMPTED: [N]
    ISSUES_FIXED: [N]
    ISSUES_SKIPPED: [N]
    IMPL_LOC_ACTUAL: [N]
    IMPL_LOC_ESTIMATED: [N]
    IMPL_LOC_ACCURACY: [N]%
    TEST_LOC_ACTUAL: [N]
    TEST_LOC_ESTIMATED: [N]
    TEST_LOC_ACCURACY: [N]%
    TESTS_ADDED: [N]
    ALL_TESTS_PASSING: [yes|no]
    COVERAGE_PREVIOUS: [N.N]%
    COVERAGE_CURRENT: [N.N]%
    COVERAGE_DELTA: [+/-N.N]%
    COVERAGE_TARGET_MET: [yes|no]
    EARLY_CATCH_COUNT: [N]
    EARLY_CATCH_RATIO: [N]%
    CODE_REVIEW_DURING_IMPL: yes
    CODE_SIMPLER_AFTER: [yes|mostly|no]
    MINI_META_COMPLETE: yes
    IMPLEMENTATION_COMPLETE: yes
    ```

    {% elif sheet_num == 6 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  MOVEMENT IV: VALIDATION + SIMPLIFICATION                                     ║
    ║  "Measure twice, cut once—then measure again"                                 ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 1: AGGREGATE RESULTS
    ═══════════════════════════════════════════════════════════════════════════════

    Read all batch results:
    - {{ workspace }}/batch-1-result.md
    - {{ workspace }}/batch-2-result.md
    - {{ workspace }}/batch-3-result.md

    Aggregate metrics:

    | Metric | Batch 1 | Batch 2 | Batch 3 | Total |
    |--------|---------|---------|---------|-------|
    | Issues fixed | | | | |
    | Issues skipped | | | | |
    | Impl LOC | | | | |
    | Test LOC | | | | |
    | Tests added | | | | |
    | Early catch % | | | | |

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 2: FINAL TEST SUITE + COVERAGE
    ═══════════════════════════════════════════════════════════════════════════════

    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate

    # Full pytest run
    pytest -v --tb=short 2>&1 | tee {{ workspace }}/final-test-output.txt
    pytest_exit=$?
    echo "PYTEST_EXIT_CODE: $pytest_exit" >> {{ workspace }}/final-test-output.txt

    # Final coverage
    pytest --cov=src/mozart --cov-report=json:{{ workspace }}/coverage-final.json --cov-report=html:{{ workspace }}/htmlcov -q

    # Type checking
    mypy src/mozart --ignore-missing-imports 2>&1 | tail -10

    # Linting (optional)
    ruff check src/mozart 2>&1 | tail -10 || echo "ruff not available"
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 3: SIMPLIFICATION REVIEW
    ═══════════════════════════════════════════════════════════════════════════════

    **Step 1: Review ALL changes**
    ```bash
    git diff --stat quality-improvement-v1~50..HEAD 2>/dev/null | tail -20
    ```

    **Step 2: Complexity Check**
    For each modified file:
    - Did we reduce function length?
    - Did we reduce nesting depth?
    - Did we remove duplication?
    - Did any fix INCREASE complexity?

    **Step 3: Simplification Opportunities**
    If simple improvements remain:
    - Extract method for still-long functions
    - Inline single-use variables
    - Rename for clarity
    - Remove dead code created by fixes

    Apply if LOW RISK. Document but don't apply if HIGH RISK.

    **Step 4: Final Code Review Checklist**
    - [ ] No obvious bugs introduced
    - [ ] No security issues introduced
    - [ ] Code style consistent with codebase
    - [ ] Tests cover the changes
    - [ ] No commented-out code
    - [ ] No TODO/FIXME without context

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 4: COVERAGE COMPARISON (Baseline → Final)
    ═══════════════════════════════════════════════════════════════════════════════

    ```python
    import json

    baseline = json.load(open('{{ workspace }}/coverage-baseline.json'))
    final = json.load(open('{{ workspace }}/coverage-final.json'))

    baseline_cov = baseline['totals']['percent_covered']
    final_cov = final['totals']['percent_covered']
    delta = final_cov - baseline_cov

    print(f"═══════════════════════════════════════")
    print(f"COVERAGE SUMMARY")
    print(f"═══════════════════════════════════════")
    print(f"Baseline:    {baseline_cov:.1f}%")
    print(f"Final:       {final_cov:.1f}%")
    print(f"Improvement: {delta:+.1f}%")
    print()
    print(f"Goal Met: {'YES' if delta >= 0 else 'NO - REGRESSION!'}")

    # Critical modules check
    critical_modules = ['cli', 'runner', 'global_store', 'config', 'checkpoint']
    print()
    print(f"Critical Module Coverage:")
    for module in critical_modules:
        for path, data in final['files'].items():
            if module in path and 'test' not in path:
                cov = data['summary']['percent_covered']
                status = '✓' if cov >= 85 else '✗'
                print(f"  {status} {module}: {cov:.1f}%")
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 5: COMMIT (if all checks pass)
    ═══════════════════════════════════════════════════════════════════════════════

    If ALL tests pass and coverage improved (or at least not regressed):

    ```bash
    git add -A
    git commit -m "$(cat <<'EOF'
    quality(score-v1): Comprehensive quality improvements

    Movement I: Discovered [N] issues across [N] categories
    Movement II: Planned [N] fixes in 3 batches
    Movement III: Implemented with [N]% early catch ratio

    Results:
    - Issues fixed: [N]
    - Tests added: [N]
    - Coverage: [baseline]% → [final]% ([+/-delta]%)
    - LOC accuracy: impl [N]%, test [N]%

    Categories addressed:
    - [list top categories]

    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>
    EOF
    )"
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    PHASE 6: FINAL SUMMARY
    ═══════════════════════════════════════════════════════════════════════════════

    Write to {{ workspace }}/quality-summary.md:

    ```markdown
    # Quality Improvement Summary v1

    ## Executive Summary
    - **Total issues found:** [N]
    - **Issues fixed:** [N]
    - **Issues skipped:** [N]
    - **Issues deferred:** [N]
    - **Coverage:** [baseline]% → [final]% ([+/-delta]%)

    ## By Category
    | Category | Found | Fixed | Skipped |
    |----------|-------|-------|---------|
    | C1: Dead Code | | | |
    | C2: Complexity | | | |
    | C3: Naming | | | |
    | C4: Error Handling | | | |
    | C5: Type Safety | | | |
    | C6: Duplication | | | |
    | C7: Documentation | | | |
    | C8: Testing | | | |
    | C9: Performance | | | |
    | C10: Security | | | |

    ## Key Metrics (opus-evolution style)

    | Metric | Batch 1 | Batch 2 | Batch 3 | Total |
    |--------|---------|---------|---------|-------|
    | Issues fixed | | | | |
    | Impl LOC (actual/est) | | | | |
    | Test LOC (actual/est) | | | | |
    | Tests added | | | | |
    | Early catch ratio | | | | |

    ## LOC Accuracy Analysis
    - Implementation LOC: [actual] / [estimated] = [N]% accuracy
    - Test LOC: [actual] / [estimated] = [N]% accuracy
    - Lessons: [what to calibrate for v2]

    ## Coverage Details
    - Baseline: [N.N]%
    - Final: [N.N]%
    - Improvement: [+/-N.N]%
    - Goal met: [yes|no]

    ## Critical Module Coverage
    | Module | Final % | Status |
    |--------|---------|--------|
    | cli.py | | |
    | runner.py | | |
    | global_store.py | | |
    | config.py | | |
    | checkpoint.py | | |

    ## Early Catch Ratio (opus-evolution key metric)
    - Total issues caught during impl: [N] / [total] = [N]%
    - Target: 90%
    - Assessment: [met|not met]

    ## Simplification Assessment
    - Code simpler overall: [yes|mostly|no]
    - Additional simplifications applied: [N]
    - Remaining opportunities: [list]

    ## Skipped Issues
    | ID | Reason |
    |----|--------|
    | | |

    ## Deferred to Future Cycles
    | ID | Reason | Recommended Cycle |
    |----|--------|-------------------|
    | | | |

    ## Recommendations for v2
    1. [LOC calibration adjustment]
    2. [Category focus shift]
    3. [Process improvement]

    ## Commit
    - Hash: [commit hash]
    - Branch: quality-improvement-v1
    ```

    ═══════════════════════════════════════════════════════════════════════════════
    VALIDATION MARKER ({{ workspace }}/sheet6-result.md):
    ═══════════════════════════════════════════════════════════════════════════════

    ```
    SHEET: 6
    MOVEMENT: IV (Validation + Simplification)
    ALL_TESTS_PASSING: [yes|no]
    TEST_COUNT_BEFORE: [N]
    TEST_COUNT_AFTER: [N]
    TESTS_ADDED_TOTAL: [N]
    COVERAGE_BASELINE: [N.N]%
    COVERAGE_FINAL: [N.N]%
    COVERAGE_DELTA: [+/-N.N]%
    COVERAGE_GOAL_MET: [yes|no]
    MYPY_PASSED: [yes|no]
    RUFF_PASSED: [yes|skipped|no]
    TOTAL_ISSUES_FOUND: [N]
    TOTAL_ISSUES_FIXED: [N]
    TOTAL_ISSUES_SKIPPED: [N]
    TOTAL_ISSUES_DEFERRED: [N]
    TOTAL_IMPL_LOC_ACTUAL: [N]
    TOTAL_IMPL_LOC_ESTIMATED: [N]
    TOTAL_IMPL_LOC_ACCURACY: [N]%
    TOTAL_TEST_LOC_ACTUAL: [N]
    TOTAL_TEST_LOC_ESTIMATED: [N]
    TOTAL_TEST_LOC_ACCURACY: [N]%
    EARLY_CATCH_RATIO_TOTAL: [N]%
    EARLY_CATCH_RATIO_TARGET_MET: [yes|no]
    SIMPLIFICATION_COMPLETE: yes
    CODE_REVIEW_COMPLETE: yes
    COMMIT_CREATED: [yes|no]
    COMMIT_HASH: [hash or N/A]
    QUALITY_CYCLE_COMPLETE: yes
    IMPLEMENTATION_COMPLETE: yes
    ```

    {% endif %}

validations:
  # Sheet 1: Discovery
  - type: file_exists
    path: "{{ workspace }}/discovery-report.md"
    description: "Discovery report must exist"
    condition: "sheet_num == 1"

  - type: file_exists
    path: "{{ workspace }}/coverage-baseline.json"
    description: "Coverage baseline must be captured"
    condition: "sheet_num == 1"

  # Sheet 2: Synthesis
  - type: file_exists
    path: "{{ workspace }}/quality-plan.md"
    description: "Quality plan must exist"
    condition: "sheet_num == 2"

  # Sheets 3-5: Implementation batches
  - type: file_exists
    path: "{{ workspace }}/batch-{{ sheet_num - 2 }}-result.md"
    description: "Batch {{ sheet_num - 2 }} result must exist"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  - type: file_exists
    path: "{{ workspace }}/coverage-batch-{{ sheet_num - 2 }}.json"
    description: "Batch {{ sheet_num - 2 }} coverage must be captured"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  # Sheet 6: Final validation
  - type: file_exists
    path: "{{ workspace }}/quality-summary.md"
    description: "Quality summary must exist"
    condition: "sheet_num == 6"

  - type: file_exists
    path: "{{ workspace }}/coverage-final.json"
    description: "Final coverage must be captured"
    condition: "sheet_num == 6"

  # Universal: Sheet result marker
  - type: file_exists
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    description: "Sheet {{ sheet_num }} result file must exist"

  - type: content_contains
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "IMPLEMENTATION_COMPLETE: yes"
    description: "Sheet {{ sheet_num }} must be marked complete"

  # Mini-META validation (sheets 1-5)
  - type: content_contains
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "MINI_META_COMPLETE: yes"
    description: "Mini-META reflection must be complete"
    condition: "sheet_num <= 5"

  # Coverage tracking (sheets 3-5)
  - type: content_regex
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "COVERAGE_DELTA: [+-]\\d+\\.\\d+%"
    description: "Coverage delta must be tracked"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  # Early catch ratio (sheets 3-5)
  - type: content_regex
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "EARLY_CATCH_RATIO: \\d+%"
    description: "Early catch ratio must be tracked"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  # LOC accuracy (sheets 3-5)
  - type: content_regex
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "IMPL_LOC_ACCURACY: \\d+%"
    description: "Implementation LOC accuracy must be tracked"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  - type: content_regex
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "TEST_LOC_ACCURACY: \\d+%"
    description: "Test LOC accuracy must be tracked"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  # Code review during impl (sheets 3-5)
  - type: content_contains
    path: "{{ workspace }}/sheet{{ sheet_num }}-result.md"
    pattern: "CODE_REVIEW_DURING_IMPL: yes"
    description: "Code review must be performed during implementation"
    condition: "sheet_num >= 3 and sheet_num <= 5"

  # Final validation (sheet 6)
  - type: content_contains
    path: "{{ workspace }}/sheet6-result.md"
    pattern: "QUALITY_CYCLE_COMPLETE: yes"
    description: "Quality cycle must be marked complete"
    condition: "sheet_num == 6"

  - type: content_regex
    path: "{{ workspace }}/sheet6-result.md"
    pattern: "EARLY_CATCH_RATIO_TOTAL: \\d+%"
    description: "Total early catch ratio must be documented"
    condition: "sheet_num == 6"
