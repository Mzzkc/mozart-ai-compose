"""Status commands for Mozart CLI.

This module implements job status display commands:
- `mozart status <job-id>` - Show detailed status for a specific job
- `mozart list` (list_jobs) - List all jobs in the workspace

★ Insight ─────────────────────────────────────
1. **Dual output modes**: Both JSON and Rich output are supported for all
   status commands. JSON mode enables scripting and CI/CD integration while
   Rich mode provides a human-friendly interactive experience.

2. **Watch mode implementation**: The status --watch flag uses a polling loop
   with screen clearing. This is simpler than event-driven updates but works
   universally across state backends (JSON files or SQLite).

3. **Circuit breaker inference**: The actual CircuitBreaker is a runtime object
   not persisted to state. We infer its likely state from failure patterns in
   the persisted sheet states to give operators visibility into retry behavior.
─────────────────────────────────────────────────
"""

from __future__ import annotations

import asyncio
import json
from datetime import UTC, datetime
from pathlib import Path
from typing import Any, Literal, TypedDict

import typer
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    TextColumn,
)

from mozart.core.checkpoint import CheckpointState, ErrorRecord, JobStatus, SheetStatus

from ..helpers import (
    ErrorMessages,
    get_last_activity_time,
    require_conductor,
)
from ..helpers import (
    _find_job_state_direct as require_job_state,
)
from ..helpers import (
    _find_job_state_fs as find_job_state,
)
from ..output import (
    StatusColors,
    console,
    create_sheet_details_table,
    create_synthesis_table,
    format_duration,
    format_timestamp,
    format_validation_status,
)
from ..output import (
    infer_error_type as _infer_error_type,
)

# Default circuit breaker failure threshold from CircuitBreakerConfig
_DEFAULT_CB_THRESHOLD: int = 5  # Must match CircuitBreakerConfig.failure_threshold default


class CircuitBreakerInference(TypedDict):
    """Inferred circuit breaker state from job failure patterns."""

    state: Literal["open", "closed"]
    consecutive_failures: int
    reason: str

# =============================================================================
# CLI Commands
# =============================================================================


def status(
    job_id: str = typer.Argument(
        ...,
        help="Job ID to check status for",
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        "-j",
        help="Output status as JSON for machine parsing",
    ),
    watch: bool = typer.Option(
        False,
        "--watch",
        "-W",
        help="Continuously monitor status with live updates",
    ),
    watch_interval: int = typer.Option(
        5,
        "--interval",
        "-i",
        help="Refresh interval in seconds for --watch mode (default: 5)",
    ),
    workspace: Path | None = typer.Option(
        None,
        "--workspace",
        "-w",
        help="Workspace directory to search for job state (debug override)",
        hidden=True,
    ),
) -> None:
    """Show detailed status of a specific job.

    Displays job progress, sheet states, timing information, and any errors.
    Use --json for machine-readable output in scripts.
    Use --watch for continuous monitoring (updates every 5 seconds by default).

    Examples:
        mozart status my-job
        mozart status my-job --json
        mozart status my-job --watch
        mozart status my-job --watch --interval 10
    """
    if watch:
        asyncio.run(_status_job_watch(job_id, json_output, watch_interval, workspace))
    else:
        asyncio.run(_status_job(job_id, json_output, workspace))


def list_jobs(
    all_jobs: bool = typer.Option(
        False,
        "--all",
        "-a",
        help="Show all jobs including completed, failed, and cancelled",
    ),
    status_filter: str | None = typer.Option(
        None,
        "--status",
        "-s",
        help="Filter by job status (queued, running, completed, failed, paused)",
    ),
    limit: int = typer.Option(
        20,
        "--limit",
        "-l",
        help="Maximum number of jobs to display",
    ),
    workspace: Path | None = typer.Option(
        None,
        "--workspace",
        "-w",
        help="Reserved for future per-workspace filtering.",
        hidden=True,
    ),
) -> None:
    """List jobs from the daemon.

    By default shows only active jobs (queued, running, paused).
    Use --all to include completed, failed, and cancelled jobs.
    """
    asyncio.run(_list_jobs(all_jobs, status_filter, limit, workspace))


# =============================================================================
# Async Implementation Functions
# =============================================================================


async def _status_job(
    job_id: str,
    json_output: bool,
    workspace: Path | None,
) -> None:
    """Asynchronously get and display status for a specific job.

    Routes through the conductor by default. Falls back to direct filesystem
    access only if --workspace is explicitly provided (debug override).
    """
    from mozart.daemon.detect import try_daemon_route

    # Try conductor first (unless workspace override is given)
    ws_str = str(workspace) if workspace else None

    params = {"job_id": job_id, "workspace": ws_str}
    try:
        routed, result = await try_daemon_route("job.status", params)
    except Exception:
        # Business logic error from conductor (e.g., job not found).
        # Treat as "job not found" from the conductor.
        if json_output:
            console.print(json.dumps({"error": f"Job not found: {job_id}"}, indent=2))
        else:
            console.print(f"[red]{ErrorMessages.JOB_NOT_FOUND}:[/red] {job_id}")
        raise typer.Exit(1) from None

    if routed and result:
        # Conductor returns CheckpointState when state file exists,
        # or JobMeta dict for queued jobs before first sheet runs.
        try:
            found_job = CheckpointState.model_validate(result)
        except Exception:
            _output_meta_status(result, json_output)
            return
    elif routed and not result:
        # Conductor returned None — shouldn't happen with current protocol,
        # but handle gracefully.
        if json_output:
            console.print(json.dumps({"error": f"Job not found: {job_id}"}, indent=2))
        else:
            console.print(f"[red]{ErrorMessages.JOB_NOT_FOUND}:[/red] {job_id}")
        raise typer.Exit(1)
    else:
        # Conductor not available — require it unless workspace is given
        if workspace is None:
            require_conductor(routed, json_output=json_output)
            return  # unreachable, require_conductor raises
        # Fallback to direct filesystem access with workspace override
        found_job, _ = await require_job_state(
            job_id, workspace, json_output=json_output,
        )

    # Output as JSON if requested
    if json_output:
        _output_status_json(found_job)
        return

    # Display rich status output
    _output_status_rich(found_job)


async def _status_job_watch(
    job_id: str,
    json_output: bool,
    interval: int,
    workspace: Path | None,
) -> None:
    """Continuously monitor job status with live updates.

    Routes through the conductor for each poll. Falls back to direct
    filesystem access only if --workspace is explicitly provided.

    Args:
        job_id: Job ID to monitor.
        json_output: Output as JSON instead of rich formatting.
        interval: Refresh interval in seconds.
        workspace: Optional workspace directory to search.
    """
    from mozart.daemon.detect import try_daemon_route

    console.print(f"[dim]Watching job [bold]{job_id}[/bold] (Ctrl+C to stop)[/dim]\n")

    try:
        while True:
            found_job: CheckpointState | None = None

            # Try conductor first
            ws_str = str(workspace) if workspace else None
            params = {"job_id": job_id, "workspace": ws_str}
            try:
                routed, result = await try_daemon_route("job.status", params)
            except Exception:
                # Business logic error (e.g., job not found) — show not found
                routed, result = True, None

            if routed and result:
                try:
                    found_job = CheckpointState.model_validate(result)
                except Exception:
                    console.clear()
                    _output_meta_status(result, json_output)
                    await asyncio.sleep(interval)
                    continue
            elif not routed and workspace is not None:
                # Fallback to filesystem with workspace override
                found_job, _ = await find_job_state(job_id, workspace)
            elif not routed:
                require_conductor(routed, json_output=json_output)
                return  # unreachable

            # Clear screen and show status
            console.clear()

            if not found_job:
                if json_output:
                    err_msg = f"{ErrorMessages.JOB_NOT_FOUND}: {job_id}"
                    console.print(json.dumps({"error": err_msg}, indent=2))
                else:
                    console.print(f"[red]{ErrorMessages.JOB_NOT_FOUND}:[/red] {job_id}")
            else:
                if json_output:
                    _output_status_json(found_job)
                else:
                    _output_status_rich(found_job)

                # Show watch mode indicator
                now = datetime.now(UTC)
                console.print(
                    f"\n[dim]Last updated: {now.strftime('%H:%M:%S')} "
                    f"| Refreshing every {interval}s | Press Ctrl+C to stop[/dim]"
                )

                # Exit watch mode if job is completed or failed
                if found_job.status in (JobStatus.COMPLETED, JobStatus.FAILED):
                    console.print(
                        f"\n[yellow]Job {found_job.status.value} - exiting watch mode[/yellow]"
                    )
                    break

            await asyncio.sleep(interval)

    except KeyboardInterrupt:
        console.print("\n[dim]Watch mode stopped[/dim]")


_ACTIVE_STATUSES = {"queued", "running", "paused"}

async def _list_jobs(
    all_jobs: bool,
    status_filter: str | None,
    limit: int,
    workspace: Path | None,
) -> None:
    """List jobs from the daemon's persistent registry."""
    from mozart.daemon.detect import try_daemon_route

    _ = workspace  # Reserved for future per-workspace filtering

    routed, result = await try_daemon_route("job.list", {})
    if not routed:
        console.print(
            "[red]Error:[/red] Mozart conductor is not running.\n"
            "Start it with: [bold]mozart start[/bold]"
        )
        raise typer.Exit(1)

    jobs: list[dict[str, Any]] = result if isinstance(result, list) else []

    # Status color map
    _colors: dict[str, str] = {
        "queued": "yellow",
        "running": "green",
        "completed": "bright_green",
        "paused": "yellow",
        "failed": "red",
        "cancelled": "dim",
    }

    # Filter: explicit --status overrides --all, otherwise default to active-only
    if status_filter:
        target = status_filter.lower()
        jobs = [j for j in jobs if str(j.get("status", "")).lower() == target]
    elif not all_jobs:
        jobs = [j for j in jobs if str(j.get("status", "")).lower() in _ACTIVE_STATUSES]

    # Limit
    jobs = jobs[:limit]

    if not jobs:
        if status_filter:
            console.print(f"[dim]No {status_filter} jobs found.[/dim]")
        elif all_jobs:
            console.print("[dim]No jobs found.[/dim]")
        else:
            console.print("[dim]No active jobs.[/dim] Use [bold]--all[/bold] to see job history.")
        return

    # Build rows and compute column widths
    headers = ("JOB ID", "STATUS", "WORKSPACE", "SUBMITTED")
    rows: list[tuple[str, str, str, str]] = []
    for dj in jobs:
        rows.append((
            dj.get("job_id", "?"),
            str(dj.get("status", "unknown")),
            dj.get("workspace", "-"),
            _format_daemon_timestamp(dj.get("submitted_at")),
        ))

    widths = [len(hdr) for hdr in headers]
    for row in rows:
        for i, val in enumerate(row):
            widths[i] = max(widths[i], len(val))

    gap = "   "
    fmt = gap.join(f"{{:<{col_w}}}" for col_w in widths)

    # Header
    console.print(f"[bold]{fmt.format(*headers)}[/bold]", soft_wrap=True)

    # Rows
    for row in rows:
        color = _colors.get(row[1], "white")
        styled = fmt.format(row[0], row[1], row[2], row[3])
        # Apply color to the status portion only
        plain_status = row[1]
        styled = styled.replace(
            plain_status,
            f"[{color}]{plain_status}[/{color}]",
            1,
        )
        console.print(styled, soft_wrap=True)

    console.print(f"\n[dim]{len(rows)} job(s)[/dim]")


def _format_daemon_timestamp(ts: float | None) -> str:
    """Format a Unix timestamp from daemon JobMeta."""
    if ts is None:
        return "-"
    return datetime.fromtimestamp(ts, tz=UTC).strftime("%Y-%m-%d %H:%M")


# =============================================================================
# Output Formatters
# =============================================================================


def _output_meta_status(meta: dict[str, Any], json_output: bool) -> None:
    """Render basic status from JobMeta when full CheckpointState is unavailable.

    This happens when the conductor has the job registered but the state file
    hasn't been written yet (job just started) or the workspace path doesn't
    resolve to a loadable state backend.
    """
    if json_output:
        console.print(json.dumps(meta, indent=2, default=str))
        return

    job_id = meta.get("job_id", "unknown")
    status = meta.get("status", "unknown")
    color = {"running": "green", "queued": "blue", "failed": "red"}.get(status, "yellow")

    console.print(Panel(
        f"[bold]{job_id}[/bold]\n"
        f"ID: {job_id}\n"
        f"Status: [{color}]{status.upper()}[/{color}]",
        title="Job Status",
        border_style=color,
    ))

    submitted = meta.get("submitted_at")
    config_path = meta.get("config_path", "-")

    console.print(f"\n  Config: {config_path}")
    if submitted:
        console.print(f"  Submitted: {_format_daemon_timestamp(submitted)}")

    console.print(
        "\n[dim]Full status unavailable — state file not yet written "
        "or workspace path mismatch.[/dim]"
    )


def _output_status_json(job: CheckpointState) -> None:
    """Output job status as JSON."""
    # Build a clean JSON representation
    # Use last_completed_sheet for progress since it's more reliable than counting sheets dict
    completed = job.last_completed_sheet
    total = job.total_sheets
    percent = (completed / total * 100) if total > 0 else 0.0

    # Collect recent errors for JSON output
    recent_errors_data: list[dict[str, Any]] = []
    for sheet_num, error in _collect_recent_errors(job, limit=5):
        recent_errors_data.append({
            "sheet_num": sheet_num,
            "timestamp": error.timestamp.isoformat() if error.timestamp else None,
            "error_type": error.error_type,
            "error_code": error.error_code,
            "error_message": error.error_message,
        })

    # Get last activity time
    last_activity = get_last_activity_time(job)

    # Infer circuit breaker state
    cb_state = _infer_circuit_breaker_state(job)

    # Build per-sheet JSON with enhanced fields
    sheets_json: dict[str, dict[str, Any]] = {}
    for num, sheet in job.sheets.items():
        sheet_data: dict[str, Any] = {
            "status": sheet.status.value,
            "attempt_count": sheet.attempt_count,
            "validation_passed": sheet.validation_passed,
            "error_message": sheet.error_message,
            "error_category": sheet.error_category,
        }
        if sheet.execution_duration_seconds is not None:
            sheet_data["execution_duration_seconds"] = sheet.execution_duration_seconds
        if sheet.estimated_cost is not None:
            sheet_data["estimated_cost"] = sheet.estimated_cost
        if sheet.status == SheetStatus.IN_PROGRESS and sheet.started_at:
            elapsed = (datetime.now(UTC) - sheet.started_at).total_seconds()
            sheet_data["elapsed_seconds"] = round(elapsed, 1)
        if sheet.progress_snapshots:
            sheet_data["progress_snapshots"] = sheet.progress_snapshots
        if sheet.last_activity_at:
            sheet_data["last_activity_at"] = sheet.last_activity_at.isoformat()
        sheets_json[str(num)] = sheet_data

    output = {
        "job_id": job.job_id,
        "job_name": job.job_name,
        "status": job.status.value,
        "progress": {
            "completed": completed,
            "total": total,
            "percent": round(percent, 1),
        },
        "timing": {
            "created_at": job.created_at.isoformat() if job.created_at else None,
            "started_at": job.started_at.isoformat() if job.started_at else None,
            "updated_at": job.updated_at.isoformat() if job.updated_at else None,
            "completed_at": job.completed_at.isoformat() if job.completed_at else None,
            "last_activity": last_activity.isoformat() if last_activity else None,
        },
        "execution": {
            "current_sheet": job.current_sheet,
            "total_retry_count": job.total_retry_count,
            "rate_limit_waits": job.rate_limit_waits,
        },
        "cost": {
            "total_estimated_cost": job.total_estimated_cost,
            "total_input_tokens": job.total_input_tokens,
            "total_output_tokens": job.total_output_tokens,
            "cost_limit_reached": job.cost_limit_reached,
        },
        "circuit_breaker": cb_state,
        "hook_results_count": len(job.hook_results),
        "hook_failures": [
            {
                "hook_name": h.get("hook_name", h.get("name", "unknown")),
                "event": h.get("event"),
                "error": h.get("error", h.get("error_message", "")),
            }
            for h in job.hook_results
            if not h.get("success", False)
        ],
        "recent_errors": recent_errors_data,
        "error": job.error_message,
        "sheets": sheets_json,
    }
    console.print(json.dumps(output, indent=2))


def _render_sheet_details(job: CheckpointState) -> None:
    """Render the sheet details table for rich status output.

    Shows elapsed time for in-progress sheets and duration for completed ones.
    When the job config includes sheet descriptions (GH#75), a Description
    column is added between the sheet number and status columns.
    """
    if not job.sheets:
        return

    # Extract descriptions from config snapshot (GH#75)
    descriptions: dict[int, str] = {}
    if job.config_snapshot:
        sheet_cfg = job.config_snapshot.get("sheet", {})
        raw_descs = sheet_cfg.get("descriptions", {})
        # Keys may be strings after JSON round-trip
        descriptions = {int(k): v for k, v in raw_descs.items()}

    has_descriptions = bool(descriptions)

    console.print("\n[bold]Sheet Details[/bold]")
    sheet_table = create_sheet_details_table(has_descriptions=has_descriptions)

    for sheet_num in sorted(job.sheets.keys()):
        sheet = job.sheets[sheet_num]
        sheet_color = StatusColors.get_sheet_color(sheet.status)
        val_str = format_validation_status(sheet.validation_passed)

        # Build status string with elapsed time for in-progress sheets
        status_str = f"[{sheet_color}]{sheet.status.value}[/{sheet_color}]"
        if sheet.status == SheetStatus.IN_PROGRESS and sheet.started_at:
            elapsed = datetime.now(UTC) - sheet.started_at
            status_str += f" [dim]({format_duration(elapsed.total_seconds())})[/dim]"
        elif sheet.execution_duration_seconds is not None:
            status_str += f" [dim]({format_duration(sheet.execution_duration_seconds)})[/dim]"

        error_str = ""
        if sheet.error_message:
            error_str = sheet.error_message[:50]
            if len(sheet.error_message) > 50:
                error_str += "..."

        row: list[str] = [str(sheet_num)]
        if has_descriptions:
            row.append(descriptions.get(sheet_num, ""))
        row.extend([status_str, str(sheet.attempt_count), val_str, error_str])

        sheet_table.add_row(*row)

    console.print(sheet_table)


def _render_recent_errors(job: CheckpointState) -> None:
    """Render the recent errors section with error source identity.

    Each error line includes sheet number, attempt number, and backend
    context (if available) so operators can trace exactly where the
    error occurred.
    """
    recent_errors = _collect_recent_errors(job, limit=3)
    if not recent_errors:
        return

    console.print("\n[bold red]Recent Errors[/bold red]")
    for sheet_num, error in recent_errors:
        type_style = StatusColors.get_error_color(error.error_type)

        message = error.error_message or ""
        if len(message) > 60:
            message = message[:57] + "..."

        # Build source identity: sheet + attempt + backend (if known)
        source_parts = [f"Sheet {sheet_num}", f"attempt {error.attempt_number}"]
        backend = error.context.get("backend") if error.context else None
        if backend:
            source_parts.append(str(backend))
        source_str = ", ".join(source_parts)

        console.print(
            f"  [{type_style}]\u2022[/{type_style}] [{type_style}]{error.error_code}[/{type_style}]"
            f" [dim]({source_str})[/dim] - {message}"
        )

    console.print(
        f"\n[dim]  Use 'mozart errors {job.job_id}' for complete error history[/dim]"
    )


def _render_synthesis_results(job: CheckpointState) -> None:
    """Render the synthesis results table for rich status output."""
    if not job.synthesis_results:
        return

    console.print("\n[bold]Synthesis Results[/bold]")
    synth_table = create_synthesis_table()

    for batch_id, result in job.synthesis_results.items():
        sheets = result.get("sheets", [])
        sheets_str = ", ".join(str(s) for s in sheets[:4])
        if len(sheets) > 4:
            sheets_str += f" (+{len(sheets) - 4})"

        strategy = result.get("strategy", "merge")
        synth_status = result.get("status", "pending")
        synth_status_color = StatusColors.SYNTHESIS_STATUS.get(synth_status, "white")

        synth_table.add_row(
            batch_id[:12],
            sheets_str,
            strategy,
            f"[{synth_status_color}]{synth_status}[/{synth_status_color}]",
        )

    console.print(synth_table)


def _render_cost_summary(job: CheckpointState) -> None:
    """Render cost tracking summary if any cost data is available."""
    has_job_cost = job.total_estimated_cost > 0
    has_sheet_cost = any(
        s.estimated_cost is not None and s.estimated_cost > 0
        for s in job.sheets.values()
    )
    if not has_job_cost and not has_sheet_cost:
        return

    # Extract cost limit from config snapshot if available
    cost_limit: float | None = None
    if job.config_snapshot:
        cost_limits_cfg = job.config_snapshot.get("cost_limits", {})
        if isinstance(cost_limits_cfg, dict):
            cost_limit = cost_limits_cfg.get("max_cost_per_job")

    limit_str = f"(limit: ${cost_limit:.2f})" if cost_limit else "(no limit)"

    console.print("\n[bold]Cost Summary[/bold]")
    if has_job_cost:
        console.print(
            f"  Cost: [yellow]${job.total_estimated_cost:.2f}[/yellow] {limit_str}"
        )
        console.print(f"  Input tokens:  {job.total_input_tokens:,}")
        console.print(f"  Output tokens: {job.total_output_tokens:,}")
        if job.cost_limit_reached:
            console.print("  [red]Cost limit reached — job was paused[/red]")
    elif has_sheet_cost:
        # Sum from individual sheets if job-level totals aren't populated
        total = sum(s.estimated_cost for s in job.sheets.values() if s.estimated_cost)
        console.print(f"  Cost: [yellow]${total:.2f}[/yellow] {limit_str} (from sheets)")


def _render_hook_results(job: CheckpointState) -> None:
    """Render hook execution results if any are recorded."""
    if not job.hook_results:
        return

    console.print("\n[bold]Hook Results[/bold]")

    # Show summary counts
    passed = sum(1 for h in job.hook_results if h.get("success", False))
    failed = len(job.hook_results) - passed

    console.print(f"  Total: {len(job.hook_results)} | "
                  f"[green]Passed: {passed}[/green] | "
                  f"[red]Failed: {failed}[/red]")

    # Show details for failed hooks (most useful for diagnostics)
    failed_hooks = [hr for hr in job.hook_results if not hr.get("success", False)]
    for hook in failed_hooks[-3:]:  # Last 3 failures
        hook_name = hook.get("hook_name", hook.get("name", "unknown"))
        event = hook.get("event", "?")
        error = hook.get("error", hook.get("error_message", ""))
        if len(error) > 60:
            error = error[:57] + "..."
        console.print(f"  [red]\u2022[/red] {hook_name} ({event}): {error}")


def _render_progress_snapshots(job: CheckpointState) -> None:
    """Render progress snapshots for any in-progress sheets.

    Shows real-time execution progress: bytes received, lines processed,
    and current phase. Only displays for sheets currently executing.
    """
    for sheet_num in sorted(job.sheets.keys()):
        sheet = job.sheets[sheet_num]
        if sheet.status != SheetStatus.IN_PROGRESS:
            continue
        if not sheet.progress_snapshots:
            continue

        console.print(f"\n[bold]Live Progress — Sheet {sheet_num}[/bold]")
        latest = sheet.progress_snapshots[-1]

        phase = latest.get("phase", "unknown")
        phase_color = {"starting": "yellow", "executing": "blue", "completed": "green"}.get(
            phase, "white"
        )
        console.print(f"  Phase: [{phase_color}]{phase}[/{phase_color}]")

        bytes_recv = latest.get("bytes_received")
        if bytes_recv is not None:
            from mozart.cli.output import format_bytes
            console.print(f"  Output received: {format_bytes(bytes_recv)}")

        lines = latest.get("lines_received")
        if lines is not None:
            console.print(f"  Lines received: {lines:,}")

        elapsed = latest.get("elapsed_seconds")
        if elapsed is not None:
            console.print(f"  Elapsed: {format_duration(elapsed)}")

        if sheet.last_activity_at:
            console.print(f"  Last activity: {format_timestamp(sheet.last_activity_at)}")

        snap_count = len(sheet.progress_snapshots)
        if snap_count > 1:
            console.print(f"  [dim]({snap_count} snapshots captured)[/dim]")


def _output_status_rich(job: CheckpointState) -> None:
    """Output job status with rich formatting."""
    status_color = StatusColors.get_job_color(job.status)

    header_lines = [
        f"[bold]{job.job_name}[/bold]",
        f"ID: [cyan]{job.job_id}[/cyan]",
        f"Status: [{status_color}]{job.status.value.upper()}[/{status_color}]",
    ]

    if job.started_at:
        if job.completed_at:
            duration = job.completed_at - job.started_at
            header_lines.append(f"Duration: {format_duration(duration.total_seconds())}")
        elif job.status == JobStatus.RUNNING and job.updated_at:
            elapsed = datetime.now(UTC) - job.started_at
            header_lines.append(f"Running for: {format_duration(elapsed.total_seconds())}")

    console.print(Panel("\n".join(header_lines), title="Job Status"))

    # Progress bar
    console.print("\n[bold]Progress[/bold]")
    with Progress(
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=40),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("({task.completed}/{task.total})"),
        console=console,
        transient=False,
    ) as progress:
        progress.add_task("Sheets", total=job.total_sheets, completed=job.last_completed_sheet)
        progress.refresh()

    # Parallel execution info
    if job.parallel_enabled:
        console.print("\n[bold]Parallel Execution[/bold]")
        console.print("  Mode: [cyan]Enabled[/cyan]")
        console.print(f"  Max concurrent: {job.parallel_max_concurrent}")
        console.print(f"  Batches executed: {job.parallel_batches_executed}")
        if job.sheets_in_progress:
            in_progress_str = ", ".join(str(s) for s in job.sheets_in_progress)
            console.print(f"  Currently running: [blue]{in_progress_str}[/blue]")

    _render_synthesis_results(job)
    _render_sheet_details(job)

    # Progress snapshots for in-progress sheets
    _render_progress_snapshots(job)

    # Error/info message
    if job.error_message:
        if "Recovered from stale" in job.error_message or "ready to resume" in job.error_message:
            console.print(f"\n[bold cyan]Note:[/bold cyan] {job.error_message}")
        else:
            console.print(f"\n[bold red]Error:[/bold red] {job.error_message}")

    # Timing info
    console.print("\n[bold]Timing[/bold]")
    if job.created_at:
        console.print(f"  Created:  {format_timestamp(job.created_at)}")
    if job.started_at:
        console.print(f"  Started:  {format_timestamp(job.started_at)}")
    if job.updated_at:
        console.print(f"  Updated:  {format_timestamp(job.updated_at)}")
    if job.completed_at:
        console.print(f"  Completed: {format_timestamp(job.completed_at)}")

    # Execution stats
    quota_waits = getattr(job, "quota_waits", 0)
    if job.total_retry_count > 0 or job.rate_limit_waits > 0 or quota_waits > 0:
        console.print("\n[bold]Execution Stats[/bold]")
        console.print(f"  Total retries: {job.total_retry_count}")
        console.print(f"  Rate limit waits: {job.rate_limit_waits}")
        if quota_waits > 0:
            console.print(f"  Quota exhaustion waits: {quota_waits}")

    # Cost summary
    _render_cost_summary(job)

    # Hook execution results
    _render_hook_results(job)

    _render_recent_errors(job)

    # Last activity timestamp
    last_activity = get_last_activity_time(job)
    if last_activity:
        console.print("\n[bold]Last Activity[/bold]")
        console.print(f"  {format_timestamp(last_activity)}")

    # Circuit breaker state indicator
    cb_state = _infer_circuit_breaker_state(job)
    if cb_state:
        cb_color = {"open": "red", "half_open": "yellow", "closed": "green"}.get(
            cb_state["state"], "white"
        )
        reason = cb_state.get("reason", "")
        cb_source = "persisted" if reason.startswith("Persisted:") else "inferred"
        console.print(f"\n[bold]Circuit Breaker ({cb_source})[/bold]")
        console.print(f"  State: [{cb_color}]{cb_state['state'].upper()}[/{cb_color}]")
        console.print(f"  Consecutive failures: {cb_state['consecutive_failures']}")
        if reason:
            console.print(f"  [dim]{reason}[/dim]")


# =============================================================================
# Helper Functions
# =============================================================================


def _collect_recent_errors(
    job: CheckpointState,
    limit: int = 3,
) -> list[tuple[int, ErrorRecord]]:
    """Collect the most recent errors from sheet states.

    Args:
        job: CheckpointState to collect errors from.
        limit: Maximum number of errors to return.

    Returns:
        List of (sheet_num, ErrorRecord) tuples, sorted by timestamp descending.
    """
    all_errors: list[tuple[int, ErrorRecord]] = []

    for sheet_num, sheet in job.sheets.items():
        # Collect from error_history
        for error in sheet.error_history:
            all_errors.append((sheet_num, error))

        # If no history but has error_message, create synthetic record
        if not sheet.error_history and sheet.error_message:
            synthetic = ErrorRecord(
                error_type=_infer_error_type(sheet.error_category),
                error_code=sheet.error_category or "E999",
                error_message=sheet.error_message,
                attempt_number=sheet.attempt_count,
                stdout_tail=sheet.stdout_tail,
                stderr_tail=sheet.stderr_tail,
                context={
                    "exit_code": sheet.exit_code,
                    "exit_signal": sheet.exit_signal,
                    "exit_reason": sheet.exit_reason,
                },
            )
            if sheet.completed_at:
                synthetic.timestamp = sheet.completed_at
            all_errors.append((sheet_num, synthetic))

    # Sort by timestamp (most recent first) and take limit
    all_errors.sort(key=lambda x: x[1].timestamp, reverse=True)
    return all_errors[:limit]


def _infer_circuit_breaker_state(job: CheckpointState) -> CircuitBreakerInference | None:
    """Get circuit breaker state from persisted history, falling back to inference.

    Prefers ground-truth from ``circuit_breaker_history`` (populated when
    the runner records CB transitions at runtime). Falls back to the legacy
    heuristic for state files that predate CB persistence.

    Args:
        job: CheckpointState to analyze.

    Returns:
        CircuitBreakerInference with state info, or None if no relevant data.
    """
    # --- Ground truth: persisted circuit breaker history ---
    cb_history = getattr(job, "circuit_breaker_history", None)
    if cb_history:
        latest = cb_history[-1]
        return CircuitBreakerInference(
            state=latest.get("state", "closed"),
            consecutive_failures=latest.get("consecutive_failures", 0),
            reason=f"Persisted: {latest.get('trigger', 'unknown')}",
        )

    # --- Backward-compat fallback: infer from failure patterns ---
    if not job.sheets:
        return None

    # Count consecutive failures from the end
    sorted_sheets = sorted(job.sheets.items(), key=lambda x: x[0], reverse=True)
    consecutive_failures = 0

    for _sheet_num, sheet in sorted_sheets:
        if sheet.status == SheetStatus.FAILED:
            consecutive_failures += 1
        elif sheet.status == SheetStatus.COMPLETED:
            break
        # PENDING/IN_PROGRESS don't count

    if consecutive_failures == 0:
        return None  # No failures, circuit likely closed, nothing special to show

    threshold = _DEFAULT_CB_THRESHOLD

    if consecutive_failures >= threshold:
        return CircuitBreakerInference(
            state="open",
            consecutive_failures=consecutive_failures,
            reason=f"\u2265{threshold} consecutive failures detected (inferred)",
        )
    return CircuitBreakerInference(
        state="closed",
        consecutive_failures=consecutive_failures,
        reason=f"Under threshold ({consecutive_failures}/{threshold}) (inferred)",
    )


# =============================================================================
# Public API
# =============================================================================

__all__ = [
    "status",
    "list_jobs",
]
