"""Job manager for the Mozart daemon.

Maps job IDs to asyncio.Tasks, enforces concurrency limits via semaphore,
routes IPC requests to JobService, and cancels all tasks on shutdown.
"""

from __future__ import annotations

import asyncio
import os
import time
import traceback
from collections import deque
from collections.abc import Coroutine
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import yaml

import mozart
from mozart.core.checkpoint import CheckpointState
from mozart.core.logging import get_logger
from mozart.daemon.backpressure import BackpressureController
from mozart.daemon.config import DaemonConfig
from mozart.daemon.event_bus import EventBus
from mozart.daemon.exceptions import DaemonError, JobSubmissionError
from mozart.daemon.job_service import JobService
from mozart.daemon.learning_hub import LearningHub
from mozart.daemon.monitor import ResourceMonitor
from mozart.daemon.observer import JobObserver
from mozart.daemon.output import StructuredOutput
from mozart.daemon.rate_coordinator import RateLimitCoordinator
from mozart.daemon.registry import DaemonJobStatus, JobRecord, JobRegistry
from mozart.daemon.scheduler import GlobalSheetScheduler
from mozart.daemon.snapshot import SnapshotManager
from mozart.daemon.task_utils import log_task_exception
from mozart.daemon.types import JobRequest, JobResponse, ObserverEvent

_logger = get_logger("daemon.manager")


@dataclass
class JobMeta:
    """Metadata tracked per job in the manager."""

    job_id: str
    config_path: Path
    workspace: Path
    submitted_at: float = field(default_factory=time.time)
    started_at: float | None = None
    status: DaemonJobStatus = DaemonJobStatus.QUEUED
    error_message: str | None = None
    error_traceback: str | None = None
    chain_depth: int | None = None
    observer: JobObserver | None = field(default=None, repr=False)

    def to_dict(self) -> dict[str, Any]:
        """Serialize to a dict suitable for JSON-RPC responses."""
        result: dict[str, Any] = {
            "job_id": self.job_id,
            "status": self.status,
            "config_path": str(self.config_path),
            "workspace": str(self.workspace),
            "submitted_at": self.submitted_at,
            "started_at": self.started_at,
        }
        if self.error_message:
            result["error_message"] = self.error_message
        if self.error_traceback:
            result["error_traceback"] = self.error_traceback
        if self.chain_depth is not None:
            result["chain_depth"] = self.chain_depth
        return result


class JobManager:
    """Manages concurrent job execution within the daemon.

    Wraps JobService with task tracking and concurrency control.
    Each submitted job becomes an asyncio.Task that the manager
    tracks from start to completion/cancellation.
    """

    def __init__(
        self,
        config: DaemonConfig,
        *,
        start_time: float | None = None,
        monitor: ResourceMonitor | None = None,
    ) -> None:
        self._config = config
        self._start_time = start_time or time.monotonic()

        # Phase 3: Centralized learning hub.
        # Single GlobalLearningStore shared across all jobs — pattern
        # discoveries in Job A are instantly available to Job B.
        self._learning_hub = LearningHub()

        # Deferred to start() where the learning hub's store is available.
        self._service: JobService | None = None
        self._jobs: dict[str, asyncio.Task[Any]] = {}
        self._job_meta: dict[str, JobMeta] = {}
        # Live CheckpointState per running job — populated by
        # _PublishingBackend on every state_backend.save() so the
        # conductor can serve status from memory, not disk.
        self._live_states: dict[str, CheckpointState] = {}
        self._concurrency_semaphore = asyncio.Semaphore(
            config.max_concurrent_jobs,
        )
        self._shutting_down = False
        self._shutdown_event = asyncio.Event()
        self._recent_failures: deque[float] = deque()

        # Phase 3: Global sheet scheduler — lazily initialized via property.
        # Infrastructure is built and tested but not yet wired into the
        # execution path.  Currently, jobs run monolithically via
        # JobService.start_job().  Lazy init avoids allocating resources
        # until Phase 3 is actually wired.
        self._scheduler_instance: GlobalSheetScheduler | None = None

        # Phase 3: Cross-job rate limit coordination.
        # Built and tested; wired into the scheduler so next_sheet()
        # skips rate-limited backends.  Not yet active because the
        # scheduler itself is not yet driving execution.
        self._rate_coordinator = RateLimitCoordinator()

        # Phase 3: Backpressure controller.
        # Uses a single ResourceMonitor instance shared with DaemonProcess
        # for both periodic monitoring and point-in-time backpressure checks.
        # When no monitor is injected (e.g. unit tests), a standalone one
        # is created that only does point-in-time reads.
        self._monitor = monitor or ResourceMonitor(config.resource_limits, manager=self)
        self._backpressure = BackpressureController(
            self._monitor, self._rate_coordinator,
        )

        # Persistent job registry — survives daemon restarts.
        db_path = config.state_db_path.expanduser()
        self._registry = JobRegistry(db_path)

        # Event bus for routing runner and observer events to consumers.
        self._event_bus = EventBus(
            max_queue_size=config.observer.max_queue_size,
        )

        # Phase 4: Completion snapshots — captures workspace artifacts
        # at job completion with TTL-based cleanup.
        self._snapshot_manager = SnapshotManager()

    # ─── Lifecycle ────────────────────────────────────────────────────

    async def start(self) -> None:
        """Start daemon subsystems (learning hub, monitor, etc.)."""
        # Open the async registry connection (tables + WAL mode)
        await self._registry.open()

        # Recover orphaned jobs (left running/queued from previous daemon).
        # Pause-aware: check each orphan's checkpoint to distinguish truly
        # running jobs from those that were mid-pause when the daemon died.
        orphans = await self._registry.get_orphaned_jobs()
        if orphans:
            failed_count = 0
            paused_count = 0
            for orphan in orphans:
                target_status = self._classify_orphan(orphan)
                await self._registry.update_status(
                    orphan.job_id,
                    target_status,
                    error_message=(
                        "Daemon restarted while job was active"
                        if target_status == DaemonJobStatus.FAILED
                        else None
                    ),
                )
                if target_status == DaemonJobStatus.PAUSED:
                    paused_count += 1
                else:
                    failed_count += 1
            _logger.info(
                "manager.orphans_recovered",
                count=len(orphans),
                failed=failed_count,
                paused=paused_count,
            )

        await self._learning_hub.start()
        await self._event_bus.start()
        # Create service with shared store now that the hub is initialized
        self._service = JobService(
            output=StructuredOutput(event_bus=self._event_bus),
            global_learning_store=self._learning_hub.store,
            rate_limit_callback=self._on_rate_limit,
            event_callback=self._on_event,
            state_publish_callback=self._on_state_published,
        )
        _logger.info(
            "manager.started",
            scheduler_status="lazy_not_wired",
            scheduler_note="Phase 3 scheduler is lazily initialized and not "
            "yet driving execution. Jobs run monolithically via JobService.",
        )

    @property
    def _scheduler(self) -> GlobalSheetScheduler:
        """Lazily create the Phase 3 scheduler on first access."""
        if self._scheduler_instance is None:
            self._scheduler_instance = GlobalSheetScheduler(self._config)
            self._scheduler_instance.set_rate_limiter(self._rate_coordinator)
            self._scheduler_instance.set_backpressure(self._backpressure)
        return self._scheduler_instance

    @property
    def _checked_service(self) -> JobService:
        """Get the job service, raising if not yet started."""
        if self._service is None:
            raise RuntimeError("JobManager not started — call start() first")
        return self._service

    def apply_config(self, new_config: DaemonConfig) -> None:
        """Hot-apply reloadable config fields from a SIGHUP reload.

        Compares the new config against the current one and applies
        changes that can be safely updated at runtime.  Rebuilds the
        concurrency semaphore if ``max_concurrent_jobs`` changed.

        Safe because asyncio is single-threaded — this runs in the
        event loop, so no concurrent access to ``_config`` or
        ``_concurrency_semaphore`` is possible.
        """
        old = self._config

        # Rebuild semaphore if concurrency limit changed
        if new_config.max_concurrent_jobs != old.max_concurrent_jobs:
            _logger.info(
                "manager.config_reloaded",
                field="max_concurrent_jobs",
                old_value=old.max_concurrent_jobs,
                new_value=new_config.max_concurrent_jobs,
            )
            self._concurrency_semaphore = asyncio.Semaphore(
                new_config.max_concurrent_jobs,
            )

        # Log other changed reloadable fields
        _reloadable_fields = [
            "job_timeout_seconds",
            "shutdown_timeout_seconds",
            "max_job_history",
            "monitor_interval_seconds",
        ]
        for field_name in _reloadable_fields:
            old_val = getattr(old, field_name)
            new_val = getattr(new_config, field_name)
            if old_val != new_val:
                _logger.info(
                    "manager.config_reloaded",
                    field=field_name,
                    old_value=old_val,
                    new_value=new_val,
                )

        self._config = new_config

    # ─── Helpers ───────────────────────────────────────────────────────

    @staticmethod
    def _classify_orphan(orphan: JobRecord) -> DaemonJobStatus:
        """Determine the correct recovery status for an orphaned job.

        Checks the job's checkpoint file to see if it was paused at the time
        the daemon died. Jobs that were paused should stay paused (resumable)
        rather than being marked as failed.
        """
        import json

        try:
            workspace = Path(orphan.workspace)
            # Check for checkpoint file in workspace
            safe_id = "".join(
                c if c.isalnum() or c in "-_" else "_" for c in orphan.job_id
            )
            state_file = workspace / f"{safe_id}.json"
            if state_file.exists():
                data = json.loads(state_file.read_text(encoding="utf-8"))
                if data.get("status") == "paused":
                    return DaemonJobStatus.PAUSED
        except (OSError, json.JSONDecodeError, ValueError):
            _logger.warning(
                "manager.orphan_classify_failed",
                job_id=orphan.job_id,
                workspace=orphan.workspace,
                exc_info=True,
            )
        return DaemonJobStatus.FAILED

    async def _generate_job_id(self, base_name: str) -> str:
        """Generate a human-friendly job ID from config file stem.

        Uses the name directly (e.g. ``quality-continuous``). If a job
        with that name is already active (in-memory or registry),
        appends ``-2``, ``-3``, etc.
        """
        async def _is_active(name: str) -> bool:
            return name in self._job_meta or await self._registry.has_active_job(name)

        if not await _is_active(base_name):
            return base_name
        suffix = 2
        while await _is_active(f"{base_name}-{suffix}"):
            suffix += 1
        return f"{base_name}-{suffix}"

    # ─── RPC Handlers ─────────────────────────────────────────────────

    async def submit_job(self, request: JobRequest) -> JobResponse:
        """Validate config, create task, return immediately."""
        if self._shutting_down:
            return JobResponse(
                job_id="",
                status="rejected",
                message="Daemon is shutting down",
            )

        if not self._backpressure.should_accept_job():
            return JobResponse(
                job_id="",
                status="rejected",
                message="System under high pressure — try again later",
            )

        job_id = await self._generate_job_id(request.config_path.stem)

        # Validate config exists and resolve workspace from it
        if not request.config_path.exists():
            return JobResponse(
                job_id=job_id,
                status="rejected",
                message=f"Config file not found: {request.config_path}",
            )

        if request.workspace:
            workspace = request.workspace
        else:
            from mozart.core.config import JobConfig
            try:
                config = JobConfig.from_yaml(request.config_path)
                workspace = config.workspace
            except (ValueError, OSError, KeyError, yaml.YAMLError) as exc:
                _logger.error(
                    "manager.config_parse_failed",
                    job_id=job_id,
                    config_path=str(request.config_path),
                    exc_info=True,
                )
                return JobResponse(
                    job_id=job_id,
                    status="rejected",
                    message=(
                        f"Failed to parse config file: {request.config_path} ({exc}). "
                        "Cannot determine workspace. Fix the config or pass --workspace explicitly."
                    ),
                )

        # Early workspace validation: reject jobs whose workspace parent
        # doesn't exist or isn't writable, instead of failing deep in
        # JobService.start_job(). Workspace itself may not exist yet —
        # it gets created by JobService — but the parent must be valid.
        ws_parent = workspace.parent
        if not ws_parent.exists():
            return JobResponse(
                job_id=job_id,
                status="rejected",
                message=(
                    f"Workspace parent directory does not exist: {ws_parent}. "
                    "Create the parent directory or change the workspace path."
                ),
            )
        if not os.access(ws_parent, os.W_OK):
            return JobResponse(
                job_id=job_id,
                status="rejected",
                message=(
                    f"Workspace parent directory is not writable: {ws_parent}. "
                    "Fix permissions or change the workspace path."
                ),
            )

        meta = JobMeta(
            job_id=job_id,
            config_path=request.config_path,
            workspace=workspace,
            chain_depth=request.chain_depth,
        )
        # Register in DB first — if this fails, no phantom in-memory entry
        await self._registry.register_job(job_id, request.config_path, workspace)
        self._job_meta[job_id] = meta

        try:
            task = asyncio.create_task(
                self._run_job_task(job_id, request),
                name=f"job-{job_id}",
            )
        except RuntimeError:
            # Clean up metadata if task creation fails
            # RuntimeError is raised by asyncio when no running event loop
            self._job_meta.pop(job_id, None)
            await self._registry.update_status(
                job_id, DaemonJobStatus.FAILED, error_message="Task creation failed",
            )
            raise
        self._jobs[job_id] = task
        task.add_done_callback(lambda t: self._on_task_done(job_id, t))

        _logger.info(
            "job.submitted",
            job_id=job_id,
            config_path=str(request.config_path),
        )

        return JobResponse(
            job_id=job_id,
            status="accepted",
            message=f"Job queued (concurrency limit: {self._config.max_concurrent_jobs})",
        )

    async def get_job_status(self, job_id: str, workspace: Path | None = None) -> dict[str, Any]:
        """Get full status of a specific job.

        Returns the conductor's live in-memory CheckpointState when
        available (populated by _PublishingBackend on every save).
        Falls back to basic JobMeta if no live state exists yet
        (e.g. job was just submitted and hasn't checkpointed).
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")

        # Serve from conductor's live state — no disk I/O
        live = self._live_states.get(job_id)
        if live is not None:
            return live.model_dump(mode="json")

        # Fallback to basic metadata
        return meta.to_dict()

    async def pause_job(self, job_id: str, workspace: Path | None = None) -> bool:
        """Send pause signal to a running job."""
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")
        if meta.status != DaemonJobStatus.RUNNING:
            raise JobSubmissionError(
                f"Job '{job_id}' is {meta.status.value}, not running"
            )

        ws = workspace or meta.workspace
        return await self._checked_service.pause_job(meta.job_id, ws)

    async def resume_job(self, job_id: str, workspace: Path | None = None) -> JobResponse:
        """Resume a paused job by creating a new task.

        If an old task for this job is still running (e.g., not yet fully
        paused), it is cancelled before the new resume task is created to
        prevent detached/duplicate execution.
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")
        if meta.status not in (DaemonJobStatus.PAUSED, DaemonJobStatus.FAILED):
            raise JobSubmissionError(
                f"Job '{job_id}' is {meta.status.value}, "
                "only PAUSED or FAILED jobs can be resumed"
            )

        # Cancel stale task to prevent detached execution
        old_task = self._jobs.pop(job_id, None)
        if old_task is not None and not old_task.done():
            old_task.cancel(msg=f"stale task replaced by resume of {job_id}")
            _logger.info("job.resume_cancelled_stale_task", job_id=job_id)

        ws = workspace or meta.workspace
        meta.status = DaemonJobStatus.QUEUED

        task = asyncio.create_task(
            self._resume_job_task(job_id, ws),
            name=f"job-resume-{job_id}",
        )
        self._jobs[job_id] = task
        task.add_done_callback(lambda t: self._on_task_done(job_id, t))

        return JobResponse(
            job_id=job_id,
            status="accepted",
            message="Job resume queued",
        )

    async def cancel_job(self, job_id: str) -> bool:
        """Cancel a running job task.

        Deregisters the job from the global sheet scheduler to remove
        any pending sheets from the global queue on cancellation.
        """
        task = self._jobs.get(job_id)
        if task is None:
            return False

        task.cancel(msg=f"explicit cancel_job({job_id}) via IPC")
        meta = self._job_meta.get(job_id)
        if meta:
            meta.status = DaemonJobStatus.CANCELLED
        await self._registry.update_status(job_id, "cancelled")

        if self._scheduler_instance is not None:
            await self._scheduler_instance.deregister_job(job_id)
        _logger.info("job.cancelled", job_id=job_id)
        return True

    async def list_jobs(self) -> list[dict[str, Any]]:
        """List all jobs from the persistent registry.

        In-memory ``_job_meta`` is authoritative for active jobs (has
        live status). The registry fills in historical jobs.
        """
        seen: set[str] = set()
        result: list[dict[str, Any]] = []

        # Active jobs first (in-memory is most current)
        for meta in self._job_meta.values():
            result.append(meta.to_dict())
            seen.add(meta.job_id)

        # Historical jobs from registry
        for record in await self._registry.list_jobs():
            if record.job_id not in seen:
                result.append(record.to_dict())

        return result

    async def clear_jobs(
        self,
        statuses: list[str] | None = None,
        older_than_seconds: float | None = None,
    ) -> dict[str, Any]:
        """Clear terminal jobs from registry and in-memory metadata.

        Args:
            statuses: Status filter (defaults to terminal statuses).
            older_than_seconds: Age filter in seconds.

        Returns:
            Dict with "deleted" count.
        """
        safe_statuses = set(statuses or ["completed", "failed", "cancelled"])
        safe_statuses -= {"queued", "running"}  # Never clear active jobs

        to_remove: list[str] = []
        now = time.time()
        for jid, meta in self._job_meta.items():
            if meta.status.value not in safe_statuses:
                continue
            if older_than_seconds is not None:
                if (now - meta.submitted_at) < older_than_seconds:
                    continue
            to_remove.append(jid)

        for jid in to_remove:
            self._job_meta.pop(jid, None)

        deleted = await self._registry.delete_jobs(
            statuses=list(safe_statuses),
            older_than_seconds=older_than_seconds,
        )

        _logger.info(
            "manager.clear_jobs",
            in_memory_removed=len(to_remove),
            registry_deleted=deleted,
        )
        return {"deleted": deleted}

    async def get_job_errors(self, job_id: str, workspace: Path | None = None) -> dict[str, Any]:
        """Get errors for a specific job.

        Loads the full CheckpointState and returns it for the CLI to
        extract error information from sheet states.
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")

        ws = workspace or meta.workspace
        state = await self._checked_service.get_status(meta.job_id, ws)
        if state is None:
            raise JobSubmissionError(f"No state found for job '{job_id}'")

        return {"state": state.model_dump(mode="json")}

    async def get_diagnostic_report(
        self, job_id: str, workspace: Path | None = None,
    ) -> dict[str, Any]:
        """Get diagnostic data for a specific job.

        Returns the full CheckpointState plus workspace path for the CLI
        to build the diagnostic report locally.
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")

        ws = workspace or meta.workspace
        state = await self._checked_service.get_status(meta.job_id, ws)
        if state is None:
            raise JobSubmissionError(f"No state found for job '{job_id}'")

        return {
            "state": state.model_dump(mode="json"),
            "workspace": str(ws),
        }

    async def get_execution_history(
        self, job_id: str, workspace: Path | None = None,
        sheet_num: int | None = None, limit: int = 50,
    ) -> dict[str, Any]:
        """Get execution history for a specific job.

        Requires the SQLite state backend for history records.
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")

        ws = workspace or meta.workspace

        from mozart.state import SQLiteStateBackend

        sqlite_path = ws / ".mozart-state.db"
        records: list[dict[str, Any]] = []
        has_history = False

        if sqlite_path.exists():
            backend = SQLiteStateBackend(sqlite_path)
            try:
                if hasattr(backend, 'get_execution_history'):
                    records = await backend.get_execution_history(
                        job_id=job_id, sheet_num=sheet_num, limit=limit,
                    )
                    has_history = True
            finally:
                await backend.close()

        return {
            "job_id": job_id,
            "records": records,
            "has_history": has_history,
        }

    async def recover_job(
        self, job_id: str, workspace: Path | None = None,
        sheet_num: int | None = None, dry_run: bool = False,
    ) -> dict[str, Any]:
        """Get state for recover operation.

        Returns the job state and workspace for the CLI to run
        validations locally. The actual validation logic stays
        in the CLI command to avoid duplicating ValidationEngine
        setup in the daemon.
        """
        meta = self._job_meta.get(job_id)
        if meta is None:
            raise JobSubmissionError(f"Job '{job_id}' not found")

        ws = workspace or meta.workspace
        state = await self._checked_service.get_status(meta.job_id, ws)
        if state is None:
            raise JobSubmissionError(f"No state found for job '{job_id}'")

        return {
            "state": state.model_dump(mode="json"),
            "workspace": str(ws),
            "dry_run": dry_run,
            "sheet_num": sheet_num,
        }

    async def get_daemon_status(self) -> dict[str, Any]:
        """Build daemon status summary.

        Returns all fields required by the ``DaemonStatus`` Pydantic model
        so ``DaemonClient.status()`` can deserialize without crashing.
        """
        mem = self._monitor.current_memory_mb()
        return {
            "pid": os.getpid(),
            "uptime_seconds": round(time.monotonic() - self._start_time, 1),
            "running_jobs": self.running_count,
            "total_jobs_active": self.active_job_count,
            "memory_usage_mb": round(mem, 1) if mem is not None else 0.0,
            "version": getattr(mozart, "__version__", "0.1.0"),
        }

    # ─── Shutdown ─────────────────────────────────────────────────────

    async def shutdown(self, graceful: bool = True) -> None:
        """Cancel all running jobs, optionally waiting for sheets.

        Deregisters all active jobs from the global sheet scheduler
        to clean up any pending sheets, running-sheet tracking, and
        dependency data before the daemon exits.
        """
        self._shutting_down = True

        if graceful:
            timeout = self._config.shutdown_timeout_seconds
            _logger.info(
                "manager.shutting_down",
                graceful=True,
                timeout=timeout,
                running_jobs=self.running_count,
            )

            # Wait for running tasks to complete (up to timeout)
            running = [t for t in self._jobs.values() if not t.done()]
            if running:
                _, pending = await asyncio.wait(
                    running, timeout=timeout,
                )
                for task in pending:
                    task.cancel(msg="graceful shutdown timeout exceeded")
                if pending:
                    results = await asyncio.gather(*pending, return_exceptions=True)
                    for result in results:
                        if isinstance(result, BaseException):
                            _logger.warning(
                                "manager.shutdown_task_exception",
                                error=str(result),
                                error_type=type(result).__name__,
                            )
        else:
            _logger.info("manager.shutting_down", graceful=False)
            for task in self._jobs.values():
                if not task.done():
                    task.cancel(msg="non-graceful shutdown")
            if self._jobs:
                results = await asyncio.gather(
                    *self._jobs.values(), return_exceptions=True,
                )
                for result in results:
                    if isinstance(result, BaseException):
                        _logger.warning(
                            "manager.shutdown_task_exception",
                            error=str(result),
                            error_type=type(result).__name__,
                        )

        # Deregister all known jobs from the scheduler to clean up
        # heap entries, running-sheet tracking, and dependency data.
        # Uses _job_meta (not _jobs) because task done-callbacks may
        # have already cleared entries from _jobs during cancellation.
        # Guard: only touch the scheduler if it was ever initialized.
        if self._scheduler_instance is not None:
            for job_id in list(self._job_meta.keys()):
                await self._scheduler_instance.deregister_job(job_id)

        self._jobs.clear()

        # Stop all observers for any remaining jobs
        for jid in list(self._job_meta.keys()):
            await self._stop_observer(jid)

        # Shutdown event bus
        await self._event_bus.shutdown()

        # Stop centralized learning hub (final persist + cleanup)
        await self._learning_hub.stop()

        await self._registry.close()
        self._shutdown_event.set()
        _logger.info("manager.shutdown_complete")

    async def wait_for_shutdown(self) -> None:
        """Block until shutdown is complete."""
        await self._shutdown_event.wait()

    # ─── Properties ───────────────────────────────────────────────────

    @property
    def shutting_down(self) -> bool:
        """Whether the manager is in the process of shutting down."""
        return self._shutting_down

    @property
    def running_count(self) -> int:
        """Number of currently running jobs."""
        return sum(
            1 for m in self._job_meta.values() if m.status == DaemonJobStatus.RUNNING
        )

    @property
    def active_job_count(self) -> int:
        """Number of concurrently executing jobs (used for fair-share scheduling).

        Currently returns ``running_count`` (job-level granularity).
        Phase 3 will replace this with ``self._scheduler.active_count``
        for sheet-level granularity once per-sheet dispatch is wired.
        """
        # TODO(Phase 3): return self._scheduler.active_count when wired
        return self.running_count

    _FAILURE_RATE_WINDOW = 60.0  # seconds
    _FAILURE_RATE_THRESHOLD = 3

    @property
    def failure_rate_elevated(self) -> bool:
        """Whether the recent job failure rate is elevated.

        Returns True if more than ``_FAILURE_RATE_THRESHOLD`` unexpected
        exceptions have occurred within the last ``_FAILURE_RATE_WINDOW``
        seconds.  Used by ``HealthChecker.readiness()`` to degrade the
        health signal when systemic failures are occurring.
        """
        now = time.monotonic()
        cutoff = now - self._FAILURE_RATE_WINDOW
        # Prune expired entries from the left
        while self._recent_failures and self._recent_failures[0] < cutoff:
            self._recent_failures.popleft()
        return len(self._recent_failures) > self._FAILURE_RATE_THRESHOLD

    @property
    def notifications_degraded(self) -> bool:
        """Whether notification delivery is degraded (forwarded from JobService)."""
        if self._service is None:
            return False
        return self._service.notifications_degraded

    @property
    def scheduler(self) -> GlobalSheetScheduler:
        """Access the global sheet scheduler for cross-job coordination."""
        return self._scheduler

    @property
    def rate_coordinator(self) -> RateLimitCoordinator:
        """Access the rate limit coordinator for cross-job rate limiting."""
        return self._rate_coordinator

    @property
    def backpressure(self) -> BackpressureController:
        """Access the backpressure controller for load management."""
        return self._backpressure

    @property
    def learning_hub(self) -> LearningHub:
        """Access the centralized learning hub."""
        return self._learning_hub

    @property
    def event_bus(self) -> EventBus:
        """Access the event bus for subscribing to events."""
        return self._event_bus

    # ─── Internal ─────────────────────────────────────────────────────

    async def _start_observer(self, job_id: str) -> None:
        """Start a JobObserver co-task for a running job.

        Called when a job transitions to RUNNING state. The observer
        monitors the workspace filesystem and process tree independently
        of the runner's self-reports.
        """
        meta = self._job_meta.get(job_id)
        if meta is None or not self._config.observer.enabled:
            return

        observer = JobObserver(
            job_id=job_id,
            workspace=meta.workspace,
            pid=os.getpid(),
            event_bus=self._event_bus,
            watch_interval=self._config.observer.watch_interval_seconds,
        )
        meta.observer = observer
        await observer.start()

    async def _stop_observer(self, job_id: str) -> None:
        """Stop the JobObserver co-task for a job."""
        meta = self._job_meta.get(job_id)
        if meta is None or meta.observer is None:
            return
        await meta.observer.stop()
        meta.observer = None

    def _on_state_published(self, state: CheckpointState) -> None:
        """Receive live CheckpointState from a running job's state backend.

        Called synchronously by ``_PublishingBackend.save()`` on every
        checkpoint.  Stores the latest state so ``get_job_status()`` can
        serve it from memory instead of re-reading from the workspace.
        """
        self._live_states[state.job_id] = state

    async def _on_event(
        self,
        job_id: str,
        sheet_num: int,
        event: str,
        data: dict[str, Any] | None,
    ) -> None:
        """Handle runner lifecycle events for registry updates, logging, and bus."""
        _logger.info(
            "runner.event",
            job_id=job_id,
            sheet_num=sheet_num,
            event_type=event,
        )

        # Publish to event bus for downstream consumers
        bus_event: ObserverEvent = {
            "job_id": job_id,
            "sheet_num": sheet_num,
            "event": event,
            "data": data,
            "timestamp": time.time(),
        }
        await self._event_bus.publish(bus_event)

        # Update registry progress on sheet events
        if event.startswith("sheet."):
            meta = self._job_meta.get(job_id)
            if meta:
                total = data.get("total_sheets") if data else None
                await self._registry.update_progress(
                    job_id,
                    current_sheet=sheet_num,
                    total_sheets=total or 0,
                )

    async def _on_rate_limit(
        self,
        backend_type: str,
        wait_seconds: float,
        job_id: str,
        sheet_num: int,
    ) -> None:
        """Forward rate limit events from runners to the coordinator.

        Wiring prerequisite (P025): This callback is passed through
        JobService → RunnerContext → RecoveryMixin._handle_rate_limit()
        so that rate limit detections from any running job feed into the
        daemon's centralized RateLimitCoordinator.  The coordinator
        then informs the scheduler to skip the limited backend.
        """
        await self._rate_coordinator.report_rate_limit(
            backend_type=backend_type,
            wait_seconds=wait_seconds,
            job_id=job_id,
            sheet_num=sheet_num,
        )

    async def _run_managed_task(
        self,
        job_id: str,
        coro: Coroutine[Any, Any, DaemonJobStatus | None],
        *,
        start_event: str = "job.started",
        fail_event: str = "job.failed",
    ) -> None:
        """Shared lifecycle wrapper for job tasks.

        Acquires the concurrency semaphore, tracks status transitions,
        and handles CancelledError / TimeoutError / Exception uniformly.

        Jobs are guarded by ``job_timeout_seconds`` — if a job exceeds
        this wall-clock limit, it is cancelled with FAILED status and a
        descriptive error message.

        Args:
            job_id: The job being executed.
            coro: Awaitable that performs the actual work. May return a
                ``DaemonJobStatus`` to override the default COMPLETED
                status on success (e.g. PAUSED).
            start_event: Structlog event name for the start log.
            fail_event: Structlog event name for the failure log.
        """
        meta = self._job_meta[job_id]
        timeout = self._config.job_timeout_seconds

        async with self._concurrency_semaphore:
            meta.status = DaemonJobStatus.RUNNING
            meta.started_at = time.time()
            await self._registry.update_status(
                job_id, "running", pid=os.getpid(),
            )
            _logger.info(start_event, job_id=job_id, timeout_seconds=timeout)

            # Start observer co-task for filesystem/process monitoring
            await self._start_observer(job_id)

            try:
                result_status = await asyncio.wait_for(coro, timeout=timeout)
                if isinstance(result_status, DaemonJobStatus):
                    meta.status = result_status
                else:
                    meta.status = DaemonJobStatus.COMPLETED

                # Capture completion snapshot for terminal statuses
                snapshot_path: str | None = None
                if meta.status in (DaemonJobStatus.COMPLETED, DaemonJobStatus.FAILED):
                    snapshot_path = self._snapshot_manager.capture(
                        job_id, meta.workspace,
                    )

                await self._registry.update_status(
                    job_id, meta.status.value,
                    snapshot_path=snapshot_path,
                )
                _logger.info(
                    "job.paused" if meta.status == DaemonJobStatus.PAUSED
                    else "job.completed",
                    job_id=job_id,
                )

            except TimeoutError:
                meta.status = DaemonJobStatus.FAILED
                elapsed = time.monotonic() - (meta.started_at or 0)
                meta.error_message = (
                    f"Job exceeded timeout of {timeout:.0f}s "
                    f"(ran for {elapsed:.0f}s)"
                )
                await self._registry.update_status(
                    job_id, "failed", error_message=meta.error_message,
                )
                self._recent_failures.append(time.monotonic())
                _logger.error(
                    "job.timeout",
                    job_id=job_id,
                    timeout_seconds=timeout,
                    elapsed_seconds=round(elapsed, 1),
                )

            except asyncio.CancelledError as cancel_exc:
                meta.status = DaemonJobStatus.CANCELLED
                cancel_reason = str(cancel_exc) if str(cancel_exc) else "unknown"
                await self._registry.update_status(job_id, "cancelled")
                _logger.error(
                    "job.cancelled_during_execution",
                    job_id=job_id,
                    reason=cancel_reason,
                )
                raise

            except (OSError, ValueError, DaemonError) as exc:
                # Expected operational errors: workspace issues, config errors,
                # permission denied, missing directories, etc.
                meta.status = DaemonJobStatus.FAILED
                meta.error_message = str(exc)
                meta.error_traceback = traceback.format_exc()
                await self._registry.update_status(
                    job_id, "failed", error_message=meta.error_message,
                )
                self._recent_failures.append(time.monotonic())
                _logger.error(fail_event, job_id=job_id, error=str(exc))

            except Exception as exc:
                # Unexpected programming bugs — log with full traceback
                meta.status = DaemonJobStatus.FAILED
                meta.error_message = f"Unexpected internal error: {exc}"
                meta.error_traceback = traceback.format_exc()
                await self._registry.update_status(
                    job_id, "failed", error_message=meta.error_message,
                )
                self._recent_failures.append(time.monotonic())
                _logger.exception(
                    "job.unexpected_error", job_id=job_id,
                )

            finally:
                # Stop observer co-task regardless of outcome
                await self._stop_observer(job_id)

    async def _run_job_task(self, job_id: str, request: JobRequest) -> None:
        """Task coroutine that runs a single job.

        Currently runs jobs monolithically via ``JobService.start_job()``.

        TODO(Phase 3 — scheduler integration): Replace monolithic execution
        with per-sheet dispatch through ``self._scheduler``:
          1. Parse sheets from config: ``config.sheets``
          2. Build SheetInfo list + dependency DAG
          3. ``await self._scheduler.register_job(job_id, sheets, deps)``
          4. Dispatch loop: ``entry = await self._scheduler.next_sheet()``
          5. Spawn per-sheet tasks, call ``mark_complete()`` on finish
          6. On error/cancel: ``await self._scheduler.deregister_job(job_id)``
        """

        async def _execute() -> DaemonJobStatus:
            from mozart.core.checkpoint import JobStatus
            from mozart.core.config import JobConfig

            config = JobConfig.from_yaml(request.config_path)
            if request.workspace:
                config = config.model_copy(
                    update={"workspace": request.workspace},
                )

            # Apply daemon-level default thinking method if the job
            # doesn't specify its own (GH#77).
            if (
                self._config.default_thinking_method
                and not config.prompt.thinking_method
            ):
                config = config.model_copy(
                    update={
                        "prompt": config.prompt.model_copy(
                            update={"thinking_method": self._config.default_thinking_method},
                        ),
                    },
                )

            summary = await self._checked_service.start_job(
                config,
                fresh=request.fresh,
                start_sheet=request.start_sheet,
                self_healing=request.self_healing,
                self_healing_auto_confirm=request.self_healing_auto_confirm,
                dry_run=request.dry_run,
            )

            if summary.final_status == JobStatus.PAUSED:
                return DaemonJobStatus.PAUSED
            if summary.final_status == JobStatus.FAILED:
                return DaemonJobStatus.FAILED
            return DaemonJobStatus.COMPLETED

        await self._run_managed_task(job_id, _execute())

    async def _resume_job_task(self, job_id: str, workspace: Path) -> None:
        """Task coroutine that resumes a paused job."""

        async def _execute() -> DaemonJobStatus:
            from mozart.core.checkpoint import JobStatus

            summary = await self._checked_service.resume_job(job_id, workspace)
            if summary.final_status == JobStatus.PAUSED:
                return DaemonJobStatus.PAUSED
            if summary.final_status == JobStatus.FAILED:
                return DaemonJobStatus.FAILED
            return DaemonJobStatus.COMPLETED

        await self._run_managed_task(
            job_id, _execute(),
            start_event="job.resuming",
            fail_event="job.resume_failed",
        )

    def _on_task_done(self, job_id: str, task: asyncio.Task[Any]) -> None:
        """Callback when a job task completes (success, error, or cancel).

        Each cleanup step is isolated so a failure in one (e.g. registry
        update) cannot prevent the others (snapshot cleanup, history prune)
        from running.  asyncio silently drops exceptions in
        Task.add_done_callback handlers, so every step must be guarded.
        """
        # 1. Remove from active jobs and live state — always runs first
        self._jobs.pop(job_id, None)
        self._live_states.pop(job_id, None)

        # 2. Check for task exception and update metadata/registry
        try:
            exc = log_task_exception(task, _logger, "job.task_failed")
            if exc:
                meta = self._job_meta.get(job_id)
                if meta and meta.status == DaemonJobStatus.RUNNING:
                    meta.status = DaemonJobStatus.FAILED
                    meta.error_message = str(exc)
                    update_task = asyncio.create_task(
                        self._registry.update_status(
                            job_id, "failed", error_message=str(exc),
                        ),
                        name=f"registry-update-{job_id}",
                    )
                    update_task.add_done_callback(
                        lambda t: log_task_exception(
                            t, _logger, "registry.update_failed",
                        ),
                    )
        except RuntimeError:
            _logger.error(
                "task_done_status_update_failed", job_id=job_id, exc_info=True,
            )

        # 3. TTL-based snapshot cleanup (runs synchronously, fast)
        try:
            self._snapshot_manager.cleanup(
                max_age_hours=self._config.observer.snapshot_ttl_hours,
            )
        except OSError:
            _logger.error(
                "task_done_snapshot_cleanup_failed", job_id=job_id, exc_info=True,
            )

        # 4. Prune old completed/failed/cancelled jobs from history
        try:
            self._prune_job_history()
        except RuntimeError:
            _logger.error(
                "task_done_prune_failed", job_id=job_id, exc_info=True,
            )

    def _prune_job_history(self) -> None:
        """Evict oldest terminal jobs when history exceeds max_job_history."""
        max_history = self._config.max_job_history
        terminal = sorted(
            (
                (jid, m) for jid, m in self._job_meta.items()
                if m.status in (
                    DaemonJobStatus.COMPLETED,
                    DaemonJobStatus.FAILED,
                    DaemonJobStatus.CANCELLED,
                )
            ),
            key=lambda x: x[1].submitted_at,
        )
        excess = len(terminal) - max_history
        if excess > 0:
            pruned_ids = [jid for jid, _ in terminal[:excess]]
            for jid in pruned_ids:
                self._job_meta.pop(jid, None)
            _logger.debug(
                "manager.job_history_pruned",
                pruned_count=excess,
                oldest_pruned=pruned_ids[0],
            )


__all__ = ["DaemonJobStatus", "JobManager", "JobMeta"]
