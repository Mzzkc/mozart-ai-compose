# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║              DAEMON SYMPHONY: PHASE 2 — DAEMON SERVICE (mozartd)           ║
# ║                                                                            ║
# ║  The daemon itself. Creates the mozartd long-running process that manages  ║
# ║  jobs, monitors resources, owns child processes, and handles signals.      ║
# ║  This is the heart of the daemon migration.                                ║
# ║                                                                            ║
# ║  Execution Flow (10 stages → 12 sheets after fan-out):                    ║
# ║                                                                            ║
# ║    [1] Discovery → [2] Daemon Process → [3] Job Manager →                 ║
# ║    [4] Resource Monitor → [5] Process Groups → [6] Health Check →         ║
# ║    [7] Tests →                                                             ║
# ║        [8a] [8b] [8c]  ← Code Reviews (PARALLEL) →                        ║
# ║    [9] Apply Fixes → [10] Commit                                          ║
# ║                                                                            ║
# ║  Depends on: Phase 0 (service abstractions) + Phase 1 (IPC layer)         ║
# ║  Chains to: daemon-phase3-scheduler.yaml                                  ║
# ╚══════════════════════════════════════════════════════════════════════════════╝

name: "daemon-phase2-service"
description: "Daemon process — mozartd entry point, job manager, resource monitor, process groups"

workspace: "./.daemon-workspace-p2"

workspace_lifecycle:
  archive_on_fresh: true
  max_archives: 5

backend:
  type: claude_cli
  skip_permissions: true
  working_directory: /home/emzi/Projects/mozart-ai-compose
  timeout_seconds: 3000  # 50 min — this phase is more complex

cross_sheet:
  auto_capture_stdout: true
  max_output_chars: 3000
  lookback_sheets: 2
  capture_files:
    - "{{ workspace }}/*.md"

sheet:
  size: 1
  total_items: 10

  fan_out:
    8: 3

  dependencies:
    2: [1]
    3: [2]
    4: [3]
    5: [4]
    6: [5]
    7: [6]
    8: [7]
    9: [8]
    10: [9]

parallel:
  enabled: true
  max_concurrent: 3

retry:
  max_retries: 2

on_success:
  - type: run_job
    job_path: "daemon-phase3-scheduler.yaml"
    description: "Chain to Phase 3: Cross-Job Scheduler"
    detached: true

concert:
  enabled: true
  max_chain_depth: 5
  cooldown_between_jobs_seconds: 30

prompt:
  template: |
    {{ preamble }}

    **Sheet:** {{ sheet_num }} of {{ total_sheets }} | **Stage:** {{ stage }}
    **Workspace:** {{ workspace }}

    {% if stage == 1 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 1: DISCOVERY — DAEMON PROCESS DESIGN                                  ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Design the daemon process architecture. Study how the codebase handles:

    **Read these files:**
    - `src/mozart/daemon/job_service.py` — Phase 0's extracted service
    - `src/mozart/daemon/ipc/server.py` — Phase 1's socket server
    - `src/mozart/daemon/ipc/handler.py` — Phase 1's request handler
    - `src/mozart/daemon/config.py` — DaemonConfig model
    - `src/mozart/execution/runner/base.py` — Signal handling (SIGINT/SIGTERM/SIGHUP)
    - `src/mozart/execution/runner/lifecycle.py` — Job lifecycle (run/resume)
    - `src/mozart/cli/commands/run.py` — How CLI creates and runs jobs

    **Design Requirements:**

    1. **Daemon Process (`src/mozart/daemon/process.py`):**
       - `mozartd` CLI entry point using Typer
       - `start` command: daemonize, write PID file, start socket server
       - `stop` command: send SIGTERM to daemon PID
       - `status` command: check if daemon is running
       - Signal handling: SIGTERM → graceful shutdown, SIGHUP → reload config
       - Clean shutdown: wait for running jobs, save state, close sockets

    2. **Job Manager (`src/mozart/daemon/manager.py`):**
       - Holds all running jobs as asyncio.Tasks
       - Routes IPC requests to JobService methods
       - Tracks job → task mapping
       - Implements concurrency limits (max_concurrent_jobs)
       - Cancels all jobs on shutdown

    3. **Resource Monitor (`src/mozart/daemon/monitor.py`):**
       - Periodic memory check (psutil or /proc)
       - Process count tracking
       - API rate budget tracking
       - Emit warnings when approaching limits
       - Hard-kill protection when limits exceeded

    4. **Process Group Management:**
       - Daemon creates its own process group (os.setpgrp)
       - All child processes (Claude CLI) inherit the group
       - On daemon shutdown, kill entire group (fixes issue #38)
       - Orphan detection: periodic scan for zombies

    **Output:** `{{ workspace }}/01-daemon-design.md`
    End with: `DAEMON_DESIGN_COMPLETE: yes`

    {% elif stage == 2 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 2: DAEMON PROCESS ENTRY POINT                                         ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create the mozartd CLI entry point and daemon lifecycle.

    {% if previous_outputs and 1 in previous_outputs %}
    **Stage 1 design:**
    {{ previous_outputs[1][:2000] }}
    {% endif %}

    **Create `src/mozart/daemon/process.py`:**

    ```python
    """mozartd — Mozart daemon process.

    Long-running orchestration service that manages job execution,
    resources, and cross-job coordination.
    """
    import asyncio
    import os
    import signal
    import sys
    from pathlib import Path

    import typer

    from mozart.daemon.config import DaemonConfig
    from mozart.core.logging import get_logger

    _logger = get_logger("mozartd")

    daemon_app = typer.Typer(name="mozartd", help="Mozart daemon service")


    @daemon_app.command()
    def start(
        config_file: Path | None = typer.Option(None, "--config", "-c"),
        foreground: bool = typer.Option(False, "--foreground", "-f"),
        log_level: str = typer.Option("info", "--log-level"),
    ) -> None:
        """Start the Mozart daemon."""
        config = DaemonConfig()
        if config_file and config_file.exists():
            # Load from YAML
            ...

        if not foreground:
            _daemonize(config)
        else:
            _logger.info("daemon.starting", pid=os.getpid(), foreground=True)

        asyncio.run(_run_daemon(config))


    @daemon_app.command()
    def stop() -> None:
        """Stop the running daemon."""
        config = DaemonConfig()
        pid = _read_pid(config.pid_file)
        if pid:
            os.kill(pid, signal.SIGTERM)
            typer.echo(f"Sent SIGTERM to mozartd (PID {pid})")
        else:
            typer.echo("mozartd is not running")
            raise typer.Exit(1)


    @daemon_app.command()
    def status() -> None:
        """Check daemon status."""
        ...


    async def _run_daemon(config: DaemonConfig) -> None:
        """Main daemon event loop."""
        # 1. Write PID file
        _write_pid(config.pid_file)

        # 2. Set up process group (fixes issue #38)
        os.setpgrp()

        # 3. Create components
        from mozart.daemon.manager import JobManager
        from mozart.daemon.monitor import ResourceMonitor
        from mozart.daemon.ipc.server import DaemonServer
        from mozart.daemon.ipc.handler import RequestHandler

        manager = JobManager(config)
        monitor = ResourceMonitor(config.resource_limits)
        handler = RequestHandler()

        # 4. Register RPC methods
        handler.register("daemon.status", manager.get_daemon_status)
        handler.register("daemon.shutdown", manager.shutdown)
        handler.register("job.submit", manager.submit_job)
        handler.register("job.status", manager.get_job_status)
        handler.register("job.pause", manager.pause_job)
        handler.register("job.resume", manager.resume_job)
        handler.register("job.cancel", manager.cancel_job)
        handler.register("job.list", manager.list_jobs)

        # 5. Start server
        server = DaemonServer(config.socket.path, handler)
        await server.start()

        # 6. Install signal handlers
        loop = asyncio.get_running_loop()
        for sig in (signal.SIGTERM, signal.SIGINT, signal.SIGHUP):
            loop.add_signal_handler(sig, lambda s=sig: asyncio.create_task(
                _handle_signal(s, manager, server, config)
            ))

        # 7. Start resource monitor
        monitor_task = asyncio.create_task(monitor.run())

        # 8. Run until shutdown
        _logger.info("daemon.started", pid=os.getpid(),
                     socket=str(config.socket.path))
        await manager.wait_for_shutdown()

        # 9. Cleanup
        monitor_task.cancel()
        await server.stop()
        config.pid_file.unlink(missing_ok=True)
        _logger.info("daemon.stopped")
    ```

    **Also add entry point to `pyproject.toml`:**
    ```toml
    [project.scripts]
    mozartd = "mozart.daemon.process:daemon_app"
    ```

    **Verify:**
    ```bash
    python3 -c "from mozart.daemon.process import daemon_app; print('OK')"
    ```

    **Output:** `{{ workspace }}/02-daemon-process.md`
    End with: `DAEMON_PROCESS_COMPLETE: yes`

    {% elif stage == 3 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 3: JOB MANAGER                                                        ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create the in-process job manager that runs jobs as asyncio tasks.

    **Create `src/mozart/daemon/manager.py`:**

    ```python
    """Job manager — runs jobs as asyncio tasks within the daemon."""
    import asyncio
    import time
    from pathlib import Path
    from typing import Any

    from mozart.daemon.config import DaemonConfig
    from mozart.daemon.job_service import JobService
    from mozart.daemon.output import StructuredOutput
    from mozart.core.config import JobConfig
    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.manager")


    class JobManager:
        """Manages concurrent job execution within the daemon process."""

        def __init__(self, config: DaemonConfig):
            self._config = config
            self._service = JobService(output=StructuredOutput())
            self._running_jobs: dict[str, asyncio.Task] = {}
            self._job_metadata: dict[str, dict[str, Any]] = {}
            self._shutdown_event = asyncio.Event()
            self._start_time = time.monotonic()
            self._lock = asyncio.Lock()

        async def submit_job(self, params: dict) -> dict:
            """Submit a new job for execution."""
            async with self._lock:
                if len(self._running_jobs) >= self._config.max_concurrent_jobs:
                    return {"status": "rejected", "message": "Max concurrent jobs reached"}

            config_path = Path(params["config_path"])
            config = JobConfig.from_yaml(config_path)
            job_id = config.name

            async with self._lock:
                if job_id in self._running_jobs:
                    return {"status": "rejected", "message": f"Job {job_id} already running"}

            # Launch job as asyncio task
            task = asyncio.create_task(
                self._run_job(config, params),
                name=f"job-{job_id}",
            )
            async with self._lock:
                self._running_jobs[job_id] = task
                self._job_metadata[job_id] = {
                    "config_path": str(config_path),
                    "workspace": params.get("workspace"),
                    "started_at": time.time(),
                }

            task.add_done_callback(lambda t: asyncio.create_task(
                self._on_job_done(job_id, t)
            ))

            return {"job_id": job_id, "status": "accepted"}

        async def _run_job(self, config: JobConfig, params: dict) -> None:
            """Execute a job using JobService."""
            try:
                await self._service.start_job(
                    config,
                    fresh=params.get("fresh", False),
                    self_healing=params.get("self_healing", False),
                )
            except Exception:
                _logger.exception("job.failed", job_id=config.name)
                raise

        async def _on_job_done(self, job_id: str, task: asyncio.Task) -> None:
            """Clean up after job completion."""
            async with self._lock:
                self._running_jobs.pop(job_id, None)
            if task.exception():
                _logger.error("job.exception", job_id=job_id,
                            error=str(task.exception()))
            else:
                _logger.info("job.completed", job_id=job_id)

        async def get_daemon_status(self, params: dict | None = None) -> dict:
            """Return daemon status."""
            import psutil
            process = psutil.Process()
            return {
                "pid": os.getpid(),
                "uptime_seconds": time.monotonic() - self._start_time,
                "running_jobs": len(self._running_jobs),
                "memory_usage_mb": process.memory_info().rss / (1024 * 1024),
                "version": "0.1.0",
            }

        async def shutdown(self, params: dict | None = None) -> bool:
            """Initiate graceful shutdown."""
            graceful = (params or {}).get("graceful", True)
            if graceful:
                # Wait for running jobs to complete
                for job_id, task in list(self._running_jobs.items()):
                    _logger.info("daemon.waiting_for_job", job_id=job_id)
                    await asyncio.wait_for(task, timeout=300)
            else:
                # Cancel all running jobs
                for task in self._running_jobs.values():
                    task.cancel()
            self._shutdown_event.set()
            return True

        async def wait_for_shutdown(self) -> None:
            await self._shutdown_event.wait()

        # ... pause_job, resume_job, cancel_job, list_jobs, get_job_status
    ```

    **Implement ALL methods.** Read `src/mozart/daemon/job_service.py` for the
    service methods you're delegating to.

    **Output:** `{{ workspace }}/03-job-manager.md`
    End with: `JOB_MANAGER_COMPLETE: yes`

    {% elif stage == 4 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 4: RESOURCE MONITOR                                                   ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create the resource monitoring system.

    **Create `src/mozart/daemon/monitor.py`:**

    ```python
    """Resource monitor — tracks memory, processes, and rate limits."""
    import asyncio
    import os
    from mozart.daemon.config import ResourceLimitConfig
    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.monitor")


    class ResourceMonitor:
        """Periodic resource monitoring with limit enforcement."""

        def __init__(
            self, limits: ResourceLimitConfig, check_interval: float = 30.0,
        ):
            self._limits = limits
            self._interval = check_interval
            self._metrics: dict[str, float] = {}

        async def run(self) -> None:
            """Periodically check resources."""
            while True:
                try:
                    await self._check_resources()
                except asyncio.CancelledError:
                    break
                except Exception:
                    _logger.exception("monitor.check_failed")
                await asyncio.sleep(self._interval)

        async def _check_resources(self) -> None:
            """Check all resource limits."""
            memory_mb = self._get_memory_usage()
            process_count = self._get_process_count()

            self._metrics = {
                "memory_mb": memory_mb,
                "process_count": process_count,
            }

            # Warn at 80% of limits
            if memory_mb > self._limits.max_memory_mb * 0.8:
                _logger.warning("monitor.memory_high",
                              usage_mb=memory_mb,
                              limit_mb=self._limits.max_memory_mb)

            if process_count > self._limits.max_processes * 0.8:
                _logger.warning("monitor.processes_high",
                              count=process_count,
                              limit=self._limits.max_processes)

            # Hard limit: refuse new work
            if memory_mb > self._limits.max_memory_mb:
                _logger.error("monitor.memory_exceeded",
                            usage_mb=memory_mb)
                # Signal job manager to stop accepting new jobs

        def _get_memory_usage(self) -> float:
            """Get current process tree memory usage in MB."""
            try:
                import psutil
                process = psutil.Process(os.getpid())
                children = process.children(recursive=True)
                total = process.memory_info().rss
                for child in children:
                    try:
                        total += child.memory_info().rss
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
                return total / (1024 * 1024)
            except ImportError:
                # Fallback to /proc
                ...

        def _get_process_count(self) -> int:
            """Get total child process count."""
            try:
                import psutil
                process = psutil.Process(os.getpid())
                return len(process.children(recursive=True))
            except ImportError:
                return 0

        @property
        def metrics(self) -> dict[str, float]:
            return dict(self._metrics)

        def is_accepting_work(self) -> bool:
            """Check if resource limits allow new work."""
            mem = self._metrics.get("memory_mb", 0)
            procs = self._metrics.get("process_count", 0)
            return (mem < self._limits.max_memory_mb and
                    procs < self._limits.max_processes)
    ```

    **Note:** Make `psutil` an optional dependency. Use `/proc/self/status` as
    fallback for Linux systems without psutil.

    **Verify:**
    ```bash
    python3 -c "from mozart.daemon.monitor import ResourceMonitor; print('OK')"
    ```

    **Output:** `{{ workspace }}/04-resource-monitor.md`
    End with: `RESOURCE_MONITOR_COMPLETE: yes`

    {% elif stage == 5 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 5: PROCESS GROUP MANAGEMENT                                           ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Implement process group management to prevent orphaned processes (issue #38).

    **Context:** Issue #38 documents cascading crashes caused by 310+ orphaned MCP
    server processes consuming 22 GB after Claude Code crashes. The daemon's process
    group management is the architectural fix.

    **Add to `src/mozart/daemon/process.py` or create `src/mozart/daemon/pgroup.py`:**

    ```python
    """Process group management for orphan prevention."""
    import os
    import signal

    class ProcessGroupManager:
        """Manages the daemon's process group to prevent orphans."""

        def __init__(self):
            self._original_pgid = os.getpgrp()

        def setup(self) -> None:
            """Create new process group for daemon + children."""
            os.setpgrp()  # Make daemon the process group leader

        def kill_all_children(self, sig: int = signal.SIGTERM) -> int:
            """Send signal to all processes in our group."""
            try:
                pgid = os.getpgrp()
                # Kill all processes in group EXCEPT ourselves
                os.killpg(pgid, sig)
                return pgid
            except ProcessLookupError:
                return 0

        def cleanup_orphans(self) -> list[int]:
            """Find and kill orphaned child processes."""
            orphans = []
            try:
                import psutil
                current = psutil.Process(os.getpid())
                for child in current.children(recursive=True):
                    if not child.is_running():
                        continue
                    # Check if child is a zombie or orphaned MCP server
                    try:
                        if child.status() == psutil.STATUS_ZOMBIE:
                            child.wait(timeout=0)
                            orphans.append(child.pid)
                        elif 'mcp' in ' '.join(child.cmdline()).lower():
                            # Check if parent process is still alive
                            if child.ppid() == 1:  # Orphaned (reparented to init)
                                child.terminate()
                                orphans.append(child.pid)
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
            except ImportError:
                pass
            return orphans
    ```

    **Wire into daemon lifecycle in `process.py`:**
    1. Call `pgroup.setup()` early in `_run_daemon()`
    2. Add periodic orphan cleanup to resource monitor
    3. Call `pgroup.kill_all_children()` during shutdown
    4. Add `atexit` handler as last-resort cleanup

    **Output:** `{{ workspace }}/05-process-groups.md`
    End with: `PROCESS_GROUPS_COMPLETE: yes`

    {% elif stage == 6 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 6: HEALTH CHECK & STATUS                                              ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create health check infrastructure for daemon readiness/liveness.

    **Add to `src/mozart/daemon/manager.py` or create `src/mozart/daemon/health.py`:**

    ```python
    class HealthChecker:
        """Daemon health and readiness probes."""

        def __init__(self, manager: JobManager, monitor: ResourceMonitor):
            self._manager = manager
            self._monitor = monitor

        async def liveness(self) -> dict:
            """Is the daemon process alive and responsive?"""
            return {"status": "ok", "pid": os.getpid()}

        async def readiness(self) -> dict:
            """Is the daemon ready to accept new jobs?"""
            ready = self._monitor.is_accepting_work()
            return {
                "status": "ready" if ready else "not_ready",
                "running_jobs": len(self._manager._running_jobs),
                "memory_mb": self._monitor.metrics.get("memory_mb", 0),
                "accepting_work": ready,
            }
    ```

    **Register as RPC methods in daemon startup:**
    ```python
    handler.register("daemon.health", health.liveness)
    handler.register("daemon.ready", health.readiness)
    ```

    **Also update `mozartd status` command** to use the client:
    ```python
    @daemon_app.command()
    def status():
        client = DaemonClient()
        if asyncio.run(client.is_daemon_running()):
            status = asyncio.run(client.status())
            # Pretty print with Rich
        else:
            print("mozartd is not running")
    ```

    **Output:** `{{ workspace }}/06-health-check.md`
    End with: `HEALTH_CHECK_COMPLETE: yes`

    {% elif stage == 7 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 7: TESTS                                                              ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Write tests for the daemon service components.

    **Create:**

    1. **`tests/test_daemon_process.py`**
       - Test daemon_app CLI commands (typer.testing.CliRunner)
       - Test PID file write/read/cleanup
       - Test signal handler installation
       - Test _daemonize() skipped in foreground mode

    2. **`tests/test_daemon_manager.py`**
       - Test JobManager submit_job with mock JobService
       - Test concurrency limit enforcement
       - Test duplicate job rejection
       - Test graceful shutdown waits for jobs
       - Test forceful shutdown cancels tasks
       - Test _on_job_done cleanup
       - Test get_daemon_status returns correct metrics

    3. **`tests/test_daemon_monitor.py`**
       - Test ResourceMonitor check cycle
       - Test memory warning thresholds
       - Test process count tracking
       - Test is_accepting_work() with various metric states
       - Test fallback when psutil unavailable (mock ImportError)

    4. **`tests/test_daemon_pgroup.py`**
       - Test ProcessGroupManager.setup()
       - Test cleanup_orphans() with mock psutil
       - Test kill_all_children() signal propagation

    **Run:**
    ```bash
    pytest tests/test_daemon_process.py tests/test_daemon_manager.py \
           tests/test_daemon_monitor.py tests/test_daemon_pgroup.py -v
    pytest tests/ -x --timeout=180 -q
    ```

    **Output:** `{{ workspace }}/07-tests.md`
    End with: `TESTS_COMPLETE: yes`

    {% elif stage == 8 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 8: CODE REVIEW ({{ instance }} of {{ fan_count }})                    ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    {% if instance == 1 %}
    **REVIEWER: Daemon Architecture**

    Review all daemon service files for:
    1. **Signal safety** — Signal handlers are async-safe, no blocking in handlers
    2. **Shutdown sequence** — Clean, deterministic, no resource leaks
    3. **Process group** — Correct usage of setpgrp/killpg, no self-signal
    4. **PID file** — Atomic write, stale detection, cleanup on crash
    5. **Concurrency** — asyncio.Lock usage correct, no deadlocks
    6. **Error isolation** — One job's failure doesn't crash the daemon

    Grade A/B/C/D. List issues with file:line references.

    {% elif instance == 2 %}
    **REVIEWER: Test Coverage**

    Review all daemon tests for:
    1. **Integration coverage** — Can we start/stop daemon in tests?
    2. **Edge cases** — Concurrent job submission, shutdown during execution
    3. **Resource limits** — Tests for memory/process limit enforcement
    4. **Mock quality** — JobService properly mocked, not leaking real execution

    Grade A/B/C/D. List missing tests.

    {% elif instance == 3 %}
    **REVIEWER: Backwards Compatibility + Security**

    1. Verify existing CLI still works without daemon
    2. Check Unix socket permissions (no world-readable)
    3. Verify PID file path safety (no symlink following)
    4. Check signal handling doesn't interfere with existing runner signals
    5. Run full test suite:
    ```bash
    pytest tests/ -x --timeout=180 -q
    ```
    {% endif %}

    **Output:** `{{ workspace }}/08-review-{{ instance }}.md`
    End with: `REVIEW_COMPLETE: yes`

    {% elif stage == 9 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 9: APPLY REVIEW FIXES                                                 ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read reviews and apply fixes:
    ```bash
    cat {{ workspace }}/08-review-1.md
    cat {{ workspace }}/08-review-2.md
    cat {{ workspace }}/08-review-3.md
    ```

    Focus on: signal safety, shutdown correctness, security, missing tests.

    Verify:
    ```bash
    pytest tests/test_daemon_*.py -v
    pytest tests/ -x --timeout=180 -q
    ruff check src/mozart/daemon/
    ```

    **Output:** `{{ workspace }}/09-fixes.md`
    End with: `FIXES_APPLIED: yes`

    {% elif stage == 10 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 10: COMMIT                                                            ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate
    pytest tests/ -x --timeout=180 -q
    ruff check src/mozart/daemon/
    ```

    Selective commit:
    ```bash
    git add src/mozart/daemon/process.py
    git add src/mozart/daemon/manager.py
    git add src/mozart/daemon/monitor.py
    git add src/mozart/daemon/pgroup.py 2>/dev/null || true
    git add src/mozart/daemon/health.py 2>/dev/null || true
    git add tests/test_daemon_process.py
    git add tests/test_daemon_manager.py
    git add tests/test_daemon_monitor.py
    git add tests/test_daemon_pgroup.py 2>/dev/null || true
    # Add pyproject.toml if mozartd entry point was added
    git add pyproject.toml

    git status

    git commit -m "feat(daemon): Phase 2 — mozartd service with job manager and resource monitor

    Create the mozartd daemon process (issue #39):

    - mozartd CLI: start/stop/status commands with daemonization
    - JobManager: in-process job execution via asyncio tasks
    - ResourceMonitor: memory and process tracking with limits
    - ProcessGroupManager: orphan prevention (fixes #38)
    - Health/readiness probes for service monitoring
    - Comprehensive tests for all daemon components

    Part of the Daemon Symphony concert (Phase 2 of 4).

    Co-Authored-By: Mozart AI Compose <noreply@anthropic.com>"

    git push -u origin daemon-symphony
    ```

    **Output:** `{{ workspace }}/10-commit.md`
    End with: `COMMIT_COMPLETE: yes`
    {% endif %}

    {{ output_footer }}

  variables:
    preamble: |
      ╔══════════════════════════════════════════════════════════════════════════════╗
      ║  DAEMON SYMPHONY: PHASE 2 — Daemon Service (mozartd)                        ║
      ║  Part of the Mozart Daemon Migration Concert (Issue #39)                    ║
      ╚══════════════════════════════════════════════════════════════════════════════╝

      **Context:** Phase 2 of 5. Phase 0 created service abstractions, Phase 1 built
      the IPC layer. This phase creates the actual daemon process.

      **Project:** Mozart AI Compose at /home/emzi/Projects/mozart-ai-compose

      **CRITICAL — Branch Protocol (do this FIRST, before any other work):**
      ```bash
      cd /home/emzi/Projects/mozart-ai-compose
      git checkout daemon-symphony
      ```
      All commits go to `daemon-symphony`, NEVER to `main`.
      NEVER use `git add .` or `git add -A`. Only stage specific source files listed in the instructions.
      NEVER commit workspace files (`.daemon-workspace-*`). They are gitignored artifacts.

      **Phase 2 Goal:** Create mozartd — the long-running daemon that manages jobs,
      monitors resources, owns child processes, and prevents orphans (issue #38).

      **Already exists from Phases 0-1:**
      - `src/mozart/daemon/config.py` — DaemonConfig, ResourceLimitConfig
      - `src/mozart/daemon/types.py` — JobRequest, JobResponse, DaemonStatus
      - `src/mozart/daemon/output.py` — OutputProtocol, StructuredOutput
      - `src/mozart/daemon/job_service.py` — Core execution service
      - `src/mozart/daemon/ipc/` — Unix socket server, client, JSON-RPC protocol

      **Coding Standards:**
      - Async throughout, signal-safe handlers
      - structlog for logging (never print)
      - psutil as optional dependency (fallback to /proc)
      - Type hints everywhere

    output_footer: |
      ---
      Write output to specified workspace file. All source files must be
      syntactically correct and importable. Run tests to verify.

      **LAST STEP — Return to main branch before this sheet ends:**
      ```bash
      cd /home/emzi/Projects/mozart-ai-compose
      git checkout main
      ```
      This keeps Mozart running from the stable main branch between sheets.

validations:
  - type: file_exists
    path: "{workspace}/01-daemon-design.md"
    condition: "sheet_num == 1"
  - type: content_contains
    path: "{workspace}/01-daemon-design.md"
    pattern: "DAEMON_DESIGN_COMPLETE"
    condition: "sheet_num == 1"

  - type: file_exists
    path: "{workspace}/02-daemon-process.md"
    condition: "sheet_num == 2"
  - type: content_contains
    path: "{workspace}/02-daemon-process.md"
    pattern: "DAEMON_PROCESS_COMPLETE"
    condition: "sheet_num == 2"

  - type: file_exists
    path: "{workspace}/03-job-manager.md"
    condition: "sheet_num == 3"
  - type: content_contains
    path: "{workspace}/03-job-manager.md"
    pattern: "JOB_MANAGER_COMPLETE"
    condition: "sheet_num == 3"

  - type: file_exists
    path: "{workspace}/04-resource-monitor.md"
    condition: "sheet_num == 4"
  - type: content_contains
    path: "{workspace}/04-resource-monitor.md"
    pattern: "RESOURCE_MONITOR_COMPLETE"
    condition: "sheet_num == 4"

  - type: file_exists
    path: "{workspace}/05-process-groups.md"
    condition: "sheet_num == 5"
  - type: content_contains
    path: "{workspace}/05-process-groups.md"
    pattern: "PROCESS_GROUPS_COMPLETE"
    condition: "sheet_num == 5"

  - type: file_exists
    path: "{workspace}/06-health-check.md"
    condition: "sheet_num == 6"
  - type: content_contains
    path: "{workspace}/06-health-check.md"
    pattern: "HEALTH_CHECK_COMPLETE"
    condition: "sheet_num == 6"

  - type: file_exists
    path: "{workspace}/07-tests.md"
    condition: "sheet_num == 7"
  - type: content_contains
    path: "{workspace}/07-tests.md"
    pattern: "TESTS_COMPLETE"
    condition: "sheet_num == 7"

  - type: file_exists
    path: "{workspace}/08-review-1.md"
    condition: "sheet_num == 8"
  - type: file_exists
    path: "{workspace}/08-review-2.md"
    condition: "sheet_num == 9"
  - type: file_exists
    path: "{workspace}/08-review-3.md"
    condition: "sheet_num == 10"

  - type: file_exists
    path: "{workspace}/09-fixes.md"
    condition: "sheet_num == 11"
  - type: content_contains
    path: "{workspace}/09-fixes.md"
    pattern: "FIXES_APPLIED"
    condition: "sheet_num == 11"

  - type: file_exists
    path: "{workspace}/10-commit.md"
    condition: "sheet_num == 12"
  - type: content_contains
    path: "{workspace}/10-commit.md"
    pattern: "COMMIT_COMPLETE"
    condition: "sheet_num == 12"

  # ═══ HARD GATES: Imports + tests pass (on daemon-symphony branch) ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/python -c 'from mozart.daemon.process import daemon_app; from mozart.daemon.manager import JobManager; from mozart.daemon.monitor import ResourceMonitor; print(\"OK\")' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 7"
    description: "All daemon service imports succeed"
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/pytest tests/test_daemon_process.py tests/test_daemon_manager.py tests/test_daemon_monitor.py -q --tb=line --no-header 2>&1 | tail -1 | grep -qE '[0-9]+ passed' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 7"
    description: "Daemon service tests pass"

  # ═══ HARD GATES: Mozart canary on main ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout main -q && .venv/bin/mozart validate examples/sheet-review.yaml --json 2>/dev/null; test $? -le 1"
    condition: "sheet_num == 12"
    description: "Mozart CLI canary — main branch still works"

  # ═══ HARD GATES: Full test suite on daemon-symphony ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/pytest tests/ -x --timeout=120 -q --tb=line --no-header 2>&1 | tail -1 | grep -qE '[0-9]+ passed' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 12"
    description: "Full test suite passes on daemon-symphony"
