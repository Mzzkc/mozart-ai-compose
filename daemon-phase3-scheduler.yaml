# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║              DAEMON SYMPHONY: PHASE 3 — CROSS-JOB SCHEDULER               ║
# ║                                                                            ║
# ║  Intelligence layer. Adds cross-job scheduling, rate limit coordination,   ║
# ║  backpressure, and centralized learning. Makes the daemon SMART about      ║
# ║  resource allocation across concurrent jobs.                               ║
# ║                                                                            ║
# ║  Execution Flow (9 stages → 11 sheets after fan-out):                     ║
# ║                                                                            ║
# ║    [1] Discovery → [2] Global Scheduler → [3] Rate Coordinator →          ║
# ║    [4] Backpressure → [5] Learning Central → [6] Tests →                  ║
# ║        [7a] [7b] [7c]  ← Code Reviews (PARALLEL) →                        ║
# ║    [8] Apply Fixes → [9] Commit                                           ║
# ║                                                                            ║
# ║  Depends on: Phases 0-2 (daemon package, IPC, daemon process)             ║
# ║  Chains to: daemon-phase4-integration.yaml                                ║
# ╚══════════════════════════════════════════════════════════════════════════════╝

name: "daemon-phase3-scheduler"
description: "Cross-job intelligence — global scheduler, rate limits, backpressure, learning centralization"

workspace: "./.daemon-workspace-p3"

workspace_lifecycle:
  archive_on_fresh: true
  max_archives: 5

backend:
  type: claude_cli
  skip_permissions: true
  working_directory: /home/emzi/Projects/mozart-ai-compose
  timeout_seconds: 2400

cross_sheet:
  auto_capture_stdout: true
  max_output_chars: 3000
  lookback_sheets: 2
  capture_files:
    - "{{ workspace }}/*.md"

sheet:
  size: 1
  total_items: 9

  fan_out:
    7: 3

  dependencies:
    2: [1]
    3: [2]
    4: [3]
    5: [4]
    6: [5]
    7: [6]
    8: [7]
    9: [8]

parallel:
  enabled: true
  max_concurrent: 3

retry:
  max_retries: 2

on_success:
  - type: run_job
    job_path: "daemon-phase4-integration.yaml"
    description: "Chain to Phase 4: Integration & Polish"
    detached: true

concert:
  enabled: true
  max_chain_depth: 5
  cooldown_between_jobs_seconds: 30

prompt:
  template: |
    {{ preamble }}

    **Sheet:** {{ sheet_num }} of {{ total_sheets }} | **Stage:** {{ stage }}
    **Workspace:** {{ workspace }}

    {% if stage == 1 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 1: DISCOVERY — SCHEDULING & LEARNING ARCHITECTURE                     ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Study the existing scheduling and learning systems to design cross-job versions.

    **Read these files:**
    - `src/mozart/execution/parallel.py` — ParallelExecutor (per-job concurrency)
    - `src/mozart/execution/dag.py` — DependencyDAG (sheet ordering)
    - `src/mozart/execution/retry_strategy.py` — AdaptiveRetryStrategy
    - `src/mozart/execution/circuit_breaker.py` — CircuitBreaker
    - `src/mozart/learning/store/base.py` — GlobalLearningStore schema
    - `src/mozart/learning/store/rate_limits.py` — Rate limit coordination (SQLite)
    - `src/mozart/learning/store/patterns_broadcast.py` — Pattern broadcasting
    - `src/mozart/daemon/manager.py` — Phase 2's JobManager

    **Design Requirements:**

    1. **Global Sheet Scheduler** — Cross-job sheet-level scheduling:
       - Priority queue: sheets from all jobs, prioritized by urgency
       - Concurrency control: max_concurrent_sheets across ALL jobs
       - Fair scheduling: no single job starves others
       - DAG-aware: only schedule sheets whose dependencies are met

    2. **Rate Limit Coordinator** — Replace SQLite-based cross-process coordination:
       - In-memory rate limit tracking (daemon owns all jobs)
       - When Job A hits rate limit, ALL jobs back off
       - Track per-backend rate limit windows
       - Much faster than current SQLite cross-process approach

    3. **Backpressure** — Load shedding when system is stressed:
       - When memory > 80% limit: slow down new sheet starts
       - When API rate limited: pause all jobs using that backend
       - When circuit breaker trips: apply cooling period across jobs
       - Configurable thresholds in DaemonConfig

    4. **Learning Centralization** — Single GlobalLearningStore instance:
       - Currently each job opens its own SQLite connection
       - Daemon can use one in-memory store, persist periodically
       - Pattern discoveries immediately available to all jobs
       - Cross-job effectiveness tracking (pattern X works for job A, not B)

    **Output:** `{{ workspace }}/01-scheduler-design.md`
    End with: `SCHEDULER_DESIGN_COMPLETE: yes`

    {% elif stage == 2 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 2: GLOBAL SHEET SCHEDULER                                             ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create the cross-job sheet scheduler.

    {% if previous_outputs and 1 in previous_outputs %}
    **Stage 1 design:**
    {{ previous_outputs[1][:2000] }}
    {% endif %}

    **Create `src/mozart/daemon/scheduler.py`:**

    ```python
    """Global sheet scheduler — cross-job concurrency control."""
    import asyncio
    import heapq
    from dataclasses import dataclass, field
    from typing import Any

    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.scheduler")


    @dataclass(order=True)
    class SheetWork:
        """A unit of work (sheet) in the global scheduler."""
        priority: int  # Lower = higher priority
        job_id: str = field(compare=False)
        sheet_num: int = field(compare=False)
        execute: Any = field(compare=False)  # Coroutine to execute
        dependencies_met: bool = field(default=True, compare=False)


    class GlobalScheduler:
        """Schedules sheets across all jobs with concurrency control.

        Unlike per-job ParallelExecutor, this coordinates sheets from
        ALL running jobs, enforcing global concurrency limits and
        fair scheduling.
        """

        def __init__(self, max_concurrent_sheets: int = 10):
            self._max_concurrent = max_concurrent_sheets
            self._queue: list[SheetWork] = []  # Min-heap by priority
            self._running: dict[str, asyncio.Task] = {}  # key: "job_id:sheet_num"
            self._semaphore = asyncio.Semaphore(max_concurrent_sheets)
            self._lock = asyncio.Lock()

        async def submit(self, work: SheetWork) -> None:
            """Submit a sheet for scheduling."""
            async with self._lock:
                heapq.heappush(self._queue, work)
            await self._try_dispatch()

        async def _try_dispatch(self) -> None:
            """Dispatch ready sheets up to concurrency limit."""
            async with self._lock:
                while self._queue and len(self._running) < self._max_concurrent:
                    work = heapq.heappop(self._queue)
                    if not work.dependencies_met:
                        heapq.heappush(self._queue, work)
                        break
                    key = f"{work.job_id}:{work.sheet_num}"
                    task = asyncio.create_task(self._execute(work))
                    self._running[key] = task

        async def _execute(self, work: SheetWork) -> None:
            """Execute a sheet with semaphore control."""
            key = f"{work.job_id}:{work.sheet_num}"
            async with self._semaphore:
                try:
                    await work.execute()
                finally:
                    async with self._lock:
                        self._running.pop(key, None)
                    await self._try_dispatch()

        @property
        def active_count(self) -> int:
            return len(self._running)

        @property
        def queued_count(self) -> int:
            return len(self._queue)
    ```

    **Integrate with JobManager:** Modify `manager.py` to route sheet execution
    through the GlobalScheduler instead of running directly.

    **Output:** `{{ workspace }}/02-scheduler.md`
    End with: `SCHEDULER_COMPLETE: yes`

    {% elif stage == 3 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 3: RATE LIMIT COORDINATOR                                             ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Create in-memory rate limit coordination replacing SQLite cross-process sharing.

    **Read first:**
    - `src/mozart/learning/store/rate_limits.py` — Current SQLite-based approach
    - `src/mozart/execution/runner/recovery.py` — How runner handles rate limits

    **Create `src/mozart/daemon/rate_coordinator.py`:**

    ```python
    """Cross-job rate limit coordination."""
    import asyncio
    import time
    from dataclasses import dataclass

    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.rate_coordinator")


    @dataclass
    class RateLimitEvent:
        backend_type: str
        detected_at: float
        suggested_wait_seconds: float
        job_id: str
        sheet_num: int


    class RateLimitCoordinator:
        """In-memory rate limit coordination across all daemon jobs.

        When any job hits a rate limit, ALL jobs using that backend
        are notified to back off. Much faster than SQLite cross-process
        approach since everything is in-process.
        """

        def __init__(self):
            self._events: list[RateLimitEvent] = []
            self._active_limits: dict[str, float] = {}  # backend -> resume_at
            self._lock = asyncio.Lock()
            self._waiters: dict[str, asyncio.Event] = {}  # backend -> cleared event

        async def report_rate_limit(
            self, backend_type: str, wait_seconds: float,
            job_id: str, sheet_num: int,
        ) -> None:
            """Report a rate limit hit — all jobs back off."""
            async with self._lock:
                resume_at = time.monotonic() + wait_seconds
                self._active_limits[backend_type] = max(
                    self._active_limits.get(backend_type, 0), resume_at,
                )
                self._events.append(RateLimitEvent(
                    backend_type=backend_type,
                    detected_at=time.monotonic(),
                    suggested_wait_seconds=wait_seconds,
                    job_id=job_id,
                    sheet_num=sheet_num,
                ))
                # Clear old events
                cutoff = time.monotonic() - 3600
                self._events = [e for e in self._events if e.detected_at > cutoff]

            _logger.warning("rate_limit.reported",
                          backend=backend_type, wait_seconds=wait_seconds,
                          job_id=job_id)

        async def wait_if_limited(self, backend_type: str) -> float:
            """Wait if backend is rate-limited. Returns seconds waited."""
            async with self._lock:
                resume_at = self._active_limits.get(backend_type, 0)

            wait_time = resume_at - time.monotonic()
            if wait_time > 0:
                _logger.info("rate_limit.waiting",
                           backend=backend_type, seconds=wait_time)
                await asyncio.sleep(wait_time)
                return wait_time
            return 0.0

        def is_limited(self, backend_type: str) -> bool:
            """Check if backend is currently rate-limited."""
            return self._active_limits.get(backend_type, 0) > time.monotonic()

        @property
        def active_limits(self) -> dict[str, float]:
            """Currently active rate limits (backend -> seconds remaining)."""
            now = time.monotonic()
            return {
                backend: resume_at - now
                for backend, resume_at in self._active_limits.items()
                if resume_at > now
            }
    ```

    **Wire into JobManager and runner:** The coordinator should be accessible
    to all running jobs. When a job's RecoveryMixin detects a rate limit, it
    should report to the coordinator. Before starting any sheet, check the
    coordinator.

    **Output:** `{{ workspace }}/03-rate-coordinator.md`
    End with: `RATE_COORDINATOR_COMPLETE: yes`

    {% elif stage == 4 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 4: BACKPRESSURE                                                       ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Implement backpressure — slow down or reject work when system is stressed.

    **Create `src/mozart/daemon/backpressure.py`:**

    ```python
    """Backpressure — adaptive load management."""
    import asyncio
    from enum import Enum

    from mozart.daemon.monitor import ResourceMonitor
    from mozart.daemon.rate_coordinator import RateLimitCoordinator
    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.backpressure")


    class PressureLevel(Enum):
        NONE = "none"          # All systems go
        LOW = "low"            # Slight delay between sheets
        MEDIUM = "medium"      # Significant delay, warn on new jobs
        HIGH = "high"          # Reject new jobs, wait for relief
        CRITICAL = "critical"  # Emergency: cancel lowest-priority jobs


    class BackpressureController:
        """Manages system load through adaptive backpressure."""

        def __init__(
            self, monitor: ResourceMonitor,
            rate_coordinator: RateLimitCoordinator,
        ):
            self._monitor = monitor
            self._rate_coordinator = rate_coordinator

        def current_level(self) -> PressureLevel:
            """Assess current pressure level."""
            metrics = self._monitor.metrics
            memory_pct = metrics.get("memory_mb", 0) / max(
                self._monitor._limits.max_memory_mb, 1
            )

            if memory_pct > 0.95 or not self._monitor.is_accepting_work():
                return PressureLevel.CRITICAL
            elif memory_pct > 0.85 or self._rate_coordinator.active_limits:
                return PressureLevel.HIGH
            elif memory_pct > 0.70:
                return PressureLevel.MEDIUM
            elif memory_pct > 0.50:
                return PressureLevel.LOW
            return PressureLevel.NONE

        async def gate(self) -> None:
            """Adaptive delay based on current pressure.
            Call before starting each sheet.
            """
            level = self.current_level()
            delays = {
                PressureLevel.NONE: 0,
                PressureLevel.LOW: 2,
                PressureLevel.MEDIUM: 10,
                PressureLevel.HIGH: 30,
                PressureLevel.CRITICAL: 60,
            }
            delay = delays[level]
            if delay > 0:
                _logger.info("backpressure.delay",
                           level=level.value, delay_seconds=delay)
                await asyncio.sleep(delay)

        def should_accept_job(self) -> bool:
            """Whether to accept new job submissions."""
            return self.current_level().value not in ("high", "critical")
    ```

    **Wire into GlobalScheduler:** Before dispatching each sheet, call
    `backpressure.gate()`. In JobManager.submit_job(), check
    `backpressure.should_accept_job()`.

    **Output:** `{{ workspace }}/04-backpressure.md`
    End with: `BACKPRESSURE_COMPLETE: yes`

    {% elif stage == 5 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 5: LEARNING ENGINE CENTRALIZATION                                     ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Centralize the learning system for in-process sharing across all daemon jobs.

    **Read first:**
    - `src/mozart/learning/global_store.py` — Current store (re-exports)
    - `src/mozart/learning/store/base.py` — GlobalLearningStoreBase
    - `src/mozart/learning/store/patterns.py` — Pattern operations
    - `src/mozart/execution/runner/patterns.py` — How runner uses patterns

    **Create `src/mozart/daemon/learning_hub.py`:**

    ```python
    """Centralized learning hub — single store shared across all daemon jobs."""
    import asyncio
    from pathlib import Path

    from mozart.learning.global_store import GlobalLearningStore
    from mozart.core.logging import get_logger

    _logger = get_logger("daemon.learning")


    class LearningHub:
        """Centralized learning for daemon mode.

        Instead of each job opening its own SQLite connection to
        ~/.mozart/global-learning.db, the daemon maintains a single
        GlobalLearningStore instance shared across all jobs.

        Benefits:
        - Pattern discoveries in Job A immediately available to Job B
        - No cross-process SQLite locking contention
        - Periodic persistence instead of per-write persistence
        - Cross-job effectiveness tracking
        """

        def __init__(self, db_path: Path | None = None):
            self._db_path = db_path or Path("~/.mozart/global-learning.db").expanduser()
            self._store: GlobalLearningStore | None = None
            self._persist_interval = 60.0  # Persist every 60 seconds
            self._persist_task: asyncio.Task | None = None

        async def start(self) -> None:
            """Initialize store and start persistence loop."""
            self._store = GlobalLearningStore(str(self._db_path))
            self._persist_task = asyncio.create_task(self._persist_loop())
            _logger.info("learning_hub.started", db_path=str(self._db_path))

        async def stop(self) -> None:
            """Persist final state and stop."""
            if self._persist_task:
                self._persist_task.cancel()
            if self._store:
                # Final persist
                _logger.info("learning_hub.final_persist")
            _logger.info("learning_hub.stopped")

        @property
        def store(self) -> GlobalLearningStore:
            """Get the shared learning store."""
            if not self._store:
                raise RuntimeError("LearningHub not started")
            return self._store

        async def _persist_loop(self) -> None:
            """Periodically persist learning state."""
            while True:
                try:
                    await asyncio.sleep(self._persist_interval)
                    _logger.debug("learning_hub.persist")
                except asyncio.CancelledError:
                    break
    ```

    **Integration:** Modify JobManager to pass `learning_hub.store` to each
    JobService instance instead of letting each job create its own store.

    **Output:** `{{ workspace }}/05-learning-hub.md`
    End with: `LEARNING_HUB_COMPLETE: yes`

    {% elif stage == 6 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 6: TESTS                                                              ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Write tests for all Phase 3 components.

    **Create:**

    1. **`tests/test_daemon_scheduler.py`**
       - Test GlobalScheduler submit and dispatch
       - Test concurrency limit enforcement
       - Test priority ordering (lower priority = dispatched first)
       - Test dependency-aware scheduling
       - Test fair scheduling across multiple jobs

    2. **`tests/test_daemon_rate_coordinator.py`**
       - Test report_rate_limit broadcasts to all jobs
       - Test wait_if_limited blocks until window expires
       - Test is_limited returns correct state
       - Test event cleanup (old events removed)
       - Test concurrent access safety

    3. **`tests/test_daemon_backpressure.py`**
       - Test PressureLevel calculation at various memory levels
       - Test gate() applies correct delays
       - Test should_accept_job at various pressure levels
       - Test with mocked ResourceMonitor and RateLimitCoordinator

    4. **`tests/test_daemon_learning_hub.py`**
       - Test LearningHub start/stop lifecycle
       - Test store property returns GlobalLearningStore
       - Test persistence loop runs periodically
       - Test multiple jobs share same store instance

    **Run:**
    ```bash
    pytest tests/test_daemon_scheduler.py tests/test_daemon_rate_coordinator.py \
           tests/test_daemon_backpressure.py tests/test_daemon_learning_hub.py -v
    pytest tests/ -x --timeout=180 -q
    ```

    **Output:** `{{ workspace }}/06-tests.md`
    End with: `TESTS_COMPLETE: yes`

    {% elif stage == 7 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 7: CODE REVIEW ({{ instance }} of {{ fan_count }})                    ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    {% if instance == 1 %}
    **REVIEWER: Scheduling Algorithm**

    Review scheduler, rate coordinator, backpressure for:
    1. **Fairness** — No job starvation, priority inversion handled
    2. **Correctness** — Heap operations correct, concurrency safe
    3. **Deadlock freedom** — Lock ordering consistent, no circular waits
    4. **Resource leaks** — Tasks cleaned up on error
    5. **Performance** — O(log n) scheduling, no hot loops

    {% elif instance == 2 %}
    **REVIEWER: Learning Integration**

    Review LearningHub and cross-job pattern sharing for:
    1. **Consistency** — SharedStore is thread-safe for concurrent access
    2. **Persistence** — Data survives daemon restart
    3. **Isolation** — One job's patterns don't corrupt another
    4. **API compatibility** — Store interface unchanged from per-job usage

    {% elif instance == 3 %}
    **REVIEWER: Backwards Compat + Integration**

    1. Verify all daemon components integrate cleanly with JobManager
    2. Test existing CLI still works without daemon
    3. Full test suite passes:
    ```bash
    pytest tests/ -x --timeout=180 -q
    ```
    {% endif %}

    **Output:** `{{ workspace }}/07-review-{{ instance }}.md`
    End with: `REVIEW_COMPLETE: yes`

    {% elif stage == 8 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 8: APPLY FIXES                                                        ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read reviews and apply fixes:
    ```bash
    cat {{ workspace }}/07-review-1.md
    cat {{ workspace }}/07-review-2.md
    cat {{ workspace }}/07-review-3.md
    ```

    Focus on: deadlocks, fairness bugs, integration issues.

    Verify:
    ```bash
    pytest tests/test_daemon_*.py -v
    pytest tests/ -x --timeout=180 -q
    ruff check src/mozart/daemon/
    ```

    **Output:** `{{ workspace }}/08-fixes.md`
    End with: `FIXES_APPLIED: yes`

    {% elif stage == 9 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  STAGE 9: COMMIT                                                             ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    ```bash
    cd /home/emzi/Projects/mozart-ai-compose
    source .venv/bin/activate
    pytest tests/ -x --timeout=180 -q
    ruff check src/mozart/daemon/
    ```

    Selective commit:
    ```bash
    git add src/mozart/daemon/scheduler.py
    git add src/mozart/daemon/rate_coordinator.py
    git add src/mozart/daemon/backpressure.py
    git add src/mozart/daemon/learning_hub.py
    # Add modified manager.py if scheduler was wired in
    git add src/mozart/daemon/manager.py
    git add tests/test_daemon_scheduler.py
    git add tests/test_daemon_rate_coordinator.py
    git add tests/test_daemon_backpressure.py
    git add tests/test_daemon_learning_hub.py

    git status

    git commit -m "feat(daemon): Phase 3 — cross-job scheduler, rate coordination, backpressure

    Add intelligence layer to mozartd (issue #39):

    - GlobalScheduler: priority-based cross-job sheet scheduling
    - RateLimitCoordinator: in-memory rate limit sharing across jobs
    - BackpressureController: adaptive load management with pressure levels
    - LearningHub: centralized pattern store shared across all jobs
    - Comprehensive tests for all scheduling components

    Part of the Daemon Symphony concert (Phase 3 of 4).

    Co-Authored-By: Mozart AI Compose <noreply@anthropic.com>"

    git push -u origin daemon-symphony
    ```

    **Output:** `{{ workspace }}/09-commit.md`
    End with: `COMMIT_COMPLETE: yes`
    {% endif %}

    {{ output_footer }}

  variables:
    preamble: |
      ╔══════════════════════════════════════════════════════════════════════════════╗
      ║  DAEMON SYMPHONY: PHASE 3 — Cross-Job Scheduler                             ║
      ║  Part of the Mozart Daemon Migration Concert (Issue #39)                    ║
      ╚══════════════════════════════════════════════════════════════════════════════╝

      **Context:** Phase 3 of 5. Phases 0-2 built the daemon foundation. This phase
      adds the intelligence that makes the daemon SMART about resource allocation.

      **Project:** Mozart AI Compose at /home/emzi/Projects/mozart-ai-compose

      **CRITICAL — Branch Protocol (do this FIRST, before any other work):**
      ```bash
      cd /home/emzi/Projects/mozart-ai-compose
      git checkout daemon-symphony
      ```
      All commits go to `daemon-symphony`, NEVER to `main`.
      NEVER use `git add .` or `git add -A`. Only stage specific source files listed in the instructions.
      NEVER commit workspace files (`.daemon-workspace-*`). They are gitignored artifacts.

      **Phase 3 Goal:** Cross-job scheduling, rate limit coordination, backpressure,
      and centralized learning. The daemon knows about ALL jobs and optimizes globally.

      **Already exists from Phases 0-2:**
      - `src/mozart/daemon/config.py` — DaemonConfig with resource limits
      - `src/mozart/daemon/ipc/` — Unix socket server + client
      - `src/mozart/daemon/process.py` — mozartd entry point
      - `src/mozart/daemon/manager.py` — JobManager with asyncio tasks
      - `src/mozart/daemon/monitor.py` — ResourceMonitor

      **Coding Standards:**
      - Async throughout, lock ordering documented
      - heapq for priority queues
      - structlog for logging
      - Type hints everywhere

    output_footer: |
      ---
      Write output to specified workspace file. All source files must be
      syntactically correct and importable. Run tests to verify.

      **LAST STEP — Return to main branch before this sheet ends:**
      ```bash
      cd /home/emzi/Projects/mozart-ai-compose
      git checkout main
      ```
      This keeps Mozart running from the stable main branch between sheets.

validations:
  - type: file_exists
    path: "{workspace}/01-scheduler-design.md"
    condition: "sheet_num == 1"
  - type: content_contains
    path: "{workspace}/01-scheduler-design.md"
    pattern: "SCHEDULER_DESIGN_COMPLETE"
    condition: "sheet_num == 1"

  - type: file_exists
    path: "{workspace}/02-scheduler.md"
    condition: "sheet_num == 2"
  - type: content_contains
    path: "{workspace}/02-scheduler.md"
    pattern: "SCHEDULER_COMPLETE"
    condition: "sheet_num == 2"

  - type: file_exists
    path: "{workspace}/03-rate-coordinator.md"
    condition: "sheet_num == 3"
  - type: content_contains
    path: "{workspace}/03-rate-coordinator.md"
    pattern: "RATE_COORDINATOR_COMPLETE"
    condition: "sheet_num == 3"

  - type: file_exists
    path: "{workspace}/04-backpressure.md"
    condition: "sheet_num == 4"
  - type: content_contains
    path: "{workspace}/04-backpressure.md"
    pattern: "BACKPRESSURE_COMPLETE"
    condition: "sheet_num == 4"

  - type: file_exists
    path: "{workspace}/05-learning-hub.md"
    condition: "sheet_num == 5"
  - type: content_contains
    path: "{workspace}/05-learning-hub.md"
    pattern: "LEARNING_HUB_COMPLETE"
    condition: "sheet_num == 5"

  - type: file_exists
    path: "{workspace}/06-tests.md"
    condition: "sheet_num == 6"
  - type: content_contains
    path: "{workspace}/06-tests.md"
    pattern: "TESTS_COMPLETE"
    condition: "sheet_num == 6"

  - type: file_exists
    path: "{workspace}/07-review-1.md"
    condition: "sheet_num == 7"
  - type: file_exists
    path: "{workspace}/07-review-2.md"
    condition: "sheet_num == 8"
  - type: file_exists
    path: "{workspace}/07-review-3.md"
    condition: "sheet_num == 9"

  - type: file_exists
    path: "{workspace}/08-fixes.md"
    condition: "sheet_num == 10"
  - type: content_contains
    path: "{workspace}/08-fixes.md"
    pattern: "FIXES_APPLIED"
    condition: "sheet_num == 10"

  - type: file_exists
    path: "{workspace}/09-commit.md"
    condition: "sheet_num == 11"
  - type: content_contains
    path: "{workspace}/09-commit.md"
    pattern: "COMMIT_COMPLETE"
    condition: "sheet_num == 11"

  # ═══ HARD GATES: Imports + tests pass (on daemon-symphony branch) ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/python -c 'from mozart.daemon.scheduler import GlobalScheduler; from mozart.daemon.rate_coordinator import RateLimitCoordinator; from mozart.daemon.backpressure import BackpressureController; from mozart.daemon.learning_hub import LearningHub; print(\"OK\")' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 6"
    description: "All scheduler imports succeed"
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/pytest tests/test_daemon_scheduler.py tests/test_daemon_rate_coordinator.py tests/test_daemon_backpressure.py tests/test_daemon_learning_hub.py -q --tb=line --no-header 2>&1 | tail -1 | grep -qE '[0-9]+ passed' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 6"
    description: "Scheduler unit tests pass"

  # ═══ HARD GATES: Mozart canary on main ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout main -q && .venv/bin/mozart validate examples/sheet-review.yaml --json 2>/dev/null; test $? -le 1"
    condition: "sheet_num == 11"
    description: "Mozart CLI canary — main branch still works"

  # ═══ HARD GATES: Full test suite on daemon-symphony ═══
  - type: command_succeeds
    command: "cd /home/emzi/Projects/mozart-ai-compose && git checkout daemon-symphony -q && .venv/bin/pytest tests/ -x --timeout=120 -q --tb=line --no-header 2>&1 | tail -1 | grep -qE '[0-9]+ passed' ; _r=$?; git checkout main -q; exit $_r"
    condition: "sheet_num == 11"
    description: "Full test suite passes on daemon-symphony"
