# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║              CONTINUOUS QUALITY IMPROVEMENT                                  ║
# ║                                                                              ║
# ║  Comprehensive quality review with expert perspectives, issue discovery,     ║
# ║  batched remediation, and automatic commits. Self-chains for continuous      ║
# ║  improvement until codebase reaches 100% quality.                            ║
# ║                                                                              ║
# ║  Usage:                                                                      ║
# ║    cd /path/to/your/project                                                  ║
# ║    mozart run /path/to/examples/quality-continuous.yaml                      ║
# ║                                                                              ║
# ║  Goal: Close the gap between 90% LLM completion and 100% production quality  ║
# ╚══════════════════════════════════════════════════════════════════════════════╝

name: "quality-continuous"
description: "Expert reviews + issue discovery + batched fixes + commits, self-chaining"

workspace: "./.quality-workspace"

workspace_lifecycle:
  archive_on_fresh: true
  max_archives: 10

backend:
  type: claude_cli
  skip_permissions: true
  timeout_seconds: 2400  # 40 min per sheet for thorough analysis

cross_sheet:
  auto_capture_stdout: true
  max_output_chars: 3000
  lookback_sheets: 2
  capture_files:
    - "{{ workspace }}/*.md"
    - "{{ workspace }}/*.yaml"

sheet:
  size: 1
  total_items: 15  # Reviews(1-5) + Batch1(6-7) + Batch2(8-9) + Batch3(10-11) + Verify(12) + Commit(13) + MergeResolve(14) + Issues(15)

retry:
  max_retries: 2

prompt:
  template: |
    {{ preamble }}

    {% if sheet_num == 1 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 1: ARCHITECTURE REVIEW                                                ║
    ║  "Structure reveals intent; misalignment reveals bugs"                       ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    **Step 0: Track iteration**
    ```bash
    mkdir -p {{ workspace }}
    if [ -f {{ workspace }}/.iteration ]; then
      ITER=$(cat {{ workspace }}/.iteration)
      ITER=$((ITER + 1))
    else
      ITER=1
    fi
    echo $ITER > {{ workspace }}/.iteration
    echo "=== QUALITY ITERATION $ITER ==="
    ```

    **Step 0.5: Survey open GitHub issues**
    ```bash
    # Pull open bugs and quality items — these are known problems to cross-reference
    gh issue list --state open --label "bug" --json number,title,body --limit 20 2>/dev/null || echo "gh not available"
    gh issue list --state open --label "mozart-quality" --json number,title,body --limit 10 2>/dev/null || true
    ```

    Save a summary of open issues to {{ workspace }}/00-known-issues.md:
    ```markdown
    # Known Open Issues (Iteration N)

    | # | Title | Labels | Relevant Modules |
    |---|-------|--------|------------------|
    ```

    For each open issue, note which source files or modules it relates to.
    Use this as **context for your architecture review** — if an open issue
    points to a specific module, pay extra attention to that area. If your
    analysis rediscovers a known issue, reference its GitHub issue number
    rather than reporting it as new.

    If `gh` is not available, skip this step and note "No GitHub access" in
    your output file. This step is informational — the review continues either way.

    You are the **Architecture Analyst**. Your job is to find structural issues
    that cause bugs, maintenance burden, and incomplete implementations.

    **Use the open issues list as a lens:** modules mentioned in open bugs
    deserve deeper scrutiny. If you find a structural problem that explains
    a known bug, call out the connection explicitly.

    **1. Module Cohesion & Correctness**
    - Do modules have single responsibilities?
    - Are there methods that belong elsewhere (feature envy)?
    - Are return types consistent? Do functions return what they claim?
    - Are edge cases handled or silently ignored?
    - Do functions do what their names suggest?

    **2. Import Graph & Dependencies**
    ```bash
    # Check for circular imports
    find . -name "*.py" -type f | head -20 | xargs grep -l "^from\|^import" | head -10

    # Look for tight coupling
    grep -r "from.*import \*" --include="*.py" | head -10
    ```
    - Circular dependencies?
    - Wildcard imports hiding dependencies?
    - Modules importing from implementation details instead of interfaces?

    **3. Code Flow & Control**
    - Are there unreachable code paths?
    - Do early returns leave resources uncleaned?
    - Are async/await patterns consistent?
    - Do loops have proper termination conditions?
    - Are there infinite loop risks?

    **4. Cleanup & Resource Management**
    - Are files/connections/locks properly closed?
    - Do context managers cover all resource acquisition?
    - Are there finally blocks that should exist but don't?
    - Do destructors/cleanup methods actually get called?

    **5. Interface Consistency**
    - Are public APIs clearly defined?
    - Do similar functions have similar signatures?
    - Are optional parameters actually optional in usage?
    - Do exceptions match documentation?

    **Output to:** {{ workspace }}/01-architecture-review.md

    Format:
    ```markdown
    # Architecture Review (Iteration N)

    ## Open Issues Cross-Reference
    | GitHub # | Title | Modules Affected | Findings This Review |
    |----------|-------|------------------|---------------------|

    ## Correctness Issues
    | Location | Issue | Severity | Fix |
    |----------|-------|----------|-----|

    ## Code Flow Problems
    | Location | Issue | Risk |
    |----------|-------|------|

    ## Cleanup Gaps
    | Resource | Location | Missing Cleanup |
    |----------|----------|-----------------|

    ## Structural Issues
    | Module | Cohesion | Problems |
    |--------|----------|----------|

    ## Top 5 Architecture Fixes Needed
    1. [Most critical]
    ...
    ```

    {% elif sheet_num == 2 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 2: TEST COVERAGE REVIEW                                               ║
    ║  "Untested code is broken code waiting to be discovered"                     ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    You are the **Test Coverage Analyst**. Find gaps that let bugs slip through.

    **1. Unit Test Gaps**
    ```bash
    # List test files
    find . -name "test_*.py" -o -name "*_test.py" | head -20

    # Count tests
    grep -r "def test_" --include="*.py" | wc -l

    # Find source files without corresponding tests
    for f in $(find src -name "*.py" -type f 2>/dev/null | head -20); do
      base=$(basename "$f" .py)
      if ! find . -name "test_${base}.py" -o -name "${base}_test.py" 2>/dev/null | grep -q .; then
        echo "No test for: $f"
      fi
    done
    ```

    **2. Functional Test Gaps**
    - Are there end-to-end tests for critical user flows?
    - Do integration tests cover module boundaries?
    - Are error paths tested, not just happy paths?
    - Do tests verify actual behavior or just call functions?

    **3. Smoke Test Coverage**
    - Can the app start without crashing?
    - Do basic commands work (--help, --version)?
    - Does import of main modules succeed?
    - Are config files validated on load?
    ```bash
    # Check for smoke tests
    grep -r "smoke\|sanity\|basic" --include="test_*.py" | head -10

    # Check if main entry points are tested
    grep -r "def test.*main\|def test.*cli\|def test.*app" --include="*.py" | head -10
    ```

    **4. Edge Case Coverage**
    - Empty inputs
    - None/null values
    - Boundary values (0, -1, max_int)
    - Unicode/special characters
    - Concurrent access
    - Network failures
    - Disk full scenarios

    **5. Test Quality Issues**
    - Tests that always pass (no assertions)
    - Tests that test implementation, not behavior
    - Flaky tests (timing-dependent)
    - Tests with shared mutable state
    - Missing cleanup in test fixtures

    **Output to:** {{ workspace }}/02-test-coverage-review.md

    Format:
    ```markdown
    # Test Coverage Review (Iteration N)

    ## Missing Unit Tests
    | Module | Functions Untested | Priority |
    |--------|-------------------|----------|

    ## Functional Test Gaps
    | User Flow | Current Coverage | Gap |
    |-----------|-----------------|-----|

    ## Missing Smoke Tests
    | Entry Point | Test Exists? | Impact if Broken |
    |-------------|--------------|------------------|

    ## Edge Cases Not Covered
    | Scenario | Files Affected | Risk |
    |----------|---------------|------|

    ## Test Quality Issues
    | Test File | Issue | Fix |
    |-----------|-------|-----|

    ## Top 5 Test Gaps to Fix
    1. [Most critical gap]
    ...
    ```

    {% elif sheet_num == 3 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 3: CODE DEBT & SIMPLICITY REVIEW                                      ║
    ║  "Complexity is the enemy of reliability"                                    ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    You are the **Code Debt & Simplicity Analyst**. Find complexity that hides bugs.

    **1. Dead Code & Cruft**
    ```bash
    # Find TODO/FIXME comments
    grep -rn "TODO\|FIXME\|HACK\|XXX" --include="*.py" | head -20

    # Find commented-out code blocks
    grep -rn "^#.*def \|^#.*class \|^#.*import " --include="*.py" | head -10

    # Find unused imports (basic check)
    for f in $(find . -name "*.py" -type f | head -20); do
      unused=$(python3 -c "
    import ast, sys
    try:
      tree = ast.parse(open('$f').read())
      imports = [n.names[0].name if hasattr(n, 'names') else n.module for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))]
      print(' '.join(imports[:5]))
    except: pass
    " 2>/dev/null)
      [ -n "$unused" ] && echo "$f: $unused"
    done | head -10
    ```

    **2. Complexity Hotspots**
    - Functions over 50 lines
    - Nesting deeper than 4 levels
    - Functions with more than 5 parameters
    - Classes with more than 10 methods
    - Files over 500 lines
    ```bash
    # Find large files
    find . -name "*.py" -type f -exec wc -l {} \; | sort -rn | head -10

    # Find deeply nested code
    grep -rn "        if\|        for\|        while" --include="*.py" | head -10
    ```

    **3. Code Simplicity Issues**
    - Overly clever one-liners that should be expanded
    - Premature abstractions (interfaces with one implementation)
    - Over-engineering (factory factories, abstract abstract classes)
    - Magic numbers without constants
    - Boolean parameters that should be enums
    - Long parameter lists that should be config objects

    **4. Naming & Clarity**
    - Single-letter variables (except loop counters)
    - Misleading names (is_valid that doesn't validate)
    - Inconsistent naming (mixedCase vs snake_case)
    - Abbreviations that aren't obvious
    - Names that don't match behavior

    **5. Type Hint Completeness**
    ```bash
    # Find functions without type hints
    grep -rn "def .*:" --include="*.py" | grep -v "def .*->.*:" | head -10

    # Find Any usage
    grep -rn ": Any\|-> Any" --include="*.py" | head -10
    ```

    **Output to:** {{ workspace }}/03-code-debt-review.md

    Format:
    ```markdown
    # Code Debt & Simplicity Review (Iteration N)

    ## Dead Code to Remove
    | Location | Type | Lines | Safe to Delete? |
    |----------|------|-------|-----------------|

    ## Complexity Hotspots
    | File | Function | LOC | Nesting | Params | Recommendation |
    |------|----------|-----|---------|--------|----------------|

    ## Over-Engineering
    | Location | Pattern | Simpler Alternative |
    |----------|---------|---------------------|

    ## Naming Issues
    | Location | Current Name | Problem | Suggested |
    |----------|--------------|---------|-----------|

    ## Type Hint Gaps
    | File | Functions Missing Hints | Priority |
    |------|------------------------|----------|

    ## Top 5 Simplification Opportunities
    1. [Most impactful simplification]
    ...
    ```

    {% elif sheet_num == 4 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 4: CATEGORY DISCOVERY (30 Specific Issues)                            ║
    ║  "Systematic discovery reveals patterns invisible to intuition"              ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    {% if previous_outputs %}
    ## Expert Reviews Summary
    {{ previous_outputs[1][:600] if 1 in previous_outputs else "" }}
    {{ previous_outputs[2][:600] if 2 in previous_outputs else "" }}
    {{ previous_outputs[3][:600] if 3 in previous_outputs else "" }}
    {% endif %}

    **Known issues context:** Read {{ workspace }}/00-known-issues.md (from sheet 1).
    If an issue you discover matches a known GitHub issue, reference its number
    instead of treating it as a new finding. Focus discovery effort on areas NOT
    already tracked.

    Find **30 specific, actionable issues** across 10 categories (3 per category).
    Focus on issues the expert reviews may have missed.

    **Categories:**
    1. **Dead Code** - unused imports, unreachable branches, orphan functions
    2. **Complexity** - functions >50 LOC, deep nesting, complex conditionals
    3. **Naming** - unclear names, inconsistent conventions, misleading names
    4. **Error Handling** - bare excepts, swallowed errors, missing context
    5. **Type Safety** - missing hints, Any abuse, type: ignore without reason
    6. **Duplication** - copy-paste code, repeated patterns extractable to functions
    7. **Documentation** - missing docstrings, stale comments, wrong docs
    8. **Testing Gaps** - untested paths, assertions that don't assert
    9. **Performance** - O(n²) loops, repeated computations, unnecessary allocations
    10. **Security** - path injection, unsafe deserialization, hardcoded secrets

    **Output to:** {{ workspace }}/04-category-discovery.yaml

    Format each issue:
    ```yaml
    issues:
      - id: Q001
        category: Dead Code
        file: path/to/file.py
        line: N
        severity: critical | high | medium | low
        description: Specific description of the issue
        current_code: |
          [5-10 line snippet showing the problem]
        suggested_fix: How to fix it
        fix_effort: quick | medium | significant
    ```

    {% elif sheet_num == 5 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 5: SYNTHESIS & PRIORITIZATION                                         ║
    ║  "Wisdom is knowing which fires to fight first"                              ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read all review files and create a prioritized remediation plan.

    **Files to synthesize:**
    - {{ workspace }}/01-architecture-review.md
    - {{ workspace }}/02-test-coverage-review.md
    - {{ workspace }}/03-code-debt-review.md
    - {{ workspace }}/04-category-discovery.yaml

    **Create prioritized plan:**

    1. **Deduplicate** - Remove issues found by multiple reviewers
    2. **Score** each issue: `severity × impact × (1/effort)`
       - Critical = 4, High = 3, Medium = 2, Low = 1
       - Quick effort = 3, Medium = 2, Significant = 1
    3. **Batch** into 3 remediation groups:
       - **Batch 1 (Sheet 6):** Quick wins - high impact, low effort
       - **Batch 2 (Sheet 7):** Medium effort - structural fixes
       - **Batch 3 (Sheet 8):** Significant - refactoring, new tests

    **Output to:** {{ workspace }}/05-fix-plan.md

    Format:
    ```markdown
    # Quality Remediation Plan (Iteration N)

    ## Summary
    | Metric | Count |
    |--------|-------|
    | Total unique issues | N |
    | Duplicates removed | N |
    | Critical | N |
    | High | N |

    ## Batch 1: Quick Wins (Sheet 6)
    | ID | Issue | Score | File |
    |----|-------|-------|------|

    ## Batch 2: Medium Effort (Sheet 7)
    | ID | Issue | Score | File |
    |----|-------|-------|------|

    ## Batch 3: Significant (Sheet 8)
    | ID | Issue | Score | File |
    |----|-------|-------|------|
    ```

    {% elif sheet_num == 6 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 6: BATCH 1 - QUICK WINS (MANDATORY 70%+ COMPLETION)                   ║
    ║  "Low-hanging fruit that compounds into significant improvement"             ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/05-fix-plan.md and fix Batch 1 issues.

    **MANDATORY REQUIREMENT: You MUST complete at least 70% of Batch 1 issues.**
    Validation will FAIL if you skip more than 30%.

    **Fix quick wins:**
    - Remove unused imports
    - Delete dead code
    - Add missing type hints (simple cases)
    - Fix obvious naming issues
    - Add missing docstrings
    - Remove commented-out code

    **Rules:**
    - One logical change at a time
    - Run tests after each change: `pytest -x -q --tb=short`
    - DO NOT SKIP unless genuinely blocked (not "risky" or "low benefit")
    - "Already fixed" is acceptable ONLY if you verify it's actually fixed

    **Valid skip reasons (ONLY these):**
    - Would break existing tests (must show which test)
    - Requires schema/migration change
    - File/function no longer exists
    - Already implemented (must verify with grep/code check)

    **INVALID skip reasons (will fail validation):**
    - "Risky" without specific test failure
    - "Low benefit" - that's not your call
    - "Extensive testing needed" - that's the job
    - "Better for future version" - do it now

    **Output to:** {{ workspace }}/06-batch1-fixes.md

    Format EXACTLY (validation parses this):
    ```markdown
    # Batch 1 Results: Quick Wins

    **Total:** N issues
    **Fixed:** N issues
    **Completion:** N% (MUST BE >= 70%)

    ## Fixed
    | ID | What was fixed | File | Verified |
    |----|---------------|------|----------|

    ## Deferred (max 30%)
    | ID | Blocker reason | Evidence |
    |----|----------------|----------|

    ## Test Results
    [pytest output showing all tests pass]
    ```

    {% elif sheet_num == 7 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 7: BATCH 1 - COMPLETION PASS (FINISH REMAINING)                       ║
    ║  "No excuses - complete what was deferred"                                   ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/06-batch1-fixes.md and complete ALL deferred items.

    **MANDATORY: Complete every deferred item or prove it's impossible.**

    For each deferred item from Batch 1:
    1. Re-attempt the fix
    2. If still blocked, provide CONCRETE evidence (error message, failing test)
    3. "Risky" or "complex" are NOT acceptable - do the work

    **Output to:** {{ workspace }}/07-batch1-completion.md

    Format:
    ```markdown
    # Batch 1 Completion Pass

    **Deferred from first pass:** N items
    **Now completed:** N items
    **Still blocked:** N items (must be 0 or have concrete evidence)

    ## Completed
    | ID | What was fixed | Evidence it works |
    |----|---------------|-------------------|

    ## Truly Blocked (with proof)
    | ID | Blocker | Error message/test failure |
    |----|---------|---------------------------|
    ```

    {% elif sheet_num == 8 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 8: BATCH 2 - MEDIUM EFFORT (MANDATORY 70%+ COMPLETION)                ║
    ║  "Structural improvements that prevent future bugs"                          ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/05-fix-plan.md and fix Batch 2 issues.

    **MANDATORY REQUIREMENT: You MUST complete at least 70% of Batch 2 issues.**

    **Fix medium effort issues:**
    - Error handling improvements
    - Extract duplicated code
    - Simplify complex conditionals
    - Add type hints requiring analysis
    - Fix code flow issues
    - Add missing cleanup/resource management

    **Rules:**
    - Test after each change
    - If a fix causes test failures, debug and fix the test issue
    - DO NOT give up on an issue without genuine blocker evidence
    - "Complex" is not a valid skip reason - break it into steps

    **Valid skip reasons (ONLY these):**
    - Would require database schema change
    - Would break public API contract
    - File/function no longer exists
    - Already implemented (must verify)

    **Output to:** {{ workspace }}/08-batch2-fixes.md

    Format EXACTLY:
    ```markdown
    # Batch 2 Results: Medium Effort

    **Total:** N issues
    **Fixed:** N issues
    **Completion:** N% (MUST BE >= 70%)

    ## Fixed
    | ID | What was fixed | File | Test verification |
    |----|---------------|------|-------------------|

    ## Deferred (max 30%)
    | ID | Blocker | Evidence command/output |
    |----|---------|------------------------|
    ```

    {% elif sheet_num == 9 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 9: BATCH 2 - COMPLETION PASS (FINISH REMAINING)                       ║
    ║  "No excuses - complete what was deferred"                                   ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/08-batch2-fixes.md and complete ALL deferred items.

    **MANDATORY: Complete every deferred item or prove it's impossible.**

    For each deferred item:
    1. Re-attempt the fix with fresh approach
    2. If blocked, provide CONCRETE evidence (specific error, test failure)
    3. "Would require significant effort" is NOT acceptable - do the work

    **Output to:** {{ workspace }}/09-batch2-completion.md

    Format:
    ```markdown
    # Batch 2 Completion Pass

    **Deferred from first pass:** N items
    **Now completed:** N items
    **Still blocked:** N items (must have concrete proof)

    ## Completed
    | ID | What was fixed | Test verification |
    |----|---------------|-------------------|

    ## Truly Blocked (with proof)
    | ID | Blocker | Specific error/failure |
    |----|---------|----------------------|
    ```

    {% elif sheet_num == 10 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 10: BATCH 3 - SIGNIFICANT FIXES (MANDATORY 50%+ COMPLETION)           ║
    ║  "The 10% that separates good from production-ready"                         ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/05-fix-plan.md and address Batch 3 issues.

    **MANDATORY REQUIREMENT: You MUST complete at least 50% of Batch 3 issues.**
    These are significant but that doesn't mean optional.

    **Fix significant issues:**
    - Write missing unit tests (PRIORITY - no skipping test writing)
    - Write missing smoke tests
    - Refactor complex functions (break into smaller steps)
    - Fix security issues (CRITICAL - no skipping)
    - Add missing functional tests

    **DO NOT be conservative. Be thorough:**
    - Tests are NEVER optional - write them
    - Security issues are NEVER optional - fix them
    - "Major architectural change" is not a skip reason - do it incrementally

    **Valid skip reasons (ONLY these):**
    - Would require multi-day refactoring (estimate hours)
    - Requires external dependency change
    - Already has adequate test coverage (show coverage proof)

    **Output to:** {{ workspace }}/10-batch3-fixes.md

    Format EXACTLY:
    ```markdown
    # Batch 3 Results: Significant Fixes

    **Total:** N issues
    **Fixed:** N issues
    **Completion:** N% (MUST BE >= 50%)

    ## Fixed
    | ID | What was fixed | File | Tests added/passed |
    |----|---------------|------|-------------------|

    ## Deferred
    | ID | Blocker | Estimated effort | Why not now |
    |----|---------|------------------|-------------|
    ```

    {% elif sheet_num == 11 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 11: BATCH 3 - COMPLETION PASS (FINISH REMAINING)                      ║
    ║  "Tests and security are NEVER optional"                                     ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    Read {{ workspace }}/10-batch3-fixes.md and complete ALL deferred items.

    **MANDATORY: Complete every deferred item. No exceptions for tests/security.**

    For each deferred item:
    1. If it's a missing test - WRITE THE TEST
    2. If it's a security issue - FIX IT
    3. If it's refactoring - do it incrementally, one function at a time
    4. Only truly multi-day efforts can remain (must estimate hours)

    **Output to:** {{ workspace }}/11-batch3-completion.md

    Format:
    ```markdown
    # Batch 3 Completion Pass

    **Deferred from first pass:** N items
    **Now completed:** N items
    **Remaining (multi-day only):** N items

    ## Completed
    | ID | What was fixed | Tests added |
    |----|---------------|-------------|

    ## Multi-Day Items (deferred to next iteration)
    | ID | Estimated hours | What's needed |
    |----|-----------------|---------------|
    ```

    {% elif sheet_num == 12 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 12: VERIFICATION & SUMMARY                                            ║
    ║  "Measure twice, commit once"                                                ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    **Step 1: Run full test suite**
    ```bash
    pytest --tb=short 2>&1 | tail -50
    ```

    **Step 2: Run type checker (if available)**
    ```bash
    mypy src/ --ignore-missing-imports 2>&1 | tail -30 || echo "mypy not configured"
    ```

    **Step 3: Check for regressions**
    ```bash
    # Verify imports still work
    python -c "import sys; sys.path.insert(0, 'src'); __import__('$(ls src/ | head -1)')" 2>&1 || echo "Import check skipped"
    ```

    **Step 4: Summarize this iteration**

    Read all batch results and create summary.

    **Output to:** {{ workspace }}/12-summary.md

    Read all batch results and completion passes:
    - {{ workspace }}/06-batch1-fixes.md
    - {{ workspace }}/07-batch1-completion.md
    - {{ workspace }}/08-batch2-fixes.md
    - {{ workspace }}/09-batch2-completion.md
    - {{ workspace }}/10-batch3-fixes.md
    - {{ workspace }}/11-batch3-completion.md

    ```markdown
    # Quality Iteration N Summary

    ## Issues Found
    | Source | Count |
    |--------|-------|
    | Architecture Review | N |
    | Test Coverage Review | N |
    | Code Debt Review | N |
    | Category Discovery | N |
    | **Total (deduplicated)** | **N** |

    ## Issues Fixed (First Pass + Completion)
    | Batch | First Pass | Completion | Total | Remaining |
    |-------|------------|------------|-------|-----------|
    | Quick Wins | N | N | N | N |
    | Medium | N | N | N | N |
    | Significant | N | N | N | N |
    | **Total** | **N** | **N** | **N** | **N** |

    ## Overall Completion Rate
    **N%** of identified issues resolved

    ## Key Improvements
    - [List major improvements]

    ## Remaining Issues (for next iteration)
    - [Only truly blocked items]

    ## Test Results
    [pytest output]
    ```

    {% elif sheet_num == 13 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 13: COMMIT CHANGES                                                    ║
    ║  "Ship it"                                                                   ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    **Step 1: Check for changes**
    ```bash
    git status --short | grep -E "\.py$" | head -20
    ```

    **Step 2: Build commit message from actual results**

    Read the batch result files to extract what was actually fixed:
    - {{ workspace }}/06-batch1-fixes.md — "## Fixed" table
    - {{ workspace }}/07-batch1-completion.md — "## Completed" table
    - {{ workspace }}/08-batch2-fixes.md — "## Fixed" table
    - {{ workspace }}/09-batch2-completion.md — "## Completed" table
    - {{ workspace }}/10-batch3-fixes.md — "## Fixed" table
    - {{ workspace }}/11-batch3-completion.md — "## Completed" table
    - {{ workspace }}/12-summary.md — overall stats

    Write a commit message file to {{ workspace }}/13-commit-message.txt with
    EXACTLY this structure:

    ```
    refactor: Quality iteration N - YYYY-MM-DD

    Fixed M issues across N files:
    - [ID]: [one-line description of what was fixed] ([file])
    - [ID]: [one-line description] ([file])
    ...
    (list EVERY fixed issue — do not truncate or summarize)

    Stats: M fixed, N deferred | Tests: [pass count] passed
    GitHub issues referenced: #X, #Y (if any known issues were addressed)

    Co-Authored-By: Mozart AI Compose <noreply@mozart.ai>
    ```

    Rules for the commit message:
    - The subject line (first line) must be under 72 characters
    - List every fixed issue by ID — this is the audit trail
    - Each line item should be a concrete change, not a category label
    - If a fix addresses a known GitHub issue, include "Refs #N" on that line
    - Do NOT paste raw markdown tables — convert to a flat list
    - Do NOT include deferred/skipped items — only what was fixed

    **Step 3: Selective staging**

    Do NOT use `git add -A` — that absorbs unrelated changes from concurrent
    jobs or manual work. Instead, stage ONLY the files this iteration touched.

    Extract the list of modified files from batch result tables (the "File"
    column in each "## Fixed" and "## Completed" section). Then stage only
    those files:
    ```bash
    # Stage only files this quality iteration actually modified.
    # Build list from batch results — example:
    #   git add src/mozart/core/config.py src/mozart/backends/claude_cli.py ...
    #
    # If you modified test files, include them too.
    # Do NOT stage workspace files, yaml scores, or unrelated changes.
    git add [list each file explicitly]

    # Verify: only our changes are staged
    git diff --cached --stat
    git diff --cached --stat | wc -l  # Should match expected file count
    ```

    If `git diff --cached --stat` shows files you did NOT modify, unstage them:
    ```bash
    git restore --staged path/to/unrelated/file.py
    ```

    If nothing is staged (all changes were already committed by another job),
    skip the commit and note "No changes to commit — likely absorbed by
    concurrent job" in your output file. Do NOT create an empty commit.

    **Step 4: Sync with remote before committing**
    ```bash
    # Check if remote has new commits we don't have
    git fetch origin main 2>/dev/null

    BEHIND=$(git rev-list HEAD..origin/main --count 2>/dev/null || echo "0")
    echo "Commits behind remote: $BEHIND"
    ```

    If behind, rebase our staged changes on top:
    ```bash
    # Stash staged changes, rebase, re-apply
    git stash
    git pull --rebase origin main
    git stash pop
    ```

    If the stash pop causes **merge conflicts**:
    1. Check which files conflict: `git diff --name-only --diff-filter=U`
    2. For each conflicted file, try to resolve:
       - If the conflict is in code you modified — keep both changes
       - If the conflict is in code you did NOT modify — accept theirs
       (`git checkout --theirs <file>`)
    3. After resolving simple conflicts: `git add <resolved files>`

    If conflicts remain that you cannot resolve inline (e.g., both sides
    modified the same function in incompatible ways), do NOT abort. Instead:

    1. Save the full conflict context for sheet 14 to resolve:
       ```bash
       # Capture every conflicted file's state
       CONFLICTED=$(git diff --name-only --diff-filter=U)
       echo "$CONFLICTED"
       ```
    2. For each conflicted file, write to `{{ workspace }}/13-merge-conflicts.md`:
       - The file path
       - What OUR quality fix was trying to do (from batch result files)
       - The full conflict markers (`<<<<<<<` ... `>>>>>>>`) verbatim
       - The remote's intent (from `git log --oneline origin/main -3 -- <file>`)
    3. Abort to a clean state:
       ```bash
       git checkout -- .       # Discard conflict markers
       git stash drop          # Drop our changes (context saved above)
       ```
       Now git is clean at remote HEAD. Sheet 14 will re-apply intelligently.
    4. Skip steps 5-7 below. Set status to "conflicts_pending" in output.

    **Step 5: Commit** (skip if conflicts_pending)
    ```bash
    ITER=$(cat {{ workspace }}/.iteration)
    git commit -F {{ workspace }}/13-commit-message.txt
    echo "Commit created for iteration $ITER"
    ```

    **Step 6: Push** (skip if conflicts_pending)
    ```bash
    git push
    echo "Pushed to remote"
    ```

    If push fails due to new remote commits (race condition), pull and retry:
    ```bash
    git pull --rebase origin main && git push
    ```
    Do NOT force-push.

    **Step 7: Verify** (skip if conflicts_pending)
    ```bash
    git log --oneline -1
    git show --stat HEAD | tail -20
    ```

    **Output to:** {{ workspace }}/13-commit.md

    ```markdown
    # Commit Results (Iteration N)

    ## Status
    [committed | conflicts_pending]

    ## Commit Message
    [full commit message as written, or "N/A — deferred to sheet 14"]

    ## Files Staged (selective)
    [list of files explicitly staged]

    ## Sync Status
    - Commits behind remote before sync: N
    - Rebase needed: yes/no
    - Merge conflicts: none / resolved inline / deferred to sheet 14

    ## Changes Committed
    [git diff --stat output, or "N/A — conflicts pending"]

    ## Commit Hash
    [git log --oneline -1 output, or "N/A"]

    ## Push Status
    - Pushed: yes/no/deferred
    - Retries needed: N

    ## Next Steps
    - If conflicts_pending: Sheet 14 will resolve and commit
    - If committed: Quality iteration complete
    ```

    {% elif sheet_num == 14 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 14: MERGE CONFLICT RESOLUTION                                         ║
    ║  "Complexity is not a reason to abort — it's a reason to think harder"       ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    **Step 1: Check if conflict resolution is needed**

    Read {{ workspace }}/13-commit.md and check the Status field.
    - If status is "committed" → sheet 13 succeeded, no conflicts.
      Write "No conflicts to resolve — sheet 13 committed successfully"
      to your output file and stop.
    - If status is "conflicts_pending" → proceed to step 2.

    Also verify the conflict context file exists:
    ```bash
    if [ -f "{{ workspace }}/13-merge-conflicts.md" ]; then
      echo "Conflict context found"
      wc -l "{{ workspace }}/13-merge-conflicts.md"
    else
      echo "No conflict file — nothing to resolve"
    fi
    ```

    **Step 2: Understand the conflicts**

    Read {{ workspace }}/13-merge-conflicts.md carefully. For each
    conflicted file, understand:
    - What our quality fix was changing and why
    - What the remote changed and why
    - Whether both changes can coexist

    Also read the relevant batch result files for full context on our
    changes:
    - {{ workspace }}/06-batch1-fixes.md
    - {{ workspace }}/08-batch2-fixes.md
    - {{ workspace }}/10-batch3-fixes.md

    **Step 3: Re-apply our changes on top of current remote**

    The working tree is clean at remote HEAD (sheet 13 cleaned up).
    For each conflicted file:

    1. Read the current version of the file
    2. Understand what our quality fix intended to change
    3. Apply the change manually — adapt it to the current code state
       rather than blindly replaying the old diff
    4. If the remote's changes made our fix unnecessary (e.g., they
       already fixed the same issue), skip it and note why

    After applying each file's changes:
    ```bash
    python -c "import ast; ast.parse(open('<file>').read())" 2>&1 || echo "SYNTAX ERROR in <file>"
    ```
    Fix any syntax errors immediately before moving to the next file.

    **Step 4: Verify the resolution**
    ```bash
    # Run tests to ensure our re-applied changes don't break anything
    pytest -x -q --tb=short 2>&1 | tail -20
    ```

    If tests fail:
    - Identify which of our re-applied changes caused the failure
    - If a specific fix conflicts with remote's changes, REVERT that one
      fix and note it as "incompatible with remote changes — deferred"
    - Re-run tests after each revert until they pass
    - Do NOT revert all changes — save as many fixes as possible

    **Step 5: Stage, commit, and push**
    ```bash
    # Stage only the files we re-applied changes to
    git add [list each resolved file]

    # Verify staging
    git diff --cached --stat

    # Commit using the message from sheet 13
    git commit -F {{ workspace }}/13-commit-message.txt

    # Push
    git push
    ```

    If push fails (race condition):
    ```bash
    git pull --rebase origin main && git push
    ```

    **Step 6: Verify**
    ```bash
    git log --oneline -1
    git show --stat HEAD | tail -20
    ```

    **Output to:** {{ workspace }}/14-merge-resolution.md

    ```markdown
    # Merge Conflict Resolution (Iteration N)

    ## Resolution Needed
    [yes / no — sheet 13 already committed]

    ## Conflicts Resolved
    | File | Our Intent | Remote's Change | Resolution | Tests Pass |
    |------|-----------|-----------------|------------|------------|

    ## Fixes Deferred (incompatible with remote)
    | File | Our Fix | Why Incompatible | Deferred To |
    |------|---------|------------------|-------------|
    (or "None — all fixes re-applied successfully")

    ## Test Results
    [pytest output summary]

    ## Commit
    - Hash: [git log --oneline -1]
    - Files changed: N
    - Pushed: yes/no

    ## Stats
    - Conflicts received from sheet 13: N files
    - Successfully re-applied: N
    - Deferred (incompatible): N
    - Resolution rate: N%
    ```

    {% elif sheet_num == 15 %}
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║  SHEET 15: FILE GITHUB ISSUES FOR DEFERRED ITEMS                             ║
    ║  "What we can't do today, we document for tomorrow"                          ║
    ╚══════════════════════════════════════════════════════════════════════════════╝

    File a GitHub issue tracking items that were deferred to future iterations.
    This prevents re-discovery waste and creates a real backlog for humans.

    **Step 1: Check prerequisites**
    ```bash
    gh auth status
    gh repo view --json name,owner
    ```
    If `gh` is not available or this isn't a GitHub repo, write a report noting
    "Skipped: no GitHub access" in your output file and stop.

    **Step 2: Collect remaining items**
    Read these files and gather ALL deferred/blocked/remaining items:
    - {{ workspace }}/12-summary.md → "Remaining Issues" section
    - {{ workspace }}/07-batch1-completion.md → "Truly Blocked" items
    - {{ workspace }}/09-batch2-completion.md → "Truly Blocked" items
    - {{ workspace }}/11-batch3-completion.md → "Multi-Day Items" section

    If there are NO remaining items (everything was fixed this iteration),
    write "All issues resolved — no GitHub issues needed" in your output
    file and stop. Do NOT create an empty tracking issue.

    **Step 3: Deduplicate against existing issues**
    ```bash
    gh issue list --label "mozart-quality" --state open --json number,title,body --limit 10
    ```
    - If a previous iteration's tracking issue exists and ALL its checklist
      items were resolved this iteration, close it with comment:
      "All items resolved in quality iteration N."
    - Note which items are already tracked to avoid filing duplicates.

    **Step 4: Create tracking issue**
    Create the label if it doesn't already exist:
    ```bash
    gh label create "mozart-quality" \
      --description "Deferred items from automated quality review" \
      --color "d4c5f9" 2>/dev/null || true
    ```

    Create ONE issue containing a checklist of all remaining items.

    Issue requirements:
    - **Title:** `Quality Iteration N: M deferred items requiring manual effort`
    - **Label:** `mozart-quality`
    - **Body must include:**
      - Checkbox list (`- [ ]`) of each deferred item with effort estimate
      - Category for each item (structural, testing, performance, etc.)
      - Summary table: issues found vs fixed vs deferred this iteration
      - Footer: `*Filed automatically by Mozart Quality Score*`

    Do NOT create individual issues per item — one summary issue per iteration.

    **Output to:** {{ workspace }}/15-issues-filed.md

    Format:
    ```markdown
    # GitHub Issues Filed (Iteration N)

    ## Summary
    | Metric | Count |
    |--------|-------|
    | Remaining items collected | N |
    | New issues created | N |
    | Previous issues closed | N |
    | Items already tracked (deduped) | N |

    ## Issues Created
    | Issue # | Title | Deferred Items |
    |---------|-------|----------------|

    ## Issues Closed (resolved this iteration)
    | Issue # | Reason |
    |---------|--------|

    ## All Tracked Items
    | ID | Description | Effort Estimate | GitHub Issue # |
    |----|-------------|-----------------|----------------|
    ```

    {% endif %}

  variables:
    preamble: |
      ╔══════════════════════════════════════════════════════════════════════════╗
      ║              CONTINUOUS QUALITY IMPROVEMENT                              ║
      ║                                                                          ║
      ║  Goal: Close the gap between 90% LLM completion and 100% quality         ║
      ║                                                                          ║
      ║  Expert Reviews → Issue Discovery → Prioritization → Fix → Commit        ║
      ╚══════════════════════════════════════════════════════════════════════════╝

      **The Last 10% Checklist:**
      - Correctness: Does code do what it claims?
      - Completeness: Are edge cases handled?
      - Cleanup: Are resources properly released?
      - Clarity: Can a new developer understand it?
      - Coverage: Are critical paths tested?

validations:
  - type: file_exists
    path: "{workspace}/01-architecture-review.md"
    description: "Architecture review must exist"
    condition: "sheet_num >= 1"

  - type: file_exists
    path: "{workspace}/02-test-coverage-review.md"
    description: "Test coverage review must exist"
    condition: "sheet_num >= 2"

  - type: file_exists
    path: "{workspace}/03-code-debt-review.md"
    description: "Code debt review must exist"
    condition: "sheet_num >= 3"

  - type: file_exists
    path: "{workspace}/04-category-discovery.yaml"
    description: "Category discovery must exist"
    condition: "sheet_num >= 4"

  - type: file_exists
    path: "{workspace}/05-fix-plan.md"
    description: "Fix plan must exist"
    condition: "sheet_num >= 5"

  - type: file_exists
    path: "{workspace}/06-batch1-fixes.md"
    description: "Batch 1 results must exist"
    condition: "sheet_num >= 6"

  # Batch 1: Must have 70%+ completion
  - type: command_succeeds
    command: |
      FILE="{workspace}/06-batch1-fixes.md"
      if [ ! -f "$FILE" ]; then echo "file missing"; exit 1; fi
      COMPLETION=$(grep -oE 'Completion.*[0-9]+%' "$FILE" | grep -oE '[0-9]+' | head -1)
      if [ -z "$COMPLETION" ]; then
        # Fallback: calculate from Fixed/Total
        FIXED=$(grep -E '^\*\*Fixed:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        TOTAL=$(grep -E '^\*\*Total:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        if [ -n "$FIXED" ] && [ -n "$TOTAL" ] && [ "$TOTAL" -gt 0 ]; then
          COMPLETION=$((FIXED * 100 / TOTAL))
        fi
      fi
      if [ -n "$COMPLETION" ] && [ "$COMPLETION" -ge 70 ]; then
        echo "Batch 1 completion: ${COMPLETION}% (>=70% required) - PASSED"
      else
        echo "Batch 1 completion: ${COMPLETION:-unknown}% (>=70% required) - FAILED"
        exit 1
      fi
    description: "Batch 1 must have >=70% completion rate"
    condition: "sheet_num >= 6"

  # Sheet 7: Batch 1 Completion Pass
  - type: file_exists
    path: "{workspace}/07-batch1-completion.md"
    description: "Batch 1 completion pass must exist"
    condition: "sheet_num >= 7"

  # Sheet 8: Batch 2 First Pass
  - type: file_exists
    path: "{workspace}/08-batch2-fixes.md"
    description: "Batch 2 results must exist"
    condition: "sheet_num >= 8"

  - type: command_succeeds
    command: |
      FILE="{workspace}/08-batch2-fixes.md"
      if [ ! -f "$FILE" ]; then echo "file missing"; exit 1; fi
      COMPLETION=$(grep -oE 'Completion.*[0-9]+%' "$FILE" | grep -oE '[0-9]+' | head -1)
      if [ -z "$COMPLETION" ]; then
        FIXED=$(grep -E '^\*\*Fixed:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        TOTAL=$(grep -E '^\*\*Total:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        if [ -n "$FIXED" ] && [ -n "$TOTAL" ] && [ "$TOTAL" -gt 0 ]; then
          COMPLETION=$((FIXED * 100 / TOTAL))
        fi
      fi
      if [ -n "$COMPLETION" ] && [ "$COMPLETION" -ge 70 ]; then
        echo "Batch 2 completion: ${COMPLETION}% (>=70% required) - PASSED"
      else
        echo "Batch 2 completion: ${COMPLETION:-unknown}% (>=70% required) - FAILED"
        exit 1
      fi
    description: "Batch 2 must have >=70% completion rate"
    condition: "sheet_num >= 8"

  # Sheet 9: Batch 2 Completion Pass
  - type: file_exists
    path: "{workspace}/09-batch2-completion.md"
    description: "Batch 2 completion pass must exist"
    condition: "sheet_num >= 9"

  # Sheet 10: Batch 3 First Pass
  - type: file_exists
    path: "{workspace}/10-batch3-fixes.md"
    description: "Batch 3 results must exist"
    condition: "sheet_num >= 10"

  - type: command_succeeds
    command: |
      FILE="{workspace}/10-batch3-fixes.md"
      if [ ! -f "$FILE" ]; then echo "file missing"; exit 1; fi
      COMPLETION=$(grep -oE 'Completion.*[0-9]+%' "$FILE" | grep -oE '[0-9]+' | head -1)
      if [ -z "$COMPLETION" ]; then
        FIXED=$(grep -E '^\*\*Fixed:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        TOTAL=$(grep -E '^\*\*Total:\*\*' "$FILE" | grep -oE '[0-9]+' | head -1)
        if [ -n "$FIXED" ] && [ -n "$TOTAL" ] && [ "$TOTAL" -gt 0 ]; then
          COMPLETION=$((FIXED * 100 / TOTAL))
        fi
      fi
      if [ -n "$COMPLETION" ] && [ "$COMPLETION" -ge 50 ]; then
        echo "Batch 3 completion: ${COMPLETION}% (>=50% required) - PASSED"
      else
        echo "Batch 3 completion: ${COMPLETION:-unknown}% (>=50% required) - FAILED"
        exit 1
      fi
    description: "Batch 3 must have >=50% completion rate"
    condition: "sheet_num >= 10"

  # Sheet 11: Batch 3 Completion Pass
  - type: file_exists
    path: "{workspace}/11-batch3-completion.md"
    description: "Batch 3 completion pass must exist"
    condition: "sheet_num >= 11"

  # Sheet 12: Verification & Summary
  - type: file_exists
    path: "{workspace}/12-summary.md"
    description: "Summary must exist"
    condition: "sheet_num >= 12"

  # Tests must pass before committing
  - type: command_succeeds
    command: "pytest -x -q --tb=no 2>&1 | tail -1 | grep -E 'passed|no tests'"
    description: "Tests must pass"
    condition: "sheet_num >= 12"

  # Sheet 13: Commit
  - type: file_exists
    path: "{workspace}/13-commit.md"
    description: "Commit report must exist"
    condition: "sheet_num == 13"

  # Verify commit was created (by sheet 13 or sheet 14 conflict resolver)
  - type: command_succeeds
    command: "git log --oneline -1 --since='30 minutes ago' 2>/dev/null | grep -qE '.' && echo 'passed' || echo 'no recent commit'"
    description: "Recent commit should exist after commit/merge-resolve"
    condition: "sheet_num == 14"

  # Sheet 14: Merge Conflict Resolution
  - type: file_exists
    path: "{workspace}/14-merge-resolution.md"
    description: "Merge resolution report must exist"
    condition: "sheet_num >= 14"

  # Sheet 15: GitHub Issues for Deferred Items
  - type: file_exists
    path: "{workspace}/15-issues-filed.md"
    description: "Issue filing report must exist"
    condition: "sheet_num >= 15"

# Self-chain for continuous improvement
on_success:
  - type: run_job
    job_path: "examples/quality-continuous.yaml"
    description: "Chain to next quality iteration"
    detached: true
    fresh: true  # Clear previous run's state to prevent infinite empty-run loop

concert:
  enabled: true
  max_chain_depth: 10
  cooldown_between_jobs_seconds: 120  # 2 min cooldown between iterations
  inherit_workspace: false
